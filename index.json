[{"content":"Online, Asynchronous Schema Change in F1\n1. 背景 分布式的 DBMS，底层数据层是共享的分布式 KV store（Spanner），中间层是 F1 servers 保存有 schema 信息并用于承接 client 请求 底层共享的 KV store 支持 get/put/del 操作，并且使用 OCC 实现分布式特性，使每个键-值对都有一个最近修改自动更新的时间戳，且多个 get 和 put 操作可以原子执行 当进行 online table schema 变更时，无法做到所有 F1 servers 的 schema 同步变更的（除非整个集群对应表停服），不同 F1 servers 的 schema 不同会导致 DML 应用多个 schemas 去操作底层共享 KV store，导致数据错乱 2. F1 Schema 变更协议 2.1 转移状态机 论文的变更协议就是为了解决在分布式数据库共享数据访问下在线、异步 schema 变更所带来的问题。其核心是：确保在任一时刻，系统使用的 schema 版本不超过两个，并且这些 schema 版本身具有特定的状态属性，不需要在变更时实现全局成员间的隐式或显式同步，也不需要在 DDL 完成后保留旧的 schema。\nF1 schema 内部包含 “tables, columns, indexes, constraints, and optimistic locks” 这些元素，其用作于关系型数据表和 KV 表间的映射。\nF1 将一次 schema 变更变为系列 schema 状态机转换，去避免 schema 变更产生的数据不一致，限定最多只有两个 schema 的原因也是为了使 schema 变更状态更有限。\nF1 把一次 schema 元素的变更拆解为多个逐步递进的中间状态，引入了两个中间状态，即 delete-only（只可删除的）和 write-only（只可写的）。同时，对于元素的非中间状态，定义为 absent（缺失的）和 public（共开的）。\n1 2 3 4 5 6 7 8 定义1: 一个只可删除的（Delete-only）元素E（tables/columns/indexes）的 KV 数据，不可以被其他事务读取； 并且： a）如果 schema 元素是一个 tables/columns，那么它只能够被 delete 操作修改； b）如果 schema 元素是一个 indexes，那么它只能够被 delete/update 操作修改，但是不能够 insert 新的 KV 数据。 定义2: 一个只可写的（Write-only）columns/indexes 的数据能够被 insert/delete/update 操作修改；但是这些数据不可以其他事务被读取。 定义3: 一个只可写的（Write-only）constraints 是对新写请求 insert/delete/update 应用的，但是不保证所有数据都满足约束。 2.2 状态一致性 任何从 schema S1 至 S2 的直接结构型元素变更，如果其添加或删除了一项 public schema 元素 E，此变更不能保持一致性。以添加 E 的情况，不论 E 是 table, column 或 index ，由 S2 定义的 insert 都将插入 S1 未定义的数据，经过 S1 的删除会导致 E 的数据残留，所以 S1 至 S2 的变更不能保持一致性。\n但是，如果其中间添加了一项 delete-only schema 元素 E 的状态将 S1 和 S2 分离，可以保证同时对于 S1 和 S2 状态避免孤立数据异常或完整性异常，进而此两两状态变更过程保持一致。更细节的说明见论文定义和证明，paper 就是花了很多理论来证明这个结论。\n当数据库在 schema（中间）状态能满足了上述定义约束后，通过这些状态去转移 schema 状态，分布式系统的数据与 schema 的一致性能够获得保证，\n所有类型 DDL 操作的状态转移总结为下图，其中：Optional 是指该 schema 元素是可选的，不一定需要存在的；Required 是指该 schema 元素是必须存在的。\n2.3 案例 文章中的例子：\n1 2 Add an index: absent(schema 中 index 不存在) --\u0026gt; delete only --\u0026gt; write only - (data reorg) --\u0026gt; public(schema 中 index 可公开访问) 由于系统允许最多只有两个 schema，因此（对 index element）存在的中间状态集合只有:\n1 2 3 4 5 6 7 8 9 10 11 12 13 1) absent + delete only： 这一过程中完成 delete only 状态变更的节点，不再能够写入老结构元素，但同时这一过程不会新出现与此索引相关的任何数据记录； 2) delete only： 所有节点进入 delete only 状态，表明整个系统不会再新增出现任一相应老结构元素的数据记录，并且不再使用 absent 状态的 schema； 3) delete only + write only： 这一过程中完成 write only 状态变更的节点，开始出现新结构元素对应的数据记录，不会有 absent 的 schema 访问到不一致数据记录； 由于不再存在 absent 状态，所有 delete-only 和 write-only 状态的节点能够保证删除记录正确，不会有删除操作因为 schema 元素缺失而残留的索引元素记录； 4) write only + (data reorg) 因为前面 delete-only 的存在，保证了系统不会存在多余的索引元素记录；当所有 delete only 状态转移到 write only 状态，从这时起，所有节点的数据的变更都能正确地更新索引。 之后进入 data reorg 阶段，reorg 要做的就是取到当前时刻的 snapshot，为每条数据补写对应的索引记录。 reorg 开始之后数据可能发生变更，这种情况下底层 Spanner 提供的一致性能保证（时间戳判断），reorg 的写入操作要么失败（说明新数据已提前写入），要么被更新数据覆盖。 5) write only + public 节点对应 reorg 完成时，索引重建完成，进入 public 状态。 版权声明：如需转载或引用，请附加本文链接并注明来源。 ","permalink":"https://mzyee.github.io/posts/paper_read/f1_online_async_sc/","summary":"Online, Asynchronous Schema Change in F1","title":"Online, Asynchronous Schema Change in F1"},{"content":"行记录多版本的实现逻辑 在初步了解 undo 系统 和 purge 系统后，我们来进一步了解 InnoDB MVCC 逻辑。\nInnoDB MVCC 基于 Undo log 实现，通过主键记录上由 roll_ptr 串联的 undo reocrd 来构建访问需要的历史 record 版本。InnoDB 表数据组织方式是主键聚簇索引，通过 undo 构建老版本的逻辑也只是对于主键索引而言的，二级索引不存在对应 undo record。InnoDB 二级索引通过索引键值加主键值组合来唯一确定一条记录，因此对于一条二级索引记录（包括 delete_mark 状态的），其对应了一条 undo 覆盖历史范围存在的主键记录（可能是当前存在的版本，也可能是构建的历史版本）。\n同时，访问通过 ReadView 来判断历史版本的数据可见性。对于主键记录实际上是判断历史主键 record 上的 tid。因此，当二级索引记录无法通过其 page 上的 max tid 过滤时，需要找到（可能需要通过 undo 构建）其对应的主键记录版本再来判断可见性。\nInnoDB 通过函数 trx_undo_prev_version_build 构建聚集索引记录的前一个版本，这个函数主要会使用在：\nMVCC 读取路径（row_vers_build_for_[semi_]consistent_read）； Purge/Undo 路径（row_vers_old_has_index_entry）； 二级索引隐式锁判断（row_vers_find_matching） 构建老版本记录 trx_undo_prev_version_build 用来构建前一个版本的主键索引记录\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 /** 构建聚集索引记录某个版本 rec 的再前一个版本。 调用者必须持有聚集索引记录索引页的锁。*/ bool trx_undo_prev_version_build(const rec_t *index_rec, mtr_t *index_mtr, const rec_t *rec, const dict_index_t *index, ulint *offsets, mem_heap_t *heap, rec_t **old_vers, mem_heap_t *v_heap, const dtuple_t **vrow, ulint v_status, lob::undo_vers_t *lob_undo) { trx_undo_rec_t *undo_rec = nullptr; dtuple_t *entry; trx_id_t rec_trx_id; ulint type; undo_no_t undo_no; table_id_t table_id; trx_id_t trx_id; roll_ptr_t roll_ptr; upd_t *update = nullptr; byte *ptr; ulint info_bits; ulint cmpl_info; bool dummy_extern; byte *buf; roll_ptr = row_get_rec_roll_ptr(rec, index, offsets); *old_vers = nullptr; /* insert undo（说明是串上第一个记录）*/ if (trx_undo_roll_ptr_is_insert(roll_ptr)) { return true;} rec_trx_id = row_get_rec_trx_id(rec, index, offsets); bool is_temp = index-\u0026gt;table-\u0026gt;is_temporary(); // 获取 undo_rec 时会判断 rec_trx_id 是否被 purge_sys-\u0026gt;view 可见 if (trx_undo_get_undo_rec(roll_ptr, rec_trx_id, heap, is_temp, index-\u0026gt;table-\u0026gt;name, \u0026amp;undo_rec)) { // 当前 rec_trx_id 在 purge_sys-\u0026gt;view 可见，更老的（prev）undo 可能版本都被处理了 if (v_status \u0026amp; TRX_UNDO_PREV_IN_PURGE) { /* 函数被 purge 流程中调用的特殊情况，用于 virtual row 处理 */ undo_rec = trx_undo_get_undo_rec_low(roll_ptr, heap, is_temp); } else { /* 正常情况，更老一个版本的 undo 不安全，到当前版本为止 */ return false; } } // 解析获取到的对应上一版本的 undo rec type_cmpl_t type_cmpl; ptr = trx_undo_rec_get_pars(undo_rec, \u0026amp;type, \u0026amp;cmpl_info, \u0026amp;dummy_extern, \u0026amp;undo_no, \u0026amp;table_id, type_cmpl); if (table_id != index-\u0026gt;table-\u0026gt;id) return true; /*table 被重建，purge 遇到老 id 的 undo*/ ptr = trx_undo_update_rec_get_sys_cols(ptr, \u0026amp;trx_id, \u0026amp;roll_ptr, \u0026amp;info_bits); ptr = trx_undo_rec_skip_row_ref(ptr, index); // 通过 undo 构建 upd_t *update ptr = trx_undo_update_rec_get_update(ptr, index, type, trx_id, roll_ptr, info_bits, heap, \u0026amp;update, lob_undo, type_cmpl); ut_a(ptr); if (row_upd_changes_field_size_or_external(index, offsets, update)) { /* 如果前一个版本记录是被标记删除的，并且存在 disowned 的 blob， 则需要判断可见性这个版本记录的可见性， 如果 purge 可见，将其视为 missing history 处理， 这是因为上一版本记录 disowned 的 blob 可能已经被 purge 了。 可以省略 row_upd_changes_disowned_external(update) 调用， 但这样 purge_sys-\u0026gt;latch 加锁更多，性能开销可更高。*/ if ((update-\u0026gt;info_bits \u0026amp; REC_INFO_DELETED_FLAG) \u0026amp;\u0026amp; row_upd_changes_disowned_external(update)) { bool missing_ext; rw_lock_s_lock(\u0026amp;purge_sys-\u0026gt;latch, UT_LOCATION_HERE); missing_ext = purge_sys-\u0026gt;view.changes_visible(trx_id, index-\u0026gt;table-\u0026gt;name); rw_lock_s_unlock(\u0026amp;purge_sys-\u0026gt;latch); if (missing_ext) { /* treat as a fresh insert, not to cause assertion error at the caller. */ if (update != nullptr) { update-\u0026gt;reset(); } return true; } } // 通过 undo 还原 entry = row_rec_to_index_entry(rec, index, offsets, heap); row_upd_index_replace_new_col_vals(entry, index, update, heap); buf = static_cast\u0026lt;byte *\u0026gt;(mem_heap_alloc(heap, rec_get_converted_size(index, entry))); *old_vers = rec_convert_dtuple_to_rec(buf, index, entry); } else { buf = static_cast\u0026lt;byte *\u0026gt;(mem_heap_alloc(heap, rec_offs_size(offsets))); *old_vers = rec_copy(buf, rec, offsets); // 通过 undo 还原 row_upd_rec_in_place(*old_vers, index, offsets, update, nullptr); } /* Set the old value (which is the after image of an update) in the update vector to dtuple vrow */ if (v_status \u0026amp; TRX_UNDO_GET_OLD_V_VALUE) row_upd_replace_vcol((dtuple_t *)*vrow, index-\u0026gt;table, update, false, nullptr, nullptr); if (vrow \u0026amp;\u0026amp; !(cmpl_info \u0026amp; UPD_NODE_NO_ORD_CHANGE)) { // 构建老版本的 virtual row // ... } if (update != nullptr) { update-\u0026gt;reset(); } return true; } MVCC 一致性读取路径 以 row_search_mvcc 查询为例，当 cursor 定位到确切 user_record 上后，如果是无锁一致性 MVCC 读并且隔离级别大于 READ_UNCOMMITTED 时，此时会通过查询所持有的 readview 判断对应记录是否可见（lock_clust/sec_rec_cons_read_sees）。对于主键索引，如果此记录不可见，则会通过 row_vers_build_for_consistent_read 尝试构建目标 readview 可见的历史行记录。对于二级索引无法判断可见性的，还需要回表到主键上按主键逻辑处理，\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 /** 构建一致性读取应该看到的聚集索引记录的版本。 函数假设当前 rec 上的事务 id 是一致性读取不应该看到的。*/ dberr_t row_vers_build_for_consistent_read( const rec_t *rec, // 聚集索引中的记录，调用者必须持有 page latch，即该记录版本堆栈的顶部 mtr_t *mtr, // 持有 rec 上锁的 mtr，并且还将持有 purge_view 上的锁 dict_index_t *index, ulint **offsets, ReadView *view, // 目标视图 mem_heap_t **offset_heap, mem_heap_t *in_heap, rec_t **old_vers, const dtuple_t **vrow, lob::undo_vers_t *lob_undo // 如果需要应用 blob 的 undo log ) { const rec_t *version; rec_t *prev_version; trx_id_t trx_id; mem_heap_t *heap = nullptr; byte *buf; dberr_t err; trx_id = row_get_rec_trx_id(rec, index, *offsets); if (lob_undo != nullptr) { lob_undo-\u0026gt;reset(); } version = rec; for (;;) { mem_heap_t *prev_heap = heap; heap = mem_heap_create(1024, UT_LOCATION_HERE); if (vrow) { *vrow = nullptr; } bool purge_sees = trx_undo_prev_version_build(rec, mtr, version, index, *offsets, heap, \u0026amp;prev_version, nullptr, vrow, 0, lob_undo); err = (purge_sees) ? DB_SUCCESS : DB_MISSING_HISTORY; // purge view（也是最老的 readview）可见，因此 undo 可能被 purge if (prev_heap != nullptr) { mem_heap_free(prev_heap); } if (prev_version == nullptr) { /* 不存在更老的对应主键版本 */ *old_vers = nullptr; break; } *offsets = rec_get_offsets(prev_version, index, *offsets, ULINT_UNDEFINED, UT_LOCATION_HERE, offset_heap); trx_id = row_get_rec_trx_id(prev_version, index, *offsets); if (view-\u0026gt;changes_visible(trx_id, index-\u0026gt;table-\u0026gt;name)) { /* 一直找到第一个 view 可见的目标历史行记录后 copy 并退出 */ buf = static_cast\u0026lt;byte *\u0026gt;(mem_heap_alloc(in_heap, rec_offs_size(*offsets))); *old_vers = rec_copy(buf, prev_version, *offsets); if (vrow \u0026amp;\u0026amp; *vrow) { *vrow = dtuple_copy(*vrow, in_heap); dtuple_dup_v_fld(*vrow, in_heap); } break; } version = prev_version; } mem_heap_free(heap); return err; } Purge/Undo 路径 当需要从二级索引 purge 标记删除的行时，会检查是否存在一个未被标记删除的、大于等于 purge view 范围的、且与 purge 目标二级索引记录字符集排序相等的主键行记录版本，如果存在（当前二级索引还会使用），则不会 purge 这个二级索引记录。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 /** 检查是否存在某个大于等于 purge view 的非标记删除行记录版本（不可清理）和目标二级索引一致 */ bool row_vers_old_has_index_entry( bool also_curr, /*!\u0026lt; in: true if also rec is included in the versions to search; otherwise only versions prior to it are searched */ const rec_t *rec, /*!\u0026lt; in: record in the clustered index; the caller must have a latch on the page */ mtr_t *mtr, /*!\u0026lt; in: mtr holding the latch on rec; it will also hold the latch on purge_view */ dict_index_t *index, /*!\u0026lt; in: the secondary index */ const dtuple_t *ientry, /*!\u0026lt; in: the secondary index entry */ roll_ptr_t roll_ptr, /*!\u0026lt; in: roll_ptr for the purge record */ trx_id_t trx_id) /*!\u0026lt; in: transaction ID on the purging record */ { const rec_t *version; rec_t *prev_version; dict_index_t *clust_index; ulint *clust_offsets; mem_heap_t *heap; mem_heap_t *heap2; dtuple_t *row; const dtuple_t *entry; ulint comp; const dtuple_t *vrow = nullptr; mem_heap_t *v_heap = nullptr; const dtuple_t *cur_vrow = nullptr; clust_index = index-\u0026gt;table-\u0026gt;first_index(); comp = page_rec_is_comp(rec); heap = mem_heap_create(1024, UT_LOCATION_HERE); clust_offsets = rec_get_offsets(rec, clust_index, nullptr, ULINT_UNDEFINED, UT_LOCATION_HERE, \u0026amp;heap); if (dict_index_has_virtual(index)) v_heap = mem_heap_create(100, UT_LOCATION_HERE); // also_curr == true，检查非 delete_mark 的当前 rec 是否和二级索引匹配 if (also_curr \u0026amp;\u0026amp; !rec_get_deleted_flag(rec, comp)) { row_ext_t *ext; row = row_build(ROW_COPY_POINTERS, clust_index, rec, clust_offsets, nullptr, nullptr, nullptr, \u0026amp;ext, heap); if (dict_index_has_virtual(index)) { // 存在 virtual row 处理 ... } else { // 构建二级索引对应 dtuple_t *entry entry = row_build_index_entry(row, ext, index, heap); // 字符集（非binary）比较输入目标 ientry 和主键二级索引部分是否一致 if (entry \u0026amp;\u0026amp; dtuple_coll_eq(entry, ientry)) { mem_heap_free(heap); if (v_heap) { mem_heap_free(v_heap); } return true; } } } else if (dict_index_has_virtual(index)) { // 存在 virtual row 处理 ... } version = rec; // 检查是否存在非 delete_mark 的老版本 rec 和二级索引匹配 for (;;) { heap2 = heap; heap = mem_heap_create(1024, UT_LOCATION_HERE); vrow = nullptr; // 构建前一个版本行记录 trx_undo_prev_version_build( rec, mtr, version, clust_index, clust_offsets, heap, \u0026amp;prev_version, nullptr, dict_index_has_virtual(index) ? \u0026amp;vrow : nullptr, 0, nullptr); mem_heap_free(heap2); if (!prev_version) { /* 不存在更老的（purge 安全）版本 */ mem_heap_free(heap); if (v_heap) mem_heap_free(v_heap); return false; } clust_offsets = rec_get_offsets(prev_version, clust_index, nullptr, ULINT_UNDEFINED, UT_LOCATION_HERE, \u0026amp;heap); if (dict_index_has_virtual(index)) { /* 存在 virtual row 处理 ... */ } if (!rec_get_deleted_flag(prev_version, comp)) { row_ext_t *ext; row = row_build(ROW_COPY_POINTERS, clust_index, prev_version, clust_offsets, nullptr, nullptr, nullptr, \u0026amp;ext, heap); if (dict_index_has_virtual(index)) {/* 存在 virtual row 处理 ... */} // 构建二级索引对应 dtuple_t *entry entry = row_build_index_entry(row, ext, index, heap); /* If entry == NULL, the record contains unset BLOB pointers. This cheated be a freshly inserted record that can ignore. */ // 字符集（非binary）比较输入目标 ientry 和主键二级索引部分是否一致 if (entry \u0026amp;\u0026amp; dtuple_coll_eq(entry, ientry)) { mem_heap_free(heap); if (v_heap) { mem_heap_free(v_heap); } return true; } } version = prev_version; } } 二级索引隐式锁判断 在做二级索引记录可见下判断时，当无法使用二级索引 page max tid 做过滤时，需要回表主键去检查是否是当前存在的活跃事务插入或修改了对应的二级索引记录。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 /* 查找是否存在当前 tid 产生的、关联 sec rec 的主键记录版本。 */ static bool row_vers_find_matching( bool looking_for_match, const dict_index_t *const clust_index, const rec_t *const clust_rec, ulint *\u0026amp;clust_offsets, const dict_index_t *const sec_index, const rec_t *const sec_rec, const ulint *const sec_offsets, const bool comp, const trx_id_t trx_id, // 当前 cluster rec 对应的 tid，必然是活跃的 mtr_t *const mtr, mem_heap_t *\u0026amp;heap) { const rec_t *version = clust_rec; trx_id_t version_trx_id = trx_id; // 这里是回表到的当前索引上 cluster_rec 对应 tid 还活跃，但是 sec_rec 可能和不是这个 tid 产生的，仍要进一步判断 // 只需要寻找当前索引 cluster_rec 对应 tid 产生的记录，如果是更老的 tid 产生的这个 sec_rec，由于这个行记录可以被后续 tid 修改，其一定已经不活跃了 while (version_trx_id == trx_id) { mem_heap_t *old_heap = heap; const dtuple_t *clust_vrow = nullptr; rec_t *prev_version = nullptr; heap = mem_heap_create(1024, UT_LOCATION_HERE); // 寻找前一个主键记录版本 trx_undo_prev_version_build( clust_rec, mtr, version, clust_index, clust_offsets, heap, \u0026amp;prev_version, nullptr, dict_index_has_virtual(sec_index) ? \u0026amp;clust_vrow : nullptr, 0, nullptr); mem_heap_free(old_heap); version = prev_version; if (version == nullptr) { version_trx_id = 0; } else { clust_offsets = rec_get_offsets(version, clust_index, nullptr, ULINT_UNDEFINED, UT_LOCATION_HERE, \u0026amp;heap); version_trx_id = row_get_rec_trx_id(version, clust_index, clust_offsets); } /* NOTE: 这里需要判断是否由对应版本主键 prev version 到这个版本这次 “修改” 而产生的 sec_rec： sec_rec 是 non-delete marked（looking_for_match = false）： 如果发现 prev version 是 delete mark 的或 version 二级索引部分与 sec_rec 不一致（不 match）， 说明是由这个版本 version 修改产生的这个 sec_rec（老版本被更新过）， 因此仍活跃。 （sec_rec 活跃，prev version 非活跃 或 不一致 时，则说明对 prev version 的修改导致此 sec_rec 产生） sec_rec 是 delete marked（looking_for_match = true）： 如果发现 prev version 是 非 delete mark 且 version 二级索引部分与 sec_rec 一致（match）， 说明是由这个版本 version 修改产生的这个 sec_rec（老版本被更新过）， 因此仍活跃。， （sec_rec 非活跃，prev version 活跃 且 一致 时，则说明对 prev version 的修改导致此 sec_rec 产生） */ if (row_clust_vers_matches_sec( clust_index, version, clust_vrow, clust_offsets, sec_index, sec_rec, sec_offsets, comp, looking_for_match, heap) == looking_for_match) { return true; } } return false; } 版权声明：如需转载或引用，请附加本文链接并注明来源。 ","permalink":"https://mzyee.github.io/posts/mysql/mvcc/","summary":"InnoDB MVCC 逻辑学习","title":"InnoDB MVCC 逻辑学习"},{"content":" 1. Docker 介绍 “Docker is a platform designed to help developers build, share, and run container applications.”\nDocker daemon (dockerd)： Docker 守护进程，侦听 Docker API 请求并管理 Docker 对象，例如镜像、容器、网络和卷。守护进程还可以与其他守护进程通信来管理 Docker 服务。\nDocker client (docker)： Docker 客户端，是许多 Docker 用户与 Docker 交互的主要方式。当使用诸如 docker run 命令时，client 会将这些命令发送到 dockerd，由后者执行这些命令。docker 命令使用的 Docker API。Docker 客户端可以与多个守护进程通信。\nDocker registries： Docker 仓库用于存储 Docker 镜像 (images)。Docker Hub 是任何人都可以使用的公共仓库，Docker 默认在 Docker Hub 上查找镜像。用户可以运行自己的私人仓库。当使用 docker pull 或 docker run 命令时，Docker 从配置的仓库中提取所需的镜像。当使用该 docker push 命令时，Docker 会将的镜像推送到配置的仓库。\nDocker objects： 使用 Docker 时 实际上是在创建和使用镜像、容器、网络、存储卷、插件等对象。\n镜像（Image） 是用于创建 docker 容器的模板程序，包含了运行应用所需的所有东西 —— 包括代码、二进制脚本、依赖库、环境变量和配置文件等等。 容器（Container） 是镜像的运行实例，它在隔离的环境中运行不与主机其他容器共享状态，可以使用 Docker API 创建、启动、停止、移动或删除容器，也可以将容器连接到一个或多个网络，为其附加存储，甚至可以根据其当前状态创建一个新的镜像。它与该宿主机上运行的所有其他进程隔离开来，这种隔离利用了内核名称空间和cgroups。 存储 在 docker 中，存储是管理和持久化数据的关键组成部分。Docker 为数据存储和管理提供了几种机制，主要是通过卷（volumes）、绑定挂载（bind mounts）和临时文件系统（tmpfs mounts）。Docker提供了卷和绑定挂载两种选项（对应不同存储定义区），使得容器可以将文件存储在宿主机上，这样即使容器停止文件也会持久保存。docker 还支持容器将临时文件存储在宿主机的内存中，这类文件不会被持久保存。 网络 容器网络指的是容器之间能够连接和通信，或者与非 docker 工作负载通信的能力。容器默认启用了网络功能可以建立外部连接，容器不知道自己连接到了什么类型的网络，只看到一个带有 IP 地址、网关、路由表、DNS 服务和其他网络详细信息的网络接口。Docker 提供了几种不同的网络驱动程序，每种驱动程序都支持不同的用例和网络操作模式，默认使用 bridge 类型驱动，它在没有指定网络的容器时会自动连接到本地主机上的一个私有内部网络。 2. Docker 使用基础 2.1 安装 以 CentOS 为例，安装、启动 docker\n1 2 3 4 sudo yum install -y yum-utils # 设置 docker 目标仓库并且从仓库安装 sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo sudo yum install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin 1 2 3 # 启动Docker服务并设置为开机启动 sudo systemctl start docker sudo systemctl enable docker 默认情况下，只有 root 用户和 sudoer 才可以运行 Docker 命令（普通用户会报 Docker daemon 等错误）。为了避免每次都使用 sudo，可以将用户添加到 docker 组后重新登录。\n1 sudo usermod -aG docker ${USER} 2.2 运行容器 通过 docker run 可以运行已有镜像创建一个新的容器，并且在创建容器后会立即启动它。docker create 命令也可以创建容器但不启动，创建容器后可以使用 docker start 命令启动。\n1 2 3 4 docker run [OPTIONS] IMAGE[:TAG|@DIGEST] [COMMAND] [ARG...] # [OPTIONS] 是容器的相关配置信息 # IMAGE[:TAG|@DIGEST] 是镜像标志名 # [COMMAND] [ARG...] 是设定容器启动后运行的命令 常见的可选运行 [OPTIONS] 有\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 ### 存储卷 --mount source=\u0026lt;VOLUME_NAME\u0026gt;,target=[PATH] #绑定 Volume，source 是 volume name，target 是容器内的挂载路径 --mount type=bind,source=[PATH],target=[PATH] # 创建 Bind mounts，source 是 host 路径，target 是容器内的 bind 路径 -v/--volume host-src:container-dest[:options] # Volume/Bind mounts 都可以用这个命令挂载，效果与上面相同 ### 网络设置 --network NETWORK_NAME # NETWORK_NAME 是事先用户创建的网络 --network container:\u0026lt;name|id\u0026gt; # 与另外一个 container 网络直连 --publish/-p -p 192.168.1.100:8080:80 # 将主机 IP 192.168.1.100 的 8080 端口映射到 docker 的 80 端口 -p 8080:80/tcp -p 8080:80/udp # 将 host 主机 的 8080 tcp/udp 端口映射到 docker 的 80 tcp/udp 端口 ### 资源限制 -m/--memory=\u0026#34;\u0026#34; # 内存限制 --memory-swap=\u0026#34;\u0026#34; # 内存+swap限制 --cpus=0.000 # cpu 数目，0.000表示 不限制 --cpuset-cpus=\u0026#34;\u0026#34; # 目标运行 cpu --cpu-quota= --cpu-period # --cpu-quota 选项允许您指定容器可以获取的 CPU 时间周期的配额，与 --cpu-period 选项一起使用：例如 --cpu-quota 被设置为 50,000（微秒），并且 --cpu-period 是默认的 100,000（微秒），则表示该容器在每个 100 毫秒的 CPU 周期内能使用的 CPU 时间被限制在 50 毫秒内（cpu 利用率限制在 50%）。 --blkio-weight # 限制 direct io的贷款，是一个权重值，范围 10-1000 --rm # 在容器退出时自动清除容器的文件系统。不使用该参数时，容器的文件系统将保留在系统中，允许你稍后可以重新启动容器或检查其状态。 -d/--detach # 直接分离，在后台运行容器 --detach-keys # 参数允许重定义默认的热键，用于从容器的交互模式中分离出来，而不是用于终止容器 -w/--workdir # 设置容器内的工作目录，当启动容器时，任何相对路径的命令或指令都将基于这个工作目录执行 -i/--interactive # 保持 STDIN（标准输入）流开放，允许容器接收来自用户的输入 -t/--tty # 为容器分配一个伪终端或 pseudo-TTY，模拟了终端设备，提供一个可以执行输入输出的文本界面 -it # 结合使用 -it 参数时，允许与运行在容器中的命令行界面互动，就像是在使用一个交互式 shell 对话。 2.3 客户端命令 docker 将默认配置文件保存在 /$HOME/.docker 下，运行时配置可以通过其中 config.json 文件进行配置。 通过 docker 客户端命令，可以管理容器的各种资源对象，其中一些客户端命令如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 #### 基本管理 docker info # 查看当前版本、仓库等信息 docker login # 登录 Docker 仓库进行认证，以允许拉取或推送镜像。 docker logout # 从当前登录的仓库登出。 docker search # 根据给定的条件在 Docker 仓库上搜索镜像。 #### 镜像管理 docker images # 提供用于管理 Docker 镜像的子命令，如列出、删除和标记镜像。 docker builder # 处理构建的生命周期。 docker buildx # 扩展的构建命令，支持比如构建多架构镜像。 #### 容器管理 docker container # 包含用于管理容器的子命令，包括创建、启动、停止、移动和删除容器。 docker checkpoint # 管理检查点；检查点可以捕获正在运行的容器的状态。 docker exec # 用于进入在已运行的容器内执行命令。 docker ps # 列出所有容器，包括未运行的。 docker stop # 停止一个或多个正在运行的容器。 docker start # 启动一个或多个已停止的容器。 docker restart # 重新启动容器。 docker rm # 删除一个或多个容器。 docker logs # 查看容器日志，dockerd的日志 linux 系统在 /var/log/messages 中 ####其他管理 docker system # 清理未使用的数据并显示 Docker 的系统范围内的信息。 docker inspect # 查看 Docker 对象（容器，镜像，卷等）的低级信息。 docker init # 为项目创建与 Docker 相关的启动文件，帮助快速设置。 #### 网络和卷管理 docker network # 管理网络。 docker volume # 管理卷。 #### Docker Swarm docker service # 管理 Docker 服务，是 Docker Swarm 缩放功能的基础单位。 docker node # 管理 Docker Swarm 集群中的节点。 docker swarm # 管理 Swarm，Docker 的本地编排和集群工具。 docker config # 管理 Docker Swarm 配置，允许存储服务的配置文件。 docker context # 管理上下文，允许 Docker 根据指定的上下文使用不同的配置。 3. 创建镜像 3.1 Dockerfile Dockerfile 是一个文本文件，里面包含了一系列的指令和参数，用于定义如何构建一个 Docker 镜像，在 Dockerfile 中定义的每条指令将在构建过程中顺序执行，并每次形成一个layer。Dockerfile 的相关语法有：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 FROM\t# 指定基础镜像（必需的指令，是构建自定义镜像的出发点） COPY\t# 从构建上下文复制新文件或目录到容器中的路径。 ADD\t# 类似于 COPY，但还可以支持自动解压缩附加在 URL 上的存档文件 ARG\t# 定义在构建时可以传递给运行时的参数 CMD\t# 提供容器启动时的默认执行命令 ENTRYPOINT # 配置容器启动时运行的命令，允许该容器像程序一样被运行 ENV\t# 设置环境变量 EXPOSE\t# 告知 Docker 在容器运行时需要监听的端口 LABEL\t# 添加元数据到镜像中，如描述、作者信息等 ONBUILD\t# 添加在构建指定的派生镜像时被执行的触发指令 RUN\t# 运行指定的命令并创建镜像层，常用于安装软件包 SHELL\t# 设置镜像的默认 shell STOPSIGNAL # 设定退出镜像的 system call signal USER\t# 设置 user 和 group ID VOLUME\t# 声明容器内的挂载点，主要用于持久化或共享数据 WORKDIR\t# 切换工作目录（路径） 一个 Dockerfile 的例子\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 # syntax=docker/dockerfile:1 FROM ubuntu:22.04 # install app dependencies RUN apt-get update \u0026amp;\u0026amp; apt-get install -y python3 python3-pip RUN pip install flask==3.0.* # install app COPY hello.py / # final configuration ENV FLASK_APP=hello EXPOSE 8000 CMD [\u0026#34;flask\u0026#34;, \u0026#34;run\u0026#34;, \u0026#34;--host\u0026#34;, \u0026#34;0.0.0.0\u0026#34;, \u0026#34;--port\u0026#34;, \u0026#34;8000\u0026#34;] 3.2 构建镜像 通过 docker build 命令可以构建镜像，这个命令也等同于 docker image build、docker buildx build、docker builder build等命令。\n1 2 docker build [OPTIONS] PATH | URL | - ^^^^context^^^^ 除了 Dockerfile 外，还有一个概念是 build “context”。Context 上下文是位于指定 PATH 或 URL 中的一组文件，构建过程可以引用上下文中的任何文件。例如，构建可以使用 COPY 指令来引用上下文中的文件。\n常见的可选构建 [OPTIONS] 有\n1 2 3 -t/--tag\t# 设置镜像 name 和可选的 tag，参数格式是 image_name:tag -f/--file\t# Dockerfile 的名字，默认为当前路径下的 \u0026#34;Dockerfile\u0026#34; --build-arg\t# build 时的环境变量，多个需要多组此参数 --build-arg VAR1=123 3.3 推送、拉取镜像 通过 docker login 可以登录到指定仓库，再通过 docker push 和 pull 推送或拉取指定镜像。\n1 2 3 docker login myregistry.com docker push myregistry.example.com/myimage:latest docker pull myregistry.example.com/myimage:latest Docker 默认使用 Docker Hub 作为仓库，如果想改变默认仓库，需要在 Docker 配置中设置 \u0026ndash;registry-mirror 选项。通常通过编辑 Docker 守护进程的配置文件 daemon.json 来完成。文件的默认位置通常在 /etc/docker/daemon.json，修改其中的相关参数。更改配置后，需要重启 Docker 服务以使更改生效 sudo systemctl restart docker。\n1 2 3 { \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;https://\u0026lt;myregistry.example.com\u0026gt;\u0026#34;] } 4. 容器操作 容器相关的一些实用操作命令\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # 查看容器 docker ps -a # 查看容器文件系统的修改 docker diff CONTAINER # 查看容器的详细信息 docker inspect CONTAINER # 查看容器的日志 docker logs CONTAINER # 查看容器内程序的运行状态 docker top CONTAINER # 启动、停止、重启容器 docker start|stop|restart CONTAINER # 在主机和容器间copy文件 docker cp [OPTIONS] CONTAINER:SRC_PATH DEST_PATH docker cp [OPTIONS] SRC_PATH CONTAINER:DEST_PATH # 在一个运行容器中执行(bash)命令 docker exec -it CONTAINER bash 版权声明：如需转载或引用，请附加本文链接并注明来源。 ","permalink":"https://mzyee.github.io/posts/linux/docker/","summary":"Docker 使用简介","title":"Docker 使用简介"},{"content":"1. 前言 数据库系统的并发控制与其多个模块相关联，其用于实现无冲突、保持一致性的多事务并发操作。并发控制模块需要保证事务并发执行的效果与要求的串行执行模式的效果完全相同，以达到隔离性的要求，不合理的并发控制可能导致更新丢失、不可重复读、脏读、幻读等问题。\n目前常用的并发控制协议可被粗略并不完整的分为悲观和乐观两大类：此处乐观协议指狭义乐观并发控制，如指基于完成有效性验证的并发控制，主要为前向 OCC 和后向 OCC 两类；而悲观协议又分为基于锁和非锁两大类，基于锁的协议有 2-Phase Locking、Altruistic Locking、Read/Write Tree Locking 等，非锁类协议有基于基于事务串行化图的协议等。此外，还可以进一步可以结合多版本并发控制（MVCC），以提高数据库的（读）性能。\n2. InnoDB 事务锁 2.1 两阶段锁（2PL Locking） 这里介绍最常用的为 2PL 协议，这也是 MySQL 使用的机制。2PL 将事务的执行阶段分为三个不同的部分：\n1）第一阶段，事务开始执行时，事务开始获取需要的锁的权限，但不释放锁； 2）第二部分是事务获取所有锁的过程； 3）当事务释放它的第一个锁时，第三阶段开始，此阶段事务不能要求任何新锁，只释放获取的锁。 从上可以看出 2PL 分为 Growing phase 和 Shrinking phase 两个阶段。根据锁定细节还可演化为 Conservative-2PL、Strict-2PL、Rigorous-2PL 等。2PL 协议提供了可序列化性，但不能保证不发生死锁，因此需要进行死锁预防或死锁检测。MySQL 采用的是死锁检测加超时恢复的机制，死锁检测一般通过 waits-for 图结构实现：图中点代表事务，有向边代表事务在等待另一个事务放锁，当 waits-for 图出现环时说明有死锁出现。此时需要进行死锁恢复，最常见的解决办法就是选择整个环中一个事务进行回滚，以打破整个等待图中的环，此时需要考虑如何选择回滚以避免饥饿并最小化代价。\n2.2 InnoDB 中的 Lock 目前 InnoDB 层主要有 LOCK_TABLE 和 LOCK_REC 两大类，后者提供更细粒度的锁操作。行锁以 scope 划分细节类型，有 LOCK_ORDINARY（next-key，记录及其前间隙的组合锁），LOCK_GAP（记录值的前间隙锁），LOCK_REC_NOT_GAP（单记录锁），LOCK_INSERT_INTENTION（为插入行为的gap锁）等。其次，行锁的锁定模式为 LOCK_IS、LOCK_IX、LOCK_S、LOCK_X、LOCK_AUTO_INC 等。\nInnoDB 的所有事务锁对象挂在全局对象 lock_sys_t 上，同时每个事务对象trx_t 上也维持了其拥有的事务锁，每个表对象 dict_table_t 上维持了其上的表级锁。8.0.21 版本还对 record hash 做 mutex 分片拆减少瓶颈。在一个系统中，每个表和每一行都可以被视作一种资源，并且事务在请求资源访问权限时会指定一个模式（mode），以表明它打算如何使用该资源。例如，LOCK_X 模式意味着事务需要排他性访问，而 LOCK_S 模式意味着事务可以与其他使用 LOCK_S 模式的事务共享资源，锁可以是等待状态（WAITING）或已授予状态（GRANTED）。锁系统使用页面编号（page_no，即包含记录的页面的标识符）和在页面内部分配记录数组中的位置（heap_no）来标识行记录。这在 b-tree 合并、分裂或可变长度记录的重新分配时变得很重要，这些操作都需要通知锁系统以反映变化。\n锁系统中的锁包含了元素：\n1）请求锁的事务（requesting transaction）；\n2）资源目标标识（特定的行或表定位）；\n3）锁类型与模式（如 LOCK_X、LOCK_S 等）；\n4）锁状态（WAITING 或 GRANTED）。\n锁的生命周期通常如下：\n1）事务请求锁，如果没有与现有锁冲突则立即授予（GRANTED），否则进入等待状态（WAITING）；\n2）若锁处于 WAITING 状态，则线程（主动）休眠；\n3）WAITING 锁要么在没有冲突的情况下变为 GRANTED，要么在回滚的情况下被取消，同时唤醒原先等待的事务线程；\n4）事务结束（提交或回滚）时释放其所有锁。\n行锁冲突关系通过函数 rec_lock_check_conflict 确定。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 static inline Conflict rec_lock_check_conflict(const trx_t *trx, ulint type_mode, const lock_t *lock2, bool lock_is_on_supremum, Trx_locks_cache \u0026amp;trx_locks_cache) { /* 锁模式的兼容矩阵 IS IX S X AI IS + + + - + IX + + - - + S + - + - - X - - - - - AI + + - - - Note that for rows, InnoDB only acquires S or X locks. For tables, InnoDB normally acquires IS or IX locks, S or X table locks are only acquired for LOCK TABLES. Auto-increment (AI) locks are needed because of statement-level MySQL binlog. */ /* 锁模式兼容 */ if (trx == lock2-\u0026gt;trx || lock_mode_compatible(static_cast\u0026lt;lock_mode\u0026gt;(LOCK_MODE_MASK \u0026amp; type_mode), lock_get_mode(lock2))) { return Conflict::NO_CONFLICT; } /* 这里需要注意一点，新来的锁是需要和对应锁链上所有相同资源目标的锁做兼容比较， 包括原先为 waiting 状态的锁，这样做防止了 X 锁饥饿的问题 */ const bool is_hp = trx_is_high_priority(trx); /* 高优先级是事务可以忽略原先的 waiting 状态的低优先级事务*/ if (is_hp \u0026amp;\u0026amp; lock2-\u0026gt;is_waiting() \u0026amp;\u0026amp; !trx_is_high_priority(lock2-\u0026gt;trx)) { return Conflict::NO_CONFLICT; } /* 要加的锁是 非insert_intention的 上界 或 gap 锁，可以兼容任何锁 */ if ((lock_is_on_supremum || (type_mode \u0026amp; LOCK_GAP)) \u0026amp;\u0026amp; !(type_mode \u0026amp; LOCK_INSERT_INTENTION)) { return Conflict::NO_CONFLICT; } /* 要加 非insert_intention的锁，可以兼容 gap 锁 */ if (!(type_mode \u0026amp; LOCK_INSERT_INTENTION) \u0026amp;\u0026amp; lock_rec_get_gap(lock2)) { return Conflict::NO_CONFLICT; } /* 要加的锁是 gap 锁，可以和 rec_not_gap 兼容 */ if ((type_mode \u0026amp; LOCK_GAP) \u0026amp;\u0026amp; lock_rec_get_rec_not_gap(lock2)) { return Conflict::NO_CONFLICT; } /* 锁链上待比较的锁是 insert_intention 的，可以兼容其他锁 */ if (lock_rec_get_insert_intention(lock2)) { return Conflict::NO_CONFLICT; } /* insert_intention 锁最大的问题是后续插入后会把 lock 区间分裂， 并由单个 lock 产生两个 lock，这样就有可能导致事务 waiting 状态的 gap 或 next-key lock 变成两个 waiting 状态的 lock，违背事务最 多一个 waiting lock 的原则 */ if (!(type_mode \u0026amp; LOCK_INSERT_INTENTION) \u0026amp;\u0026amp; lock2-\u0026gt;is_waiting() \u0026amp;\u0026amp; lock2-\u0026gt;mode() == LOCK_X \u0026amp;\u0026amp; (type_mode \u0026amp; LOCK_MODE_MASK) == LOCK_X) { /* 如果 waiting 状态的锁是被当前 trx 自己已经 granted 的锁给阻塞，则不需要等这个 waiting 状态的锁 */ if (trx_locks_cache.has_granted_blocker(trx, lock2)) { return Conflict::CAN_BYPASS; } } return Conflict::HAS_TO_WAIT; } 2.2 加锁逻辑（案例） 以 sysbench 表结构为例，介绍写入时的加锁逻辑。\n1 2 3 4 5 6 7 8 CREATE TABLE `sbtest1` ( `id` int(11) NOT NULL AUTO_INCREMENT, `k` int(11) NOT NULL DEFAULT \u0026#39;0\u0026#39;, `c` char(120) NOT NULL DEFAULT \u0026#39;\u0026#39;, `pad` char(60) NOT NULL DEFAULT \u0026#39;\u0026#39;, PRIMARY KEY (`id`), KEY `k_1` (`k`) ) ENGINE=InnoDB; 这里先额外说明一下 innodb 在加行锁时一定 cursor 已经定位了（为了确定资源目标标识，即 page_id 和 heap_no），并且获得了 page 页面锁以防止修改。\n首先讨论 insert 场景，insert 流程先主键（row_ins_clust_index_entry）再二级索引（row_ins_sec_index_entry），在每个索引上执行的操作步骤类似：\n定位 cursor 至目标位置（这里还可以获得定位匹配情况）； a）如果是唯一索引（包括主键），并且定位时发现和目标索引上存在 record 和将要插入的记录物理 match，则要走到 record 判重逻辑中（row_ins_duplicate_error_in_clust / row_ins_scan_sec_index_for_duplicate），其中会先对应去加目标行的行锁（lock_clust_rec_read_check_and_lock / lock_sec_rec_read_check_and_lock 两个函数的执行逻辑基本一致），这里可能产生锁等待，再判断重复性是否满足要求； 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 dberr_t lock_clust_rec_read_check_and_lock( const lock_duration_t duration, const buf_block_t *block, const rec_t *rec, dict_index_t *index, const ulint *offsets, const select_mode sel_mode, const lock_mode mode, const ulint gap_mode, que_thr_t *thr) { if (srv_read_only_mode || index-\u0026gt;table-\u0026gt;is_temporary()) return (DB_SUCCESS); dberr_t err; ulint heap_no = page_rec_get_heap_no(rec); // 隐式锁判断：通过 \u0026#34;主键record上面的系统列\u0026#34; 和 \u0026#34;活跃事务数组\u0026#34; 判断对应记录是否存在隐式锁，如有则为其加上显示锁 // 判断二级索引无法过滤时，需要回表 if (heap_no != PAGE_HEAP_NO_SUPREMUM) { lock_rec_convert_impl_to_expl(block, rec, index, offsets); } { locksys::Shard_latch_guard guard{UT_LOCATION_HERE, block-\u0026gt;get_page_id()}; if (duration == lock_duration_t::AT_LEAST_STATEMENT) { lock_protect_locks_till_statement_end(thr); } // 加行锁，不可为隐式锁 err = lock_rec_lock(false, sel_mode, mode | gap_mode, block, heap_no, index, thr); } return (err); } b）如果是非唯一索引，或者未发现已存在匹配 record，则在 insert 阶段可以跳过重复性检查（进而这里也不再需要走加锁逻辑）；\n3. 进行真正的插入操作（btr_cur_optimistic_update / btr_cur_pessimistic_update 这两个接口为通过修改插入； btr_cur_optimistic_insert / btr_cur_pessimistic_insert 这两个接口为直接插入），其中会对应进行行锁操作 （lock_clust_rec_modify_check_and_lock / lock_sec_rec_modify_check_and_lock 通过修改插入； lock_rec_insert_check_and_lock 直接插入）：\n修改插入（update）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 dberr_t lock_clust_rec_modify_check_and_lock( ulint flags, /*!\u0026lt; in: if BTR_NO_LOCKING_FLAG bit is set, does nothing */ const buf_block_t *block, /*!\u0026lt; in: buffer block of rec */ const rec_t *rec, /*!\u0026lt; in: record which should be modified */ dict_index_t *index, /*!\u0026lt; in: clustered index */ const ulint *offsets, /*!\u0026lt; in: rec_get_offsets(rec, index) */ que_thr_t *thr) /*!\u0026lt; in: query thread */ { if (flags \u0026amp; BTR_NO_LOCKING_FLAG) return (DB_SUCCESS); dberr_t err; ulint heap_no = rec_offs_comp(offsets) ? rec_get_heap_no_new(rec) : rec_get_heap_no_old(rec); // 隐式锁判断：通过 \u0026#34;主键record上面的系统列\u0026#34; 和 \u0026#34;活跃事务数组\u0026#34; 判断对应记录是否存在隐式锁，如有则为其加上显示锁； // 判断二级索引无法过滤时，需要回表 lock_rec_convert_impl_to_expl(block, rec, index, offsets); { locksys::Shard_latch_guard guard{UT_LOCATION_HERE, block-\u0026gt;get_page_id()}; // 加行锁，可为隐式锁，即无冲突时不真正加行锁 err = lock_rec_lock(true, SELECT_ORDINARY, LOCK_X | LOCK_REC_NOT_GAP, block, heap_no, index, thr); } if (err == DB_SUCCESS_LOCKED_REC) err = DB_SUCCESS; return (err); } 直接插入（insert）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 dberr_t lock_rec_insert_check_and_lock( ulint flags, /*!\u0026lt; in: if BTR_NO_LOCKING_FLAG bit is set, does nothing */ const rec_t *rec, /*!\u0026lt; in: record after which to insert */ buf_block_t *block, /*!\u0026lt; in/out: buffer block of rec */ dict_index_t *index, /*!\u0026lt; in: index */ que_thr_t *thr, /*!\u0026lt; in: query thread */ mtr_t *mtr, /*!\u0026lt; in/out: mini-transaction */ bool *inherit) /*!\u0026lt; out: set to true if the new inserted record maybe should inherit LOCK_GAP locks from successor record */ { if (flags \u0026amp; BTR_NO_LOCKING_FLAG) return (DB_SUCCESS); dberr_t err = DB_SUCCESS; lock_t *lock; auto inherit_in = *inherit; trx_t *trx = thr_get_trx(thr); const rec_t *next_rec = page_rec_get_next_const(rec); ulint heap_no = page_rec_get_heap_no(next_rec); { locksys::Shard_latch_guard guard{UT_LOCATION_HERE, block-\u0026gt;get_page_id()}; if (lock_rec_get_first(lock_sys-\u0026gt;rec_hash, block, heap_no) == nullptr) { // 锁链上无锁，直接加隐式锁 *inherit = false; } else { *inherit = true; const ulint type_mode = LOCK_X | LOCK_GAP | LOCK_INSERT_INTENTION; // 判断锁链上是否存在冲突模式的锁：如果存在其他后继记录的除 insert 目的外的 gap 锁 const auto conflicting = lock_rec_other_has_conflicting(type_mode, block, heap_no, trx); if (conflicting.wait_for != nullptr) { RecLock rec_lock(thr, index, block, heap_no, type_mode); trx_mutex_enter(trx); // 存在冲突，加行锁 err = rec_lock.add_to_waitq(conflicting.wait_for); trx_mutex_exit(trx); } } } switch (err) { case DB_SUCCESS_LOCKED_REC: err = DB_SUCCESS; case DB_SUCCESS: if (!inherit_in || index-\u0026gt;is_clustered()) break; /* 二级索引更新 page 上的 max trx id 标志 */ page_update_max_trx_id(block, buf_block_get_page_zip(block), trx-\u0026gt;id, mtr); default: break; } return (err); } 更多隐式锁相关内容可以参考这篇 mysql 月报文章。\nupdate 流程会先通过读接口来寻找原先需要被更新的行记录（index_read -\u0026gt; row_search_mvcc），在读阶段就会对目标行进行加锁（lock_clust/sec_rec_read_check_and_lock），这里的加锁接口和 insert 判重阶段一致，具体加锁模式还和隔离级别有关。 update 第二个阶段会真正去修改数据，也有直接 update 模式和 delete_mark + insert 模式，到 btr_cur_ 层的接口一致，因此加锁函数接口也一致。\n2.3 锁的释放 大多数情况下事务持有的所有锁（trx_t::lock.trx_locks）都是在事务提交时释放(trx_release_impl_and_expl_locks-\u0026gt;lock_trx_release_locks，在 trx_commit_in_memory 阶段的开始)。有两个例外：\na）AUTO-INC 锁（由参数 innodb_autoinc_lock_mode 控制）在SQL结束时直接释放（innobase_commit \u0026ndash;\u0026gt; lock_unlock_table_autoinc）；\nb）在 RC 隔离级别下执行 DML 语句时，从引擎层返回到 Server 层的记录，如果不满足 where 条件，则会立刻 unlock 掉（ha_innobase::unlock_row）。\n对于行锁，从 lock sys hash 中删除后还需要判断是否有正在等待的会话可以被唤醒（lock_rec_dequeue_from_page）。这里采用的是 CATS 策略将所有处于 LOCK_WAIT 状态的锁对象按trx优先权、调度权重 trx-\u0026gt;lock.schedule_weight 排序，然后依次处理。对于每个等待状态的锁对象，如果其不再等待任何已有 granted 的锁对象（包括这次循环前面 grant 的），则清理等待事务的相关锁等待状态并唤醒线程已经挂起的等待事务；反之，则更新等待关系、继续等待。\n2.4 死锁检测及CATS策略 官方在 Bug#29882690: UNDETECTED DEADLOCK WITH GAP AND INSERT INTENTION LOCKS IS POSSIBLE 中修复部分原先无法探测的死锁，还将锁阻塞信息（wait-for图）从 lock 转移到 trx 中，并且将死锁检测从前台事务线程转移到后台监控线程。这样减轻了前台事务线程的取锁冲突时的检测代价；但是死锁会先进入事务系统，后续检测出后回滚，也就意味着死锁事务会占用一段时间的线程、内存资源、导致锁链、事务数组等的膨胀后（对应线程先进入、然后 suspend 进入睡眠等待后）才会被回滚，若是死锁概率较高的高冲突场景这可能会导致整体事务系统的性能下降。死锁处理的逻辑在 lock_wait_timeout_thread 线程中（lock_wait_update_schedule_and_check_for_deadlocks），顾名思义这个函数更新了事务权重比检查了是否存在死锁。新版死锁检测逻辑的理论基础可以参考这篇 mysql 月报文章，下面主要介绍代码逻辑。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 /** Takes a snapshot of transactions in waiting currently in slots, updates their schedule weights, searches for deadlocks among them and resolves them. */ static void lock_wait_update_schedule_and_check_for_deadlocks() { ut::vector\u0026lt;waiting_trx_info_t\u0026gt; infos; ut::vector\u0026lt;int\u0026gt; outgoing; ut::vector\u0026lt;trx_schedule_weight_t\u0026gt; new_weights; /* （以下持有 lock_sys-\u0026gt;wait_mutex） 将 lock_sys 中所有 waiting 状态的 trx 及阻塞它的 blocking_trx 加入到 infos，构建 wait trx 的 snapshot； */ auto table_reservations = lock_wait_snapshot_waiting_threads(infos); /* 构建 waiting for graph，实质上就是寻找某个 trx 的 blocking_trx 是否存在在 info 中， blocking_trx 也存在的话通过 outgoing 数组关联位置起来，这样就将所有依赖的 trx 串联起来， (即将这个 trx 在 info 中的 index 位置作为 outgoing index， 将 blocking_trx 在 info 中的 index 位置作为 outgoing 中这个 index 对应位置的元素） */ lock_wait_build_wait_for_graph(infos, outgoing); /* 初始化事务权重值，正常等待事务初始为1，超时事务会初始为较大值； 通过关联树图算出 new_weights（依赖子树元素数目）： 将所有无入度的节点入栈，再遍历栈元素根据其出度确定父节点并将这个子节点的权重增加到父节点上，并减少其父节点入度计数； 如果对应父节点还存在有出度，当其所有入度处理完成后（入度计数为0，其所有入度已经转化为权重值），作为无入度节点入栈同上处理； 直到栈中无任何元素（不存在还有出度的元素） NOTE：如果有环存在，其上一定不存在无入度的节点，因此不会在上述步骤处理，因此入度计数不为0 将除死锁环外的所有等待状态的事务权重更新（持有 lock_sys-\u0026gt;wait_mutex） */ lock_wait_compute_and_publish_weights_except_cycles(infos, table_reservations, outgoing, new_weights); if (innobase_deadlock_detect) { /* 用 DFS 方式确定的一个死锁环，如果存在的话就选择目标并回滚事务（lock_cancel_waiting_and_release） */ lock_wait_find_and_handle_deadlocks(infos, outgoing, new_weights); } } CATS 策略基于后台生产的事务权重，在事务放锁选择最合适等待事务去唤醒，提升整体事务效率。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 static void lock_rec_grant_by_heap_no(lock_t *in_lock, ulint heap_no) { const auto hash_table = in_lock-\u0026gt;hash_table(); using LockDescriptorEx = std::pair\u0026lt;trx_schedule_weight_t, lock_t *\u0026gt;; Scoped_heap heap((sizeof(lock_t *) * 3 + sizeof(LockDescriptorEx)) * 32, UT_LOCATION_HERE); RecID rec_id{in_lock, heap_no}; Locks\u0026lt;lock_t *\u0026gt; low_priority_light{heap.get()}; Locks\u0026lt;lock_t *\u0026gt; waiting{heap.get()}; Locks\u0026lt;lock_t *\u0026gt; granted{heap.get()}; Locks\u0026lt;LockDescriptorEx\u0026gt; low_priority_heavier{heap.get()}; const auto in_trx = in_lock-\u0026gt;trx; /* 对于锁链上每个 lock 划分到 granted、waiting、low_priority_light、low_priority_heavier 中： granted: 已经获取到的锁，如果等待锁被授权，也会从 waiting 转移到 granted； low_priority_light：低权重的 waiting 状态锁； low_priority_heavier：高权重的 waiting 状态锁； waiting：处于 waiting 状态的锁，最终 low_priority_light 和 low_priority_heavier 会合并入此数组； */ Lock_iter::for_each(rec_id, [\u0026amp;](lock_t *lock) { if (!lock-\u0026gt;is_waiting()) { granted.push_back(lock); return (true); } const auto trx = lock-\u0026gt;trx; if (trx-\u0026gt;error_state == DB_DEADLOCK || trx-\u0026gt;lock.was_chosen_as_deadlock_victim) { return (true); } const auto blocking_trx = trx-\u0026gt;lock.blocking_trx.load(std::memory_order_relaxed); if (blocking_trx != in_trx) { return (true); } if (trx_is_high_priority(trx)) { waiting.push_back(lock); return (true); } /* 这个等待的事务还有等待它的事务，认为是高权重 */ const auto schedule_weight = trx-\u0026gt;lock.schedule_weight.load(std::memory_order_relaxed); if (schedule_weight \u0026lt;= 1) { low_priority_light.push_back(lock); } else { low_priority_heavier.push_back(LockDescriptorEx{schedule_weight, lock}); } return (true); }, hash_table); if (waiting.empty() \u0026amp;\u0026amp; low_priority_light.empty() \u0026amp;\u0026amp; low_priority_heavier.empty()) { return; } /* 按权重进行排序，高权重优先 grant */ std::stable_sort(low_priority_heavier.begin(), low_priority_heavier.end(), [](const LockDescriptorEx \u0026amp;a, const LockDescriptorEx \u0026amp;b) { return (a.first \u0026gt; b.first);}); for (const auto \u0026amp;descriptor : low_priority_heavier) {waiting.push_back(descriptor.second);} waiting.insert(waiting.end(), low_priority_light.begin(), low_priority_light.end()); const auto new_granted_index = granted.size(); granted.reserve(granted.size() + waiting.size()); for (lock_t *wait_lock : waiting) { /* 已授权锁是否会阻塞当前锁 */ const lock_t *blocking_lock = lock_rec_has_to_wait_for_granted(wait_lock, granted, new_granted_index); if (blocking_lock == nullptr) { lock_grant(wait_lock); lock_rec_move_granted_to_front(wait_lock, rec_id); granted.push_back(wait_lock); } else { /* 等待的锁等待原因可能改变，更新 wait_for 图 */ lock_update_wait_for_edge(wait_lock, blocking_lock); } } } 版权声明：如需转载或引用，请附加本文链接并注明来源。 ","permalink":"https://mzyee.github.io/posts/mysql/lock/","summary":"MySQL 并发控制介绍和事务锁系统学习","title":"MySQL 事务锁系统"},{"content":"1. 前言 大家都知道元数据（Metadata）是用来描述数据的数据，没有元数据的情况下我们就没办法理解、使用数据库中存储的数据。本文通过 InnoDB 开启一个表的流程来讨论 InnoDB 的元信息管理和相关开表流程代码。\n2. 元数据管理 2.1 元数据的物理存储 在 MySQL 8.0 之前，Server 层和存储引擎层会各自保留一份元数据（schema name, table definition 等），不仅在信息存储上有着重复冗余，而且可能存在两者之间存储的元数据不同步的现象。MySQL 在 8.0 中引入了 data dictionary 来进行 Server 层和不同引擎间统一的元数据管理，这些元数据都存储在 InnoDB 引擎的表中，Server 层和引擎层共享一份元数据，且支持原子性。\n这些元数据对应的 InnoDB 引擎表我们一般称为系统表，其表结构是固定的直接定义在代码类结构中（因此不再需要记录额外的元数据，要不然就套娃了），对应表文件在整个 MySQL 进行初始化时就建立了，有如 tables、columns、indexes、foreign_keys等系统表。可以通过下面的 SQL 在 debug 版本查看所有系统表：\n1 2 3 4 5 6 7 8 SET SESSION debug=\u0026#39;+d,skip_dd_table_access_check\u0026#39;; SELECT id, name, schema_id, hidden FROM mysql.tables WHERE hidden=\u0026#39;System\u0026#39; AND type=\u0026#39;BASE TABLE\u0026#39;; 对于某张用户表来说，其元数据就是通过这些系统表对应的行内容记录构成。各元数据表的逻辑关系可以近似看成是一颗树关联结构。其顶层入口是 table 表（mysql.tables）中的对应唯一记录，再通过记录的 table id 等索引项关联到如 columns、indexes 等各元数据系统表，进而获取这个表对应的所有元数据的记录内容。\ngraph TD; tables--`name`--\u003ecatalogs; catalogs--`catalog_id``name`--\u003eschemata; tables--`table_id``name`--\u003ecolumns; tables--`table_id``name`--\u003eindexes; tables--`...`--\u003e...; 2.2 元数据的内存结构 在明确了 DD 元信息在物理层面的存储格式后，我们能够很清楚的知道，当需要构建一张用户表的元数据时，我们首先需要访问所有需要的系统表（元数据表），并依次在各系统表内找到对应这张用户表的相应记录，然后用这些元数据记录构建出用户表的内存对象。\n元数据表本身对应的内存对象结构是从 Object_table 类从派生出来的，有 Entity_object_table_impl 和 Object_table_impl 两大类。前者对应持久化有对应具体键对象的基本 DD 表，而后者对应着不单独存在需要通过前者关联访问的 DD 表（不能直接被 create，search，drop），例如两者分别对应派生出 Tables、Tablespaces、\u0026hellip; 和 Columns、Indexes、\u0026hellip; 等具体内存结构体，一一对应各元数据系统表。\n值得一提，上面说的这些内存对象是元数据表本身的内存对象，也就是访问元数据表所使用的内存对象，而非某一张用户表的内存对象。类似的，对于一张用户表，也是通过对应的元数据记录构建其内存对象结构。一般从 Entity_object_impl 类从派生出来的，像常用的 Table_impl 和 View_impl 分别表示用户表和视图对应元数据的内存对象。\n当缓存穿透时，这些 objects 的底层操作逻辑被封装在 Storage_adapter 中，通过提供的 get() / drop() / store() 等接口，遍历查询或修改所有所需元数据表（btree 结构）中对应的 record，构建或持久性相应的 object 内存对象由/到引擎层。\n2.3 元数据的 cache 缓存 从上述操作中可以看出，直接从元数据表构建 DD object 内存对象是开销十分巨大的，过程中需要物理访问并索引多个 Btree 索引。因此 MySQL 建立了多层的 DD Cache 来就可能减小元数据的访问开销，一般称为元数据的 3 层缓存架构：\n每个 client 的独享缓存，即线程 THD 独占的 Dictionary_client 结构，其中有 committed、uncommitted、dropped 三类 objects map，最初 acquire 的 object 会被加入到committed map 中，client 调用 store 或 update 接口时将 object 放到 uncommitted map 中，然后在事务提交后将相应 objects 从 uncommitted map 移到 committed map 中，而调用 drop 接口会将 objects 加入 dropped map。当访问 Dictionary_client 穿透时，从 Shared_dictionary_cache 获取； Server 全局唯一的共享缓存，使用单例 Shared_dictionary_cache 来实现，其实质上也是 objects map 集合。在此缓存层相同 key 对应的 DD objects 对象唯一，这里的 key 其实就是对应元数据表的索引 key。当 Shared_dictionary_cache 穿透时，通过 Storage_adapter 从 InnoDB handler 读取元数据表记录。 存储引擎层（元数据表数据直接 BP 缓存），系统表的访问模式和普通用户表基本一致，注意采用的是 READ_COMMITTED 隔离级别。 3. 元数据的使用 3.1 Server 层元数据的使用 实现了 DD 元数据的管理，MySQL 就能够通过使用元数据构建访问用户表的环境，这也就是我们常说的“开表”逻辑。前面已经介绍了元数据表本身的内存对象，通过访问元数据表相应记录，类似的可以构建用户表（或其他内容，如视图等）的内存对象。\n我们需要知道 Server 层的两个关键结构体 TABLE_SHARE 和 TABLE:\n一张表被初次访问时，MySQL 会其建立一个 TABLE_SHARE 对象，与其与引擎层中的对应表 dict_table_t 相对应关联。TABLE_SHARE 是静态的，不能修改的，且一张表只存在一份，其中记录表定义相关的一些 DD 信息，如包含的字段等。TABLE_SHARE 只有在表结构被修改后才会删除，或者缓存使用满了会淘汰。简单的说，TABLE_SHARE 就是某张表定义的实体化对象。 对每一个会话查询中涉及的表，MySQL 会通过 TABLE_SHARE 为每个表建一个 TABLE 实体对象，这一过程叫表结构实例化。如果是 InnoDB 表还会创建 InnoDB 的 handler，server 层会话通过 TABLE 对象经引擎层操作表文件实体。可以将 TABLE 对象看做表在 server 层的映射，将 handler 看做其为操作底层数据文件而在引擎层创建的句柄。 “开表”逻辑就是通过访问元数据来获取 TABLE_SHARE 和构建 TABLE 实体对象的过程。我们具体看下 open_table 的代码逻辑。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 bool open_table(THD *thd, Table_ref *table_list, Open_table_context *ot_ctx) { // Step 0. 做一些前置检查... // Step 1. 除特殊 DD 表等场景，LOCK TABLES mode(LTM)下校验是否表对象都是 pre-opened 的； // Step 2. 非 LTM 模式（通常的模式），根据 mdl 请求模式获取 mdl 锁； if ((flags \u0026amp; (MYSQL_OPEN_HAS_MDL_LOCK | MYSQL_OPEN_SECONDARY_ENGINE)) == 0) { // 如果需要 write_lock mdl，还要获取 global read lock 保证正确冲突性 // ... if (open_table_get_mdl_lock(thd, ot_ctx, table_list, flags, \u0026amp;mdl_ticket) || mdl_ticket == nullptr) { return true; } } else { /* caller 获取过 MDL 锁 */ mdl_ticket = table_list-\u0026gt;mdl_request.ticket; } // Step 3. 检查目标表的存在性； if (table_list-\u0026gt;open_strategy == Table_ref::OPEN_IF_EXISTS || table_list-\u0026gt;open_strategy == Table_ref::OPEN_FOR_CREATE) { bool exists; if (check_if_table_exists(thd, table_list, \u0026amp;exists)) return true; /* 空读 */ if (!exists) { /* table 不存在于 DD，升级到 EXCLUSIVE MDL lock. */ if (table_list-\u0026gt;open_strategy == Table_ref::OPEN_FOR_CREATE \u0026amp;\u0026amp; !(flags \u0026amp; (MYSQL_OPEN_FORCE_SHARED_MDL | MYSQL_OPEN_FORCE_SHARED_HIGH_PRIO_MDL))) { MDL_deadlock_handler mdl_deadlock_handler(ot_ctx); thd-\u0026gt;push_internal_handler(\u0026amp;mdl_deadlock_handler); bool wait_result = thd-\u0026gt;mdl_context.upgrade_shared_lock( table_list-\u0026gt;mdl_request.ticket, MDL_EXCLUSIVE, thd-\u0026gt;variables.lock_wait_timeout); thd-\u0026gt;pop_internal_handler(); /* Deadlock or timeout occurred while upgrading the lock. */ if (wait_result) return true; } return false; } } else if (table_list-\u0026gt;open_strategy == Table_ref::OPEN_STUB) /* 底层不真打开 */ return false; // Step 4. Table 存在，尝试开表，首先从 table_cache_manager 这个缓存找 retry_share : { Table_cache *tc = table_cache_manager.get_cache(thd); tc-\u0026gt;lock(); if (!table_list-\u0026gt;is_view()) table = tc-\u0026gt;get_table(thd, key, key_length, \u0026amp;share); if (table) { /* Case 1. 找到未使用的 TABLE object */ if (!(flags \u0026amp; MYSQL_OPEN_IGNORE_FLUSH)) { /* 检查 DD 的版本是否过老或不一致 */ if (thd-\u0026gt;open_tables \u0026amp;\u0026amp; thd-\u0026gt;open_tables-\u0026gt;s-\u0026gt;version() != share-\u0026gt;version()) { // 清理操作 ... return true; } } tc-\u0026gt;unlock(); table-\u0026gt;file-\u0026gt;rebind_psi(); table-\u0026gt;file-\u0026gt;ha_extra(HA_EXTRA_RESET_STATE); thd-\u0026gt;status_var.table_open_cache_hits++; goto table_found; } else if (share) { /* Case 2. 无未使用 TABLE 对象，但是找到表的 TABLE_SHARE */ mysql_mutex_lock(\u0026amp;LOCK_open); tc-\u0026gt;unlock(); share-\u0026gt;increment_ref_count(); goto share_found; } else { /* Case 3. 无未使用 TABLE 对象，也无表的 TABLE_SHARE */ tc-\u0026gt;unlock(); } } mysql_mutex_lock(\u0026amp;LOCK_open); // Step 5. 获取 TABLE_SHARE： // 首先从 table_def_cache 缓存里找， // 穿透情况下确认 schema MDL 锁后建立新 TABLE_SHARE，并从元数据表读取记录填充。 if (!(share = get_table_share_with_discover( thd, table_list, key, key_length, flags \u0026amp; MYSQL_OPEN_SECONDARY_ENGINE, \u0026amp;error))) { // 失败清理操作 ... return true; } if (table_list-\u0026gt;is_view() || share-\u0026gt;is_view) { /* 如果是 view 对象情况下的处理 */ // ... // return ... } share_found: if (!(flags \u0026amp; MYSQL_OPEN_IGNORE_FLUSH)) { if (share-\u0026gt;has_old_version()) { // 释放对 shard 的 reference，等待老版本 table share 更新 release_table_share(share); mysql_mutex_unlock(\u0026amp;LOCK_open); MDL_deadlock_handler mdl_deadlock_handler(ot_ctx); bool wait_result; thd-\u0026gt;push_internal_handler(\u0026amp;mdl_deadlock_handler); uint deadlock_weight = ot_ctx-\u0026gt;can_back_off() ? MDL_wait_for_subgraph::DEADLOCK_WEIGHT_DML : mdl_ticket-\u0026gt;get_deadlock_weight(); wait_result = tdc_wait_for_old_version(thd, table_list-\u0026gt;db, table_list-\u0026gt;table_name, ot_ctx-\u0026gt;get_timeout(), deadlock_weight); thd-\u0026gt;pop_internal_handler(); if (wait_result) return true; goto retry_share; } if (thd-\u0026gt;open_tables \u0026amp;\u0026amp; thd-\u0026gt;open_tables-\u0026gt;s-\u0026gt;version() != share-\u0026gt;version()) { /* 如果 version 改变，让步后重新开表 */ // 清理操作 ... return true; } } mysql_mutex_unlock(\u0026amp;LOCK_open); // Step 6. 由 TABLE_SHARE 构建 TABLE 对象 { dd::cache::Dictionary_client::Auto_releaser releaser(thd-\u0026gt;dd_client()); const dd::Table *table_def = nullptr; if (!(flags \u0026amp; MYSQL_OPEN_NO_NEW_TABLE_IN_SE) \u0026amp;\u0026amp; thd-\u0026gt;dd_client()-\u0026gt;acquire(share-\u0026gt;db.str, share-\u0026gt;table_name.str, \u0026amp;table_def)) { goto err_lock; } if (table_def \u0026amp;\u0026amp; table_def-\u0026gt;hidden() == dd::Abstract_table::HT_HIDDEN_SE) { my_error(ER_NO_SUCH_TABLE, MYF(0), table_list-\u0026gt;db, table_list-\u0026gt;table_name); goto err_lock; } /* make a new table */ if (!(table = (TABLE *)my_malloc(key_memory_TABLE, sizeof(*table), MYF(MY_WME)))) goto err_lock; error = open_table_from_share( thd, share, alias, ((flags \u0026amp; MYSQL_OPEN_NO_NEW_TABLE_IN_SE) ? 0 : ((uint)(HA_OPEN_KEYFILE | HA_OPEN_RNDFILE | HA_GET_INDEX | HA_TRY_READ_ONLY))), EXTRA_RECORD, thd-\u0026gt;open_options, table, false, table_def); if (error) { // 清理操作 ... goto err_lock; } else if (share-\u0026gt;crashed) { switch (thd-\u0026gt;lex-\u0026gt;sql_command) { case SQLCOM_ALTER_TABLE: case SQLCOM_REPAIR: case SQLCOM_CHECK: case SQLCOM_SHOW_CREATE: break; // 可以处理的 case default: // 清理操作 ... goto err_lock; } } /* Finalize the process of TABLE creation by loading table triggers */ if (open_table_entry_fini(thd, share, table_def, table)) { // 清理操作 ... goto err_lock; } } // Step 7. 将新生成的 TABLE 对象加入当前连接的 table cache { Table_cache *tc = table_cache_manager.get_cache(thd); tc-\u0026gt;lock(); if (tc-\u0026gt;add_used_table(thd, table)) { tc-\u0026gt;unlock(); goto err_lock; } tc-\u0026gt;unlock(); } thd-\u0026gt;status_var.table_open_cache_misses++; table_found: // 当前有了 TABLE 对象 table-\u0026gt;mdl_ticket = mdl_ticket; table-\u0026gt;next = thd-\u0026gt;open_tables; /* Link into simple list */ thd-\u0026gt;set_open_tables(table); table-\u0026gt;reginfo.lock_type = TL_READ; /* Assume read */ reset: // 成功，初始化返回 table-\u0026gt;reset(); table-\u0026gt;set_created(); table_list-\u0026gt;set_updatable(); table_list-\u0026gt;set_insertable(); table_list-\u0026gt;table = table; // skipping partitions bitmap setting in MYSQL_OPEN_NO_NEW_TABLE_IN_SE // ... table-\u0026gt;init(thd, table_list); return false; err_lock: // 失败 mysql_mutex_lock(\u0026amp;LOCK_open); release_table_share(share); mysql_mutex_unlock(\u0026amp;LOCK_open); return true; } 考虑所有 cache 都穿透的情况，则此时在 get_table_share 中，通过 DD 接口从元数据表读取相应（表）对象的元数据记录，再以之填充新生成的 TABLE_SHARE。\n3.2 Server 层表对象缓存 从前面 open_table代码可见，获取 TABLE_SHARE 和构建 TABLE 实体对象过程中也涉及多层 cache 缓存机制。首先是 Table_cache_manager 缓存了 TABLE 对象，维护了所有正在使用或曾经打开过的 TABLE 对象，其大小由 table_cache_size 维护，内部按 THD 分片为 table_cache_instances 个 Table_cache。每个 Table_cache 内部由 object name（例如某张用户表） 映射到 Table_cache_element，可见 Table_cache_element 唯一对应一个 object，因此也唯一对应一个 TABLE_SHARE，其内部链接了这个缓存分片内中的所有由此 TABLE_SHARE 生成的 TABLE 实例。\n如果 Table_cache_manager 缓存穿透，则会去 Table_definition_cache 缓存寻找是否有存在 TABLE_SHARE 对象，其大小设置为 min(400 + table_cache_size / 2, 2000)。 如果 Table_definition_cache 进一步穿透，则会去 InnoDB 层读取元数据构建 TABLE_SHARE。\n另外，在 InnoDB 也为每一个 InnoDB 表加载一个数据字典对象，这些对象的集合就是 InnoDB 中的 data dictionary。InnoDB 的 dictionary system 以 全局 dict_sys_t 管理，而单个表对象对应 dict_table_t，类似的，索引对象对应 dict_index_t，列对象对应 dict_col_t 等。InnoDB 同样通过读取元数据表记录来构建 dict_table_t 对象，并且 dict_sys_t 中也有两个 dict_table_t 缓存，分别以 table name 和 table id 进行映射关联，其最大容量限制和 Table_definition_cache 一致。\n版权声明：如需转载或引用，请附加本文链接并注明来源。 ","permalink":"https://mzyee.github.io/posts/mysql/meta/","summary":"MySQL 元信息管理及开表流程代码学习","title":"MySQL 如何准备开启一个表"},{"content":"1. 前言 InnoDB 的 redo log 模块是保证事务持久性的核心，InnoDB 遵守 WAL 原则保证总是日志先行，即在持久化数据文件时保证其对应的 redo 日志已经写到磁盘，这样在崩溃的情况下，它就可以用于恢复对已修改但尚未刷新到磁盘的页面的修改。本文主要讨论 InnoDB 中 redo 日志的物理组织格式，内存结构及前后向的生成/应用流程。可以参考阅读文档：\n数据库故障恢复机制：ARIES、ARIES/IM InnoDB Redo Log 官方介绍 InnoDB redo log 漫游 庖丁解 InnoDB 之 REDO LOG 2. Redo 日志的物理格式 在 8.0.30 版本前，MySQL 通过 innodb_log_file_size 和 innodb_log_files_in_group 分别控制 redo 文件的大小和数目，文件名为 ib_logfilexx；在 8.0.30 版本后，官方新加了参数 innodb_redo_log_capacity 并允许动态配置 redo 日志的总容量，系统一共维护了 32 个文件名为 #ib_redoxx 和 #ib_redoxx_tmp，具有 _tmp 后缀的文件为空余未使用的文件。redo 日志中的数据是以 append 的形式不断增加，文件中任一位点的数据对应一个永久递增的 LSN 号标志。\n一个 redo 文件首先以 LOG_FILE_HDR_SIZE (2KB = 4*512B) 大小的 file header 开头，包含 header info block、checkpoint 1、encryption info、checkpoint 2 这 4 个block；其中 header info block 记录了一些，4字节的 Log 版本 FORMAT、4字节 LOG_UUID、8字节 START_LSN 标识的当前文件开始LSN、最长32位的 Creator 信息表示当前 mysql 版本。有两个 checkpoint 的原因是通过 double write 机制防止单个 checkpoint 记录因为写盘过程中间 crash 而损坏（现在多数 SSD 支持原子写 4K 粒度）。\n这里需要注意的是在 8.0 之前的版本中，checkpoint_lsn 一定指向一个 mtr record group 的开头，并且该 mtr record 应该被恢复（虽然相关页面仍然可能已被刷写下去）。但从 8.0 开始，由于 recent_closed buffer 的存在，这个值可能指向 record group 中间的某个字节，在这种情况下，恢复应该跳过包含检查点 lsn 的日志记录组并从其下一条开始。redo 文件中写入的 checkpoint_lsn 一定在此 redo 的 lsn 范围。\n接着 file header 的是一个个 OS_FILE_LOG_BLOCK_SIZE (512B) 大小的 log block，包括12字节的 Header (block number + data length + first record offset + checkpoint number)、496个字节的 Body、4字节的 Tailer (checksum)。对于 Header 部分的说明是：\n4字节 Block Number，老版本中 Flush Flag 占用最高位bit标识一次 I/O 的第一个 Block，剩下的31个 bit 是当前 Block 编号 2字节 Data Length，长度为 0 为 未使用 block（reuse 情况下可能无效），长度为 [12 , 508) 表示最后一个未写完的 block， 长度为 512 表示写完整的 block； 2字节 First Record Group Offset，用来指向 Block 中第一个 mtr record group 的开始位置，如果和 Data Length 相同则说明内部没有开启记录新的 mtr record group； 4字节 Epoch Number，和 Block Number 一起组成了 block 的唯一标志，通过 64 位 block start lsn 转换获得。在老版本中，这里记录的是每次刷 checkpoint 时推进的 log_sys-\u0026gt;next_checkpoint_no 的低四位。 在 log block 的 body 部分，则是一条条实际的 redo log record，或者在 InnoDB 中称为 mtr record 更为贴切。每个 redo record 的第一个 Byte 是这个记录的类型，其中最高位的 bit 为 MLOG_SINGLE_REC_FLAG，如果被设置则表示此 mtr 只包含了（最多单个页相关）的单条记录，否则是由多条单 record 组成的 mtr record group，并且在 group 结尾会以 MLOG_MULTI_REC_END标记；之后以压缩格式记录当前 record 对应的 space id 和 page no（除 TABLE_DYNAMIC_META 等类型）；接着是当前 record 对应的 record body 部分内容，具体内容会由 record 的类型决定。\nInnoDB 的 redo 是 Physiological Logging，网上一种常用说法是 “Physical to a page，logical within a page”，实际上可以理解是在记录日志时对前向过程日志记录量和后向日志恢复速度的优化考虑，在对于 page 内的修改日志并非一定是 logical 的，但在对于一个或多个 page 的固定模式的修改可以通过 logical log 来减小 redo 记录量。另外在 undo 相关的分析文章中提到过 undo 表空间数据也是通过 redo 来维护的，而 undo 本身是基于逻辑而非物理去做回滚的。\n3. Redo 日志的生成 首先有必要介绍 InnoDB 中 mtr (mini-transaction) 的概念，InnoDB 中对于物理文件的修改都是以 mtr 作为原子单位（无论其内是 single 还是 multi record 的）。一个 mtr 在前向执行过程中会占据所需的资源，包括 index、page 及其对应物理锁等，以保证并发操作正确性，并将数据修改操作对应生成的 redo 在 mtr 内部缓存。在 mtr commit 的时候此 mtr 会将缓存的 redo record 提交到全局 log buffer 中等待落盘。需要注意的是，在内存逻辑来看（不发生crash情况下）mtr 一旦提交就代表了这个物理操作在系统全局产生持久化效果，哪怕对应所属的 transaction 还没有或最终最终提交，因此这里在必须时就需要 undo mtr 的操作，因此在对应数据操作前都需要先记录 undo；此外，部分物理操作可能不会被撤销，比如空间拓展分配等。\n如果 mtr 产生 redo log 则其在提交过程中：\n根据数据长度向 log_sys 申请对应的全局顺序记录的 sn 范围，其通过加上 block header 和 tailer，不包括 file header 可以转换为对应的 lsn 范围；并且等待到 log buffer 的空间足够对应 sn 范围数据被写入； 对于 mtr 中缓存的所有 redo record，按 log block 的格式将 header 和 tailer 留空 copy body 内容，并将 header 中的 First Record Group Offset 字段（写完后的下一个）填上； 等待至 recent_written link_buf 有空闲位置，表示到这一位置的 lsn 对应的 log 可以被写入；再推进此结构的中相应的 lsn 范围，表示这一段内容已经完整写入到 log buffer 中，并尝试推进其 m_tail（已经连续完整写入到位置）； 等待至 recent_closed link_buf 有空闲位置，表示到这一位置的 lsn 对应的 dirty page 可以被加到 flush list 上；再将 dirty page 挂到 flush list 上； 最终释放所有 page 锁等独占资源。 4. Redo 日志的写入 在 mtr redo record 写入到全局 log buffer 中后，系统通过全局结构 log_t *log_sys 和后台工作线程来写入 redo 日志，并维护如各种 lsn 位点等相关状态。\n后台工作线程有如下这些：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 /* 控制 log 文件的轮转 */ void log_files_governor(log_t *log_ptr); /* 将全局 log buffer 中的日志写入到 OS buffer 中 */ void log_writer(log_t *log_ptr); /* 将 OS buffers 中的数据刷写到磁盘上 (fsyncs) */ void log_flusher(log_t *log_ptr); /* 通知用户线程对应 log write 完成，write_lsn 已更新 */ void log_write_notifier(log_t *log_ptr); /* 通知用户线程对应 log flush 完成，flushed_to_disk_lsn 已更新 */ void log_flush_notifier(log_t *log_ptr); /* 检查是否需要要求完成强制刷脏并写入 checkpoint lsn 到日志文件中 */ void log_checkpointer(log_t *log_ptr); 这里刷写相关的任务其实都是围绕 log file 和 log buffer 首位端的推进：\n系统维护了所有的 log file 的内存对象 Log_files_dict，并通过 log_files_governor 控制已写入完全（可 purge 或仍需要）、当前正在写入、后续可使用的 redo 文件状态； log buffer 头部的推进的相关状态，buf_limit_sn（限制可被写入到 buffer 的最大 sn，即 wirte_lsn + buf_size）、recent_written buffer（追踪控制 mtr 并发写入状态 log buffer）、recent_closed buffer（追踪控制 dirty page 挂载 flush list）； log buffer 刷写状态，到 recent_written tail 位置的 redo 已经完整可写盘（这里需要等至上一次 checkpoint 加 redo file 总容量超过目标写入 lsn），log writer 会将 log block 的 header 和 tailer 填充，然后确定写入范围直接从 log buffer（完整 log_write_ahead_size 大小倍数）或通过 write_ahead_buffer（小于 log_write_ahead_size） 写入 log file； log buffer 尾部的推进的相关状态，如 write_lsn、flushed_to_disk_lsn，事件通知等。 checkpoint 推进，并对 log file 尾部文件的回收。 对应线程具体的执行流程介绍可以参考官方文档。 5. Redo 日志应用及 InnoDB 奔溃恢复 在奔溃恢复的情况下，InnoDB通过应用 redo 来恢复已经提交但还没有刷盘的事务数据。另外，一些基于物理复制架构的数据库，像 PolarDB、AWS Aurora 等，还存在通过应用 redo 来进行数据同步的流程。我们这里只讨论 recovery 的 redo 阶段，整个 InnoDB server 的启动简化流程如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 dberr_t srv_start(bool create_new_db) { // Step 0. 环境准备和检查，... // Step 1. 初始化 SRV 变量，... srv_boot(); // Step 2. 扫描配置的目录，生成文件map fil_init(innobase_get_open_files_limit()); // 确定路径... // 扫描获取 .IBD 和 undo 文件，从首 page 中读取 space_id 并记录到 fil_system 的 Tablespace_dirs err = fil_scan_for_tablespaces(); // Step 3. INNODB STATUS 监控文件... // Step 4. 初始化 AIO 线程... os_aio_init(srv_n_read_io_threads, srv_n_write_io_threads) // Step 5. 初始化 buffer pool... err = buf_pool_init(srv_buf_pool_size, srv_buf_pool_instances); // Step 6. 初始化多个子系统... fsp_init(); pars_init(); recv_sys_create(); recv_sys_init(); trx_sys_create(); lock_sys_create(srv_lock_table_size); os_aio_start_threads();/* i/o-handler threads */ buf_flush_page_cleaner_init(); // Step 7. 初始化 system tablespace... // ibdata1 文件内含系统表空间（注意与 元数据DD表空间 或叫 内部系统表 的 mysql.ibd 区分） // 内含 change buffer(，特定情况下含用户表、undo 表)等 // 创建对应的 sysspace 并加入 fil_system (fil_space_create + fil_node_create) err = srv_sys_space.open_or_create(false, create_new_db, \u0026amp;sum_of_new_sizes, \u0026amp;flushed_lsn); dict_persist_init(); // Step 8. 初始化 log sys，这里还会扫描出所有 redo file 来构建 Log_files_dict err = log_sys_init(create_new_db, flushed_lsn, new_files_lsn); //... if (create_new_db) { // 初始化建立新 db } else { // Step 9. BP状态重置 err = dblwr::v1::init(); /* 初始化 double write */ buf_pool_invalidate(); /* 淘汰所有 page 确保 recovery 重读 */ // Step 10. 打开所有 log files 和 system data files fil_open_system_tablespace_files(); // Step 11. 开始恢复 redo 日志... err = recv_recovery_from_checkpoint_start(*log_sys, flushed_lsn); // ... 初始化 innodb data dictionary system // Step 12. 启动后台 log 线程 ... if (!srv_read_only_mode) { log_start_background_threads(*log_sys); } // Step 13. 应用剩余的最后一批 hashed log records if (srv_force_recovery \u0026lt; SRV_FORCE_NO_LOG_REDO) { err = recv_apply_hashed_log_recs(*log_sys, !recv_sys-\u0026gt;is_cloned_db \u0026amp;\u0026amp; !log_upgrade); } // 一些检查... // Step 14. 刷写所有脏页 if (!srv_force_recovery \u0026amp;\u0026amp; !srv_read_only_mode) { buf_flush_sync_all_buf_pools(); } // Step 15. 完成 redo 日志恢复后的清理，恢复dynamic metadata MetadataRecover *dict_metadata = recv_recovery_from_checkpoint_finish(false); /* 此时 DD（table persistent data）还没有完全 recovery */ if (!recv_sys-\u0026gt;is_cloned_db \u0026amp;\u0026amp; !dict_metadata-\u0026gt;empty()) { fil_space_t *space = fil_space_acquire_silent(dict_sys_t::s_dict_space_id); if (space == nullptr) { dberr_t error = fil_ibd_open(true, FIL_TYPE_TABLESPACE, dict_sys_t::s_dict_space_id, predefined_flags, dict_sys_t::s_dd_space_name, dict_sys_t::s_dd_space_file_name, true, false); } else { fil_space_release(space); } dict_persist-\u0026gt;table_buffer = ut::new_withkey\u0026lt;DDTableBuffer\u0026gt;(UT_NEW_THIS_FILE_PSI_KEY); dict_metadata-\u0026gt;store(); // 将恢复过程中persistent dynamic metadata修改 store 到 mysql.innodb_dynamic_metadata log_buffer_flush_to_disk(*log_sys); } ut::delete_(dict_metadata); // Step 16. 构建 Undo Tablespaces 和 Rollback Segments 内存结构并进行恢复 err = srv_undo_tablespaces_init(false); trx_purge_sys_mem_create(); purge_queue = trx_sys_init_at_db_start(); srv_undo_tablespaces_upgrade(); trx_purge_sys_initialize(srv_threads.m_purge_workers_n, purge_queue); } /* Open temp-tablespace and keep it open until shutdown. */ err = srv_open_tmp_tablespace(create_new_db, \u0026amp;srv_tmp_space); err = ibt::open_or_create(create_new_db); // Step 17. 完整化 undo 表空间 // 增加 rollback segment 数目到 srv_rollback_segments，此次配置可能和上次不同 trx_rseg_adjust_rollback_segments(srv_rollback_segments); // 构建完成完整 undo 表空间，删除 trunc.log，设置active srv_undo_tablespaces_mark_construction_done(); undo::spaces-\u0026gt;s_lock(); for (auto undo_space : undo::spaces-\u0026gt;m_spaces) { if (!undo_space-\u0026gt;is_empty()) { undo_space-\u0026gt;set_active(); } } undo::spaces-\u0026gt;s_unlock(); // Step 18. 监控系统等... // Step 19. something... return (DB_SUCCESS); } 其中比较关键的阶段在于 Step 11 recv_recovery_from_checkpoint_start 中进行 redo 恢复以及 Step 16 恢复 undo 和 trx 系统的状态，后者在 undo 系统的讨论中进行介绍，本文主要讨论前者。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 dberr_t recv_recovery_from_checkpoint_start(log_t \u0026amp;log, lsn_t flush_lsn) { // Step 1. 初始化flush_rbt，保证脏页按序插入flush list buf_flush_init_flush_rbt(); // Step 2. 通过扫描 Log_files_dict 找最后的 checkpoint 记录 Log_checkpoint_location checkpoint; recv_find_max_checkpoint(log, checkpoint); // ... /* Step 3. 解析存储 redo record： 1. 从 checkpoint 读取 redo 文件暂存到 log_sys-\u0026gt;buf； 2. 按 log block 扫描去 block head/tail 存到 recv_sys-\u0026gt;buf 中； 3. 将 recv_sys-\u0026gt;buf 中的记录 parse，生成 recv_t *recv（单个 record）和 recv_addr_t *recv_addr（page 所有 record 串）； 4. 存储 recv_addr_t 到 recv_sys 哈希表对应的 (space_id, page_no) 处 5. （如果占用内存较大）将所有存储的 record apply 到 page 上，bp内的走 recv_recover_page、bp外的走 buf_read_recv_pages */ recv_recovery_begin(log, checkpoint_lsn); // Step 4. 初始化 log_sys 状态，包括各 lsn 位点、各 buf 状态 lsn_t recovered_lsn = log.recovered_lsn = recv_sys-\u0026gt;recovered_lsn; auto check_scanned_lsn = log.m_scanned_lsn; if (check_scanned_lsn % OS_FILE_LOG_BLOCK_SIZE == 0) { // If it is at block boundary, add header size. check_scanned_lsn += LOG_BLOCK_HDR_SIZE; } err = log_start(log, checkpoint_lsn, recovered_lsn, false); if (!srv_read_only_mode) { log.next_checkpoint_header_no = log_next_checkpoint_header(checkpoint.m_checkpoint_header_no); err = log_files_next_checkpoint(log, checkpoint_lsn); } mutex_enter(\u0026amp;recv_sys-\u0026gt;mutex); recv_sys-\u0026gt;apply_log_recs = true; mutex_exit(\u0026amp;recv_sys-\u0026gt;mutex); return DB_SUCCESS; } 对应 redo 的使用主要在于 parse 和 apply 两个阶段，分别对应 recv_parse_log_recs 和 recv_apply_log_rec 两个接口。parse 阶段完成后 recv_sys 内 redo record hash 的结构层次如下，存储的是对应 (space,page) 的解析完成的 redo record body。\n1 2 3 4 recv_sys =\u0026gt; space_m =\u0026gt; page_x =\u0026gt; recv_addr_t =\u0026gt; recv_t(data1)-\u0026gt;recv_t(data2)-\u0026gt;recv_t(data3) =\u0026gt; page_y =\u0026gt; recv_addr_t =\u0026gt; recv_t(data1)-\u0026gt;... =\u0026gt; ... space_n =\u0026gt; ... 解析的逻辑较为简单，由于 InnoDB 对于单个 redo record 不会记录长度，因此就是通过 redo 类型确定走不同解析逻辑确定 redo record 长度（这里 MariaDB 记录了长度来加速，当然其 redo 内容也还有许多其他修改）。需要注意的是对应一些非物理 page 的 redo (如 MLOG_FILE_EXTEND) 或特殊 page (如 page 0) 元信息修改，在 parse 阶段就会做额外的处理工作来维护状态。\n应用的逻辑触发逻辑有两种，对于 bp 内的 page 直接走 recv_recover_page，对于 bp 外的 page 在读 I/O 完成时 buf_page_io_complete 也是调用 recv_recover_page（唯一不同的是后者会转移 page 的 x-latch 的 ownership 到当前线程）。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 void recv_recover_page_func(bool just_read_in, buf_block_t *block) { mutex_enter(\u0026amp;recv_sys-\u0026gt;mutex); if (recv_sys-\u0026gt;apply_log_recs == false) { mutex_exit(\u0026amp;recv_sys-\u0026gt;mutex); return; } recv_addr_t *recv_addr = recv_get_rec(block-\u0026gt;page.id.space(), block-\u0026gt;page.id.page_no()); if (recv_addr == nullptr || recv_addr-\u0026gt;state == RECV_BEING_PROCESSED || recv_addr-\u0026gt;state == RECV_PROCESSED) { mutex_exit(\u0026amp;recv_sys-\u0026gt;mutex); return; } recv_addr-\u0026gt;state = RECV_BEING_PROCESSED; mutex_exit(\u0026amp;recv_sys-\u0026gt;mutex); mtr_t mtr; mtr_start(\u0026amp;mtr); mtr_set_log_mode(\u0026amp;mtr, MTR_LOG_NONE); // redo 应用不在记录redo // 获取 page block page_t *page = block-\u0026gt;frame; page_zip_des_t *page_zip = buf_block_get_page_zip(block); if (just_read_in) { rw_lock_x_lock_move_ownership(\u0026amp;block-\u0026gt;lock); } bool success = buf_page_get_known_nowait( RW_X_LATCH, block, Cache_hint::KEEP_OLD, __FILE__, __LINE__, \u0026amp;mtr); ut_a(success); // 获取 page lsn lsn_t page_lsn = mach_read_from_8(page + FIL_PAGE_LSN); lsn_t page_newest_lsn = buf_page_get_newest_modification(\u0026amp;block-\u0026gt;page); lsn_t end_lsn = 0; lsn_t start_lsn = 0; bool modification_to_page = false; if (page_newest_lsn) { page_lsn = page_newest_lsn; } for (auto recv : recv_addr-\u0026gt;rec_list) { end_lsn = recv-\u0026gt;end_lsn; byte *buf = nullptr; if (recv-\u0026gt;len \u0026gt; RECV_DATA_BLOCK_SIZE) { buf = static_cast\u0026lt;byte *\u0026gt;(ut::malloc_withkey(UT_NEW_THIS_FILE_PSI_KEY, recv-\u0026gt;len)); recv_data_copy_to_buf(buf, recv); } else if (recv-\u0026gt;data != nullptr) { buf = ((byte *)(recv-\u0026gt;data)) + sizeof(recv_data_t); } if (recv-\u0026gt;type == MLOG_INIT_FILE_PAGE) { // init page 类型先修正 page lsn，相当于第一次会强制做 apply page_lsn = page_newest_lsn; memset(FIL_PAGE_LSN + page, 0, 8); memset(UNIV_PAGE_SIZE - FIL_PAGE_END_LSN_OLD_CHKSUM + page, 0, 8); if (page_zip) memset(FIL_PAGE_LSN + page_zip-\u0026gt;data, 0, 8); } // 过滤条件：1. page lsn 不超过 redo 的 lsn；2. 对于 undo 空间没有 truncate if (recv-\u0026gt;start_lsn \u0026gt;= page_lsn \u0026amp;\u0026amp; undo::is_active(recv_addr-\u0026gt;space)) { lsn_t end_lsn; unsigned char *buf_end = nullptr; if (!modification_to_page) { modification_to_page = true; start_lsn = recv-\u0026gt;start_lsn; } if (buf != nullptr) { buf_end = buf + recv-\u0026gt;len; } // 这里按照 redo 类型对 page 真正进行数据修改恢复 recv_parse_or_apply_log_rec_body(recv-\u0026gt;type, buf, buf_end, recv_addr-\u0026gt;space, recv_addr-\u0026gt;page_no, block, \u0026amp;mtr, ULINT_UNDEFINED, LSN_MAX); end_lsn = recv-\u0026gt;start_lsn + recv-\u0026gt;len; // 更新 page lsn mach_write_to_8(FIL_PAGE_LSN + page, end_lsn); mach_write_to_8(UNIV_PAGE_SIZE - FIL_PAGE_END_LSN_OLD_CHKSUM + page, end_lsn); if (page_zip) mach_write_to_8(FIL_PAGE_LSN + page_zip-\u0026gt;data, end_lsn); ++applied_recs; } else { ++skipped_recs; } if (recv-\u0026gt;len \u0026gt; RECV_DATA_BLOCK_SIZE) ut::free(buf); } // 有修改要加入脏页 if (modification_to_page) { buf_flush_recv_note_modification(block, start_lsn, end_lsn); } mtr_commit(\u0026amp;mtr); // 更新 recv_sys 状态 mutex_enter(\u0026amp;recv_sys-\u0026gt;mutex); if (recv_max_page_lsn \u0026lt; page_lsn) recv_max_page_lsn = page_lsn; recv_addr-\u0026gt;state = RECV_PROCESSED; --recv_sys-\u0026gt;n_addrs; mutex_exit(\u0026amp;recv_sys-\u0026gt;mutex); } 版权声明：如需转载或引用，请附加本文链接并注明来源。 ","permalink":"https://mzyee.github.io/posts/mysql/redo/","summary":"InnoDB Redo 日志的前后向流程","title":"InnoDB Redo 日志系统"},{"content":"1. 前言 事务系统是 InnoDB 实现 MVCC 及 ACID、进行事务并发控制的核心模块。本文主要讨论事务系统结构和一个事务的执行流程（基于 MySQL 8.0.30），需要涉及到对 redo、undo 系统的相关知识。本文还会涉及一部分 MVCC 和 事务锁的讨论，但是更详细内容会在对 Concurrency Control 讨论的文章中给出。\n2. 事务和事务系统的内存结构 事务和事务系统对应的内存结构分别是 trx_t 和 trx_sys_t。每个 session 连接持有一个 trx_t，其在创建连接执行第一个事务开始整个结构体就在 innobase_trx_allocate 初始化了，后续这个连接的所有事务一直复用此数据结构，直到连接断开。\n事务启动后不管读写，把这个结构体加入到全局事务链表中 (trx_sys-\u0026gt;mysql_trx_list)； 如果转换为读写事务，还会加入到全局读写事务链表中 (trx_sys-\u0026gt;rw_trx_list)；同时，读写事务在开启时（更确切的说是在分配回滚段时）通过全局 id 产生器产生以区分不同的写事务（这里 trx_id 只读事务为0，只读事务只需要通过指针地址来获取区分，如果只读事务需要写临时表，也会分配）；同时，还会分配回滚段给 trx_t 以供记录 undo record。 在事务内存提交的时候，还会加入到全局提交事务链表中(trx_sys-\u0026gt;serialisation_list)。同时，在 trx 提交时 (trx_commit_low 的 trx_write_serialisation_history) trx_no 字段通过全局产生器产生并加入 serialisation_list，这样可以确定事务提交的顺序，保证加入到 purge_queue 和 history list 中的 update undo 有序。然后在提交的最后阶段 (trx_commit_low 的 trx_commit_in_memory)，删除 serialisation_list、释放所有事务锁、清理 insert undo、等待刷完 redo log。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 struct trx_t { mutable TrxMutex mutex; /* 保护 `state` 和 `lock` */ trx_id_t id; /* 事务 id，在开启读写事务或分配回滚段时赋予 */ trx_id_t no; /* 事务 number，在事务提交阶段赋予 */ /* 事务的可能状态: TRX_STATE_NOT_STARTED TRX_STATE_FORCED_ROLLBACK TRX_STATE_ACTIVE TRX_STATE_PREPARED TRX_STATE_COMMITTED_IN_MEMORY (alias below COMMITTED) 有效的状态转移模式: 1. Regular transactions: * NOT_STARTED -\u0026gt; ACTIVE -\u0026gt; COMMITTED -\u0026gt; NOT_STARTED 2. Auto-commit non-locking read-only: * NOT_STARTED -\u0026gt; ACTIVE -\u0026gt; NOT_STARTED 3. XA (2PC): * NOT_STARTED -\u0026gt; ACTIVE -\u0026gt; PREPARED -\u0026gt; COMMITTED -\u0026gt; NOT_STARTED 4. Recovered XA: * NOT_STARTED -\u0026gt; PREPARED -\u0026gt; COMMITTED -\u0026gt; (freed) 5. XA (2PC) (shutdown or disconnect before ROLLBACK or COMMIT): * NOT_STARTED -\u0026gt; PREPARED -\u0026gt; (freed) 6. Disconnected XA can become recovered: * ... -\u0026gt; ACTIVE -\u0026gt; PREPARED (connected) -\u0026gt; PREPARED (disconnected) */ std::atomic\u0026lt;trx_state_t\u0026gt; state; /* 一致性读所使用的 read view，里面记录了启动时刻系统的活跃事务状态： 1. trx ids 的全可见（up_to）上界/ 全不可见（low_from）下界 2. trx no 的 undo 需求下界，小于此的 undo 不被需要 3. 所有活跃事务 trx_id 的数组 */ ReadView *read_view; UT_LIST_NODE_T(trx_t) trx_list; /* rw_trx_list 节点，读写事务 */ UT_LIST_NODE_T(trx_t) no_list; /* serialisation_list 节点，提交事务 */ UT_LIST_NODE_T(trx_t) mysql_trx_list; /* mysql_trx_list 节点，所有事务 */ /* 事务锁相关信息： 当前事务等待的事务锁； 阻塞当前事务的事务； 当前事务所持有的事务锁； 预分配的事务锁池； */ trx_lock_t lock; /* 事务锁信息 */ isolation_level_t isolation_level; /* 隔离等级 */ std::atomic\u0026lt;std:🧵:id\u0026gt; killed_by; /*------------------------------*/ trx_dict_op_t dict_operation; /* 是否修改 data dictionary */ lsn_t commit_lsn; /* commit 时的 lsn */ /*------------------------------*/ /*----------- Undo 相关信息 -------------*/ UT_LIST_BASE_NODE_T_EXTERN(trx_named_savept_t, trx_savepoints) trx_savepoints{}; UndoMutex undo_mutex; undo_no_t undo_no; /* 每个事务独立连续的 undo record number */ space_id_t undo_rseg_space; trx_savept_t last_sql_stat_start; trx_rsegs_t rsegs; /* rollback segments for undo logging */ undo_no_t roll_limit; ulint pages_undone; /*------------------------------*/ /* 一些事务状态，例如： 是启动后台恢复的事务； 是否修改 dd table； 是否需要 gap lock； 是否需要滞后刷写 redo log； 当前执行状态 ... */ // something... /* 如果有事务正给当前事务加隐式锁会增加这个标记， 加完后减少，事务在提交放锁前需要保证标记为 0， 避免事务锁加给提交事务 */ lint n_ref; uint32_t in_depth; /* 事务进入 innodb 层执行 */ uint32_t in_innodb; bool abort; /* 事务被中断 */ std::atomic_uint64_t version; trx_mod_tables_t mod_tables; /* 所有修改的 tables */ /* Xid信息 */ // something... }; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 struct trx_sys_t { /* 存储所有的 readview，包括 active 和 free 的对象 */ MVCC *mvcc; /* 所有的 undo 回滚段的内存对象 */ Rsegs rsegs; Rsegs tmp_rsegs; /* history list 的长度 */ std::atomic\u0026lt;uint64_t\u0026gt; rseg_history_len; /* 最小的未分配 trx id */ std::atomic\u0026lt;trx_id_t\u0026gt; next_trx_id_or_no; /* 事务提交时的全局有序队列 */ TrxSysMutex serialisation_mutex; UT_LIST_BASE_NODE_T(trx_t, no_list) serialisation_list; std::atomic\u0026lt;trx_id_t\u0026gt; serialisation_min_trx_no; TrxSysMutex mutex; /* 读写事务队列 */ UT_LIST_BASE_NODE_T(trx_t, trx_list) rw_trx_list; /* 所有开启事务队列 */ UT_LIST_BASE_NODE_T(trx_t, mysql_trx_list) mysql_trx_list; /* 当前活跃读写事务的 trx id 数组，在分配 trx id 时加入； 用于给 ReadView 创建 snapshot 记录事务启动时刻全局事务状态，来判断可见性； 另外就是 purge 时候 clone_oldest 来 purge 限制位点； */ trx_ids_t rw_trx_ids; /* trx id 到 trx_t 的映射 map，用于事务活跃性判断 */ Trx_shard shards[TRX_SHARDS_N]; ulint n_prepared_trx; /* XA PREPARED 阶段事务数目 */ bool found_prepared_trx; }; MySQL 的事务有如下四种隔离级别，不同的级别下事务的事务锁加锁逻辑不同。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 enum isolation_level_t { /* 执行非锁定 SELECT 语句时，不查看记录可能存在的早期一致性版本 */ READ_UNCOMMITTED, /* 范围 UPDATE 和 DELETE 会加 next-key locks 避免幻读； 执行锁定 SELECT 语句时，只加 record 锁不加 gap 锁； 一致性非锁定 SELECT 语句，语句级别 snapshot 实现 */ READ_COMMITTED, /* 执行锁定 SELECT 语句时，加 next-key 锁锁定 gap； 一致性非锁定 SELECT 语句，事务级别 snapshot 实现 */ REPEATABLE_READ, /* 所有 SELECT 语句以锁定模式执行 */ SERIALIZABLE }; 3. 事务开启 用户可以通过 START TRANSACTION [READ WRITE] / [WITH CONSISTENT SNAPSHOT] / [READ ONLY] 等语句显示开启（不同特性的）事务 (trans_begin)。显式开启事务的行为都会隐式的将上一条事务提交掉。只有通过 WITH CONSISTENT SNAPSHOT 的方式才会在开启时就进入InnoDB层，（调用 innobase_start_trx_and_assign_read_view）去开启一个 trx 并通过 trx_assign_read_view 分配 readview，否则会在事务第一次需要进行一执行读的时候分配 readview。\n对于 InnoDB 通过 trx_start_low 在存储层真正配置 trx_t 对象，将事务设置 TRX_STATE_ACTIVE 及相关读写状态，对于读写事务还会（或切换为读写事务 trx_set_rw_mode）：\n分配 undo 回滚段， trx_assign_rseg_durable，通过轮询方式从 undospace 中分配一个有效的回滚段； 分配 trx id，并将其加入全局的活跃事务id数组 trx_ids_t、读写事务队列 rw_trx_list 和 活跃事务对象哈希表 shards。 4. 事务提交 事务的提交分为两种方式，一种是隐式提交，一种是显式提交。显式开启一个新的事务，或者执行一条非临时表的DDL语句时，就会隐式的将上一个事务提交掉。另外一种就是显式的执行“COMMIT” 语句来提交事务。 在 MySQL 的 server 层有两个提交函数 trans_commit_stmt 和 trans_commit，前者在每个语句执行完成时都会调用，而后者是在整个事务真正提交时候调用。\n再往前进一层到 handler 接口处，为了保证分布式/多事务引擎事务的一致性，MySQL 实现了经典的 XA 标准，使用两阶段提交 2PC 协议来保证一个全局事务在所有参与节点要么都提交，要么都中止。如果不需要实现分布事务的能力，则不会进行 XA 实际操作。\nMySQL的 XA 事务支持包括内部 XA 和外部 XA。内部 XA 事务主要指单节点实例内部，一个事务跨多个存储引擎进行读写，那么就会产生内部XA事务；另外，若打开 binlog 需要保证 binlog 与引擎修改的一致性，即使事务只涉及一个引擎，MySQL 内部也会启动 XA 事务。外部 XA 事务与内部 XA 事务核心逻辑类似，提供给用户一套 XA 事务的操作命令，包括 XA start，XA end，XA prepre 和 XA commit等，可以支持跨多个节点的 XA 事务。外部 XA 的协调者是用户的应用，参与者是 MySQL 节点，需要应用持久化协调信息，解决事务一致性问题。无论外部XA事务还是内部XA事务，存储引擎实现的都是同一 prepare 和 commit 接口。\n在开启 binlog 情况下，则 XA 控制对象（TC_LOG）为 MYSQL_BIN_LOG；若关闭了binlog，且存在不止一种事务引擎时，则 XA 控制对象（TC_LOG）为 TC_LOG_MMAP；若没有 XA 需求则实际上无需任何协调者，使用的是 TC_LOG_DUMMY。这里下文不对 XA 控制流程做进一步的讨论，主要讨论 InnoDB 层在事务提交时的操作。\n在 tc_log-\u0026gt;commit 阶段会调用引擎层的事务提交接口，InnoDB 的接口函数为 innobase_commit。trans_commit_stmt 和 trans_commit 两者最后都会走到 innobase_commit 中，但其有一个参数 commit_trx 来进一步控制是否真的进行存储引擎层的提交处理，trans_commit_stmt 会设置 commit_trx 为 0 而 trans_commit 会设置为 1。只有当 commit_trx = 1 或者设置 autocommit = 1 的情况下，才会真正进入事务提交逻辑。\n顺便一提在知乎上看到过一个很有意思的说法，有人通过下面这个图来总结出：“结果MySQL的默认表现竟然是允许部分成功的事务提交（写入一条，丢弃一条），也就是丧失了原子性。 没有原子性就没有一致性”。这个说法如果是对数据库原理，或者数据库实现源码了解不太深入的人很容易混淆，它错误的将语句和事务概念混淆，导致了错误的结论。另外，实际业务中应用端是需要对 DB 的操作返回状态进行判断处理的，这也是 rollback 操作存在的意义之一。\nInnoDB 的事务提交 trx_commit_low 主要分成两个部分 trx_write_serialisation_history 和 trx_commit_in_memory。\ntrx_write_serialisation_history 主要是处理事务所使用的 insert / update 回滚段 对于 undo 只使用单个 page 且使用量小于 3/4 的会被设置为 TRX_UNDO_CACHED 状态；其余的 insert undo 状态被设置为 TRX_UNDO_TO_FREE，update undo 状态被设置为 TRX_UNDO_TO_PURGE； 给 trx 分配提交顺序 trx_no，并且加入 serialisation_list； 将使用过的 undo 回滚段内存结构加入 purge_sys-\u0026gt;purge_queue 这一队列（其内按事务提交时的 trx-\u0026gt;no 排列）以给 purge 系统定位回收； 将 trx-\u0026gt;no 写入 rseg/undo header，并 undo header 将加入到 rseg header 中 history list 的开始； 对 update undo，内存结构 trx_undo_t 按状态加入 rseg-\u0026gt;update_undo_cached 或释放 trx_undo_mem_free； trx_commit_in_memory 处理事务锁、Readview、savepoint 等 在 trx_sys 系统中清理当前事务，等到没有隐式锁引用后释放所有事务锁并尝试唤醒阻塞事务； 对 insert undo，内存结构 trx_undo_t 按状态加入 rseg-\u0026gt;insert_undo_list 或释放 trx_undo_mem_free，并且在 rollback segment 中清理不在 history 的 insert undo segment（trx_undo_seg_free）； 非 2PC 时等待 redo 落盘； 清理所有 savepoint 5. 事务回滚 当由于各种原因（例如死锁，或者显式回滚）需要将事务回滚时，会调用接口 trans_rollback，进而调用 InnoDB 函数 trx_rollback_for_mysql 来回滚事务。另外类似 commit 逻辑分为语句级别和事务级别，如果 SQL 语句级别的执行失败，就会进行语句级别的回滚操作 trx_rollback_last_sql_stat_for_mysql 来回滚语句。两种最终又会通过 trx_rollback_to_savepoint 回滚到对应的 savepoint，而 savepoint 是通过 undo_no 来标记回滚到哪个事务状态。\n构建一个回滚用的执行图节点，启动执行图，首先通过 QUE_NODE_ROLLBACK 类型用 trx 上的相关信息构建其内执行的 undo 执行节点（undo_node_t 类型）； 再以 QUE_NODE_UNDO 类型进行具体的数据回滚操作（这块操作可以参见 Undo Log 代码学习 这一章节）。 版权声明：如需转载或引用，请附加本文链接并注明来源。 ","permalink":"https://mzyee.github.io/posts/mysql/trx/","summary":"InnoDB 事务系统和事务流程代码学习","title":"InnoDB 事务系统和事务执行流程"},{"content":"1. 前言 本文讨论 InnoDB 的 Purge 子系统的代码实现，建议先阅读 Undo 系统的介绍。\n2. Purge 系统 InnoDB 控制 purge 操作的结构体是 trx_purge_t，其中主要维护了需要被 purge 的回滚段、purge view、purge 状态位置等。全局 trx_purge_t 结构会在 innodb 启动时的trx_sys_init_at_db_start函数通过扫描所有rollback segment 来初始化设定，其内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 struct trx_purge_t { sess_t *sess; // purge的系统session trx_t *trx; // purge的系统trx，不在 trx list rw_lock_t latch; // purge view，state的保护锁 os_event_t event; // state的信号 ulint n_stop; // 停止追踪器 volatile bool running; // 是否在运行 volatile purge_state_t state; // Coordinator状态：INIT、RUN、STOP、EXIT、DISABLED que_t *query; // 运行purge用的query graph ReadView view; // purge view，大于或出现在这个view的undo不会被purge bool view_active; // purge view是否有效 volatile ulint n_submitted; // 提交的purge任务数目 std::atomic\u0026lt;ulint\u0026gt; n_completed; // 完成的purge任务数目 /* 追踪purge的位置，用于history list truncation */ purge_iter_t iter; // 已经read和parsed的UNDO log位置，一定比limit更新 purge_iter_t limit; // 已经purge（或已经分配马上要purge）的UNDO log位置 bool next_stored; // 标记要purge的下一个undo是否存在下面这些变量中 trx_rseg_t *rseg; // 下一个purge的回滚段 page_no_t page_no; // 下一个purge的undo的page no ulint offset; // 下一个purge的undo的page in-page-offset page_no_t hdr_page_no; // 下一个purge的undo的header page ulint hdr_offset; // // 下一个purge的undo的header page in-page-offset TrxUndoRsegsIterator *rseg_iter; // 用于获取下一个purge的回滚段 purge_pq_t *purge_queue; // 按trx_no排序的要被purge的（update）回滚段，内存 PQMutex pq_mutex; undo::Truncate undo_trunc; // 标记要truncate的undospace mem_heap_t *heap; std::vector\u0026lt;trx_rseg_t *\u0026gt; rsegs_queue; // 存储所有的回滚段 }; 3. Purge 主流程代码 在ddl恢复完成（innobase_post_recover），保证 tablespaces 相关元信息状态一致后，系统会启动 srv_purge_coordinator_thread 和 srv_worker_thread 来进行 undo purge。srv_purge_coordinator_thread 是主要控制 purge 流程的任务线程，在运行期间循环调用 srv_do_purge 去尽可能 purge 所有 undo。在srv_do_purge中每次 purge 一批 undo 会根据系统状态自适应调整 purge 系统所使用的线程数目。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 void srv_purge_coordinator_thread() { // 初始化环境，something... do { // 常态循环 /* 如果上一次purge没有清理东西，则suspend暂停等待信号，避免空转； 另外，外部操作像 FLUSH TABLES FOR EXPORT 会静默tablespace等的也会暂停purge */ if (srv_shutdown_state == SRV_SHUTDOWN_NONE \u0026amp;\u0026amp; (purge_sys-\u0026gt;state == PURGE_STATE_STOP || n_total_purged == 0)) { srv_purge_coordinator_suspend(slot, rseg_history_len); } if (srv_purge_should_exit(n_total_purged)) break; n_total_purged = 0; rseg_history_len = srv_do_purge(\u0026amp;n_total_purged); // 自己做purge } while (!srv_purge_should_exit(n_total_purged)); // 退出清理undo /* 如果不是fast shutdown，确保所有记录被purge，退出阶段也可能有加入undo记录，清理所有后台线程参数的undo */ ulint n_pages_purged = ULINT_MAX; while (srv_fast_shutdown == 0 \u0026amp;\u0026amp; n_pages_purged \u0026gt; 0) { n_pages_purged = trx_purge(1, srv_purge_batch_size, false); } n_pages_purged = trx_purge(1, ut_min(srv_purge_batch_size, 20), true); ut_a(n_pages_purged == 0 || srv_fast_shutdown != 0); // 清理环境，something... } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 static ulint srv_do_purge(ulint *n_total_purged) { // 初始化环境，something... do { // 常态循环尽可能purge // S1. 通过当前系统和history状态，调整purge threads数目：... // S2. 判断是否需要进行undo truncate：... // S3. 调用trx_purge实际做purge n_pages_purged = trx_purge(n_use_threads, srv_purge_batch_size, do_truncate); *n_total_purged += n_pages_purged; // S4. 判断是否有需要truncate的undo space以再次进入：... } while (purge_sys-\u0026gt;state == PURGE_STATE_RUN \u0026amp;\u0026amp; (n_pages_purged \u0026gt; 0 || need_explicit_truncate) \u0026amp;\u0026amp; !srv_purge_should_exit(n_pages_purged)); return rseg_history_len; // 上一批purge前的history长度 } Purge coordinator 的实际 purge 任务是在 trx_purge 中进行分配和进行的， coordinator 会将 undo record 分配给 srv_sys-\u0026gt;tasks 中对应数目的 query thread，Purge worker 直接匹配系统环境的 query thread 拿 query thread node（purge_node_t类型）进行执行。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 ulint trx_purge(ulint n_purge_threads, ulint batch_size, bool truncate) { // S0. 初始化，something... /****************************************************** S1. 获取（clone）最老 read view 用于为 MVCC 限制 purge 的位置 *******************************************************/ rw_lock_x_lock(\u0026amp;purge_sys-\u0026gt;latch, UT_LOCATION_HERE); purge_sys-\u0026gt;view_active = false; trx_sys-\u0026gt;mvcc-\u0026gt;clone_oldest_view(\u0026amp;purge_sys-\u0026gt;view); purge_sys-\u0026gt;view_active = true; rw_lock_x_unlock(\u0026amp;purge_sys-\u0026gt;latch); /****************************************************** S2. 给 purge_sys 每个 query thread 的执行节点（purge_node_t）分配 undo recs *******************************************************/ /* S2.0. 在 trx_commit 时候将 update undo 写入 purge_queue，（insert undo 直接更新成 cache 或者 free）； S2.1. purge_sys 维护了 next undo rec 的位置（purge_sys-\u0026gt;iter），这里从这一位置开始获取batch_size大小的 undo rec S2.2. 将获取的 undo rec 分配给不同的work thread，这里官方先按table ID进行分配，再进行平衡尽可能保证各 worker 的 undo 数目均匀 （查看提交 Bug #32089028 CONCURRENTLY UPDATING MANY JSON DOCUMENTS STEADILY INCREASES IBD FILE SIZE） S2.3. 将 undo rec 真正分配挂到 query thread 的执行节点 purge_node_t 上 */ n_pages_handled = trx_purge_attach_undo_recs(n_purge_threads, batch_size); /****************************************************** S3. 启动并进行 purge 任务 *******************************************************/ // S3.1. 启动所有query thread if (n_purge_threads \u0026gt; 1) { for (ulint i = 0; i \u0026lt; n_purge_threads - 1; ++i) { thr = que_fork_scheduler_round_robin(purge_sys-\u0026gt;query, thr); // 向后台 srv_sys-\u0026gt;tasks 提交任务，以供 purger worker 执行 srv_que_task_enqueue_low(thr); } purge_sys-\u0026gt;n_submitted += n_purge_threads - 1; thr = que_fork_scheduler_round_robin(purge_sys-\u0026gt;query, thr); } else { thr = que_fork_scheduler_round_robin(purge_sys-\u0026gt;query, nullptr); } ++purge_sys-\u0026gt;n_submitted; // S3.2. 执行purge任务，并等待所有worker完成 que_run_threads(thr); purge_sys-\u0026gt;n_completed.fetch_add(1); if (n_purge_threads \u0026gt; 1) trx_purge_wait_for_workers_to_complete(); /****************************************************** S4. 将所有 blob\u0026#39;s first page 延迟到末尾统一释放，避免访问 freed page *******************************************************/ for (thr = UT_LIST_GET_FIRST(purge_sys-\u0026gt;query-\u0026gt;thrs); thr != nullptr; thr = UT_LIST_GET_NEXT(thrs, thr)) { purge_node_t *node = static_cast\u0026lt;purge_node_t *\u0026gt;(thr-\u0026gt;child); node-\u0026gt;free_lob_pages(); } /****************************************************** S5. 进行 undospace truncate *******************************************************/ if (truncate || srv_upgrade_old_undo_found) { trx_purge_truncate(); } return (n_pages_handled); } 4. Purge 物理操作 在row_purge中会对对应 query thread 的 run_node（purge_node_t）中取的 undo record 记录进行 purge，直到 purge 完所有分配的 undo record：\nrow_purge_parse_undo_rec:\n首先逻辑类似 rollback 操作的的第一步，解析 undo record 获取 undo type、trxid、tableid 等操作信息（trx_undo_rec_get_pars，trx_undo_update_rec_get_sys_cols）\n通过 tableid 开启 innodb 表并获取 Shared MDL 锁（dd_table_open_on_id）\n构建 目标主键的 row reference (dtuple_t) ，upd_t，row (dtuple_t)等信息（trx_undo_rec_get_row_ref，trx_undo_update_rec_get_update）\nrow_purge_record:\n对 delete mark (TRX_UNDO_DEL_MARK_REC) 类型的 undo，操作 purge 掉所有二级索 row_purge_remove_sec_if_poss 和主键 row_purge_remove_clust_if_poss 上不再需要的 index record（包括 extern field），这里也是先根据 row reference 索引 BTree 到对应的 cursor 上，删除走的是 btr_cur 的 delete 接口 btr_cur_optimistic_delete 和 btr_cur_pessimistic_delete；\n对于 update existing (TRX_UNDO_UPD_EXIST_REC) 类型的 undo，操作\n5. Purge truncate 前一个阶段 purge 物理操作只是将索引上的数据删除，但是不会处理 undospace 内的空间。虽然 undo log 可以被 purge，但是类似 ibd 文件（不主动optimize）一旦文件增大那么就无法缩小。Innodb 在一个 undo 表空间没有事务使用时，允许将其 truncate 来回收 undo 表空间。回收动作在 coordinator 一批 purge 任务完成后触发，接口为trx_purge_truncate，其内部主要分为 purge rollback segment 和 truncate space 两个部分。\npurge rollback segment（trx_purge_truncate_history）会从所有表空间的所有回滚段回收无用的 undo 数据并清理 history list； truncate space（trx_purge_truncate_undo_spaces）会检索是否存在满足truncate要求的 undospace（1. 手动设置为 inactive 或大小超过限制；2. 空间中 undo 记录被 purge 完全且没有被任何事务使用），并进行物理文件 truncate 操作。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 /** 清理某个 rollback segment */ static void trx_purge_truncate_rseg_history( trx_rseg_t *rseg, /*!\u0026lt; in: rollback segment */ const purge_iter_t *limit) /*!\u0026lt; in: truncate offset */ { // 变量初始化，something... // 从当前 rollback segment 的 history list 上面获取最后位置的 undo log header rseg-\u0026gt;latch(); rseg_hdr = trx_rsegf_get(rseg-\u0026gt;space_id, rseg-\u0026gt;page_no, rseg-\u0026gt;page_size, \u0026amp;mtr); hdr_addr = trx_purge_get_log_from_hist(flst_get_last(rseg_hdr + TRX_RSEG_HISTORY, \u0026amp;mtr)); loop: if (hdr_addr.page == FIL_NULL) { // history 不剩下需要处理的 undo rseg-\u0026gt;unlatch(); mtr_commit(\u0026amp;mtr); return; } undo_page = trx_undo_page_get(page_id_t(rseg-\u0026gt;space_id, hdr_addr.page), rseg-\u0026gt;page_size, \u0026amp;mtr); log_hdr = undo_page + hdr_addr.boffset; seg_hdr = undo_page + TRX_UNDO_SEG_HDR; undo_trx_no = mach_read_from_8(log_hdr + TRX_UNDO_TRX_NO); if (undo_trx_no \u0026gt;= limit-\u0026gt;trx_no) { // 当前要处理的 undo 已经到达、超过 purge limit if (undo_trx_no == limit-\u0026gt;trx_no \u0026amp;\u0026amp; rseg-\u0026gt;space_id == limit-\u0026gt;undo_rseg_space) { // 将所有小于 limit 的 undo normal page free，header page empty trx_undo_truncate_start(rseg, hdr_addr.page, hdr_addr.boffset, limit-\u0026gt;undo_no); } rseg-\u0026gt;unlatch(); mtr_commit(\u0026amp;mtr); return; } prev_hdr_addr = trx_purge_get_log_from_hist(flst_get_prev_addr(log_hdr + TRX_UNDO_HISTORY_NODE, \u0026amp;mtr)); if ((mach_read_from_2(seg_hdr + TRX_UNDO_STATE) == TRX_UNDO_TO_PURGE) \u0026amp;\u0026amp; (mach_read_from_2(log_hdr + TRX_UNDO_NEXT_LOG) == 0)) { // 无log剩余，回收整个 undo segment rseg-\u0026gt;unlatch(); mtr_commit(\u0026amp;mtr); trx_purge_free_segment(rseg, hdr_addr, is_temp); // 内部有trx_purge_remove_log_hdr删除history list节点 } else { // 在history list上删除当前 log header（这里相当于我那次一条history的purge） trx_purge_remove_log_hdr(rseg_hdr, log_hdr, \u0026amp;mtr); rseg-\u0026gt;unlatch(); mtr_commit(\u0026amp;mtr); } // 转移到 history list 上的下一条log mtr_start(\u0026amp;mtr); if (is_temp) { mtr.set_log_mode(MTR_LOG_NO_REDO); } rseg-\u0026gt;latch(); rseg_hdr = trx_rsegf_get(rseg-\u0026gt;space_id, rseg-\u0026gt;page_no, rseg-\u0026gt;page_size, \u0026amp;mtr); hdr_addr = prev_hdr_addr; goto loop; } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 // 进行物理文件 truncate static bool trx_purge_truncate_marked_undo() { // S0. 变量初始化，something... /****************************************************** S1. 获取 MDL 锁 *******************************************************/ MDL_ticket *mdl_ticket; bool dd_result = dd_tablespace_get_mdl(space_name.c_str(), \u0026amp;mdl_ticket, false); // something... /****************************************************** S2. 开始 truncate 操作，切换 undospace *******************************************************/ mutex_enter(\u0026amp;undo::ddl_mutex); if (!trx_purge_truncate_marked_undo_low(space_num, space_name)) { mutex_exit(\u0026amp;undo::ddl_mutex); dd_release_mdl(mdl_ticket); MONITOR_INC_TIME(MONITOR_UNDO_TRUNCATE_MICROSECOND, counter_time_truncate); return (false); } /****************************************************** S3. 删除 undo log file，标志 undo truncate 完成 *******************************************************/ undo::spaces-\u0026gt;x_lock(); undo::done_logging(space_num); undo::spaces-\u0026gt;x_unlock(); // S4. 清理环境，something... return (true); } static bool trx_purge_truncate_marked_undo_low(space_id_t space_num, std::string space_name) { /****************************************************** S2.1. 获取环境 *******************************************************/ // someting... /****************************************************** S2.2. 创建 undo 文件，标志 undo truncate 开始 *******************************************************/ dberr_t err = undo::start_logging(marked_space); // someting... /****************************************************** S2.3. 过滤特殊条件，someting... *******************************************************/ /****************************************************** S2.4. 实际 undo space 文件裁剪轮转 *******************************************************/ /* 计算新 undo space id 和 undo space no； 删除+新建 file tablespace（fil_delete_tablespace + fil_ibd_create）； 重新初始化新 tablespace 文件，构建 undo 文件内容； 重新设置相应 undo space 回滚段内存结构体（Rsegs *m_rsegs） */ bool success = trx_undo_truncate_tablespace(marked_space); // someting... /****************************************************** S2.5. 设定此 undo 空间后面的可用状态 *******************************************************/ space_id_t new_space_id = marked_space-\u0026gt;id(); dd_space_states next_state; undo::spaces-\u0026gt;s_lock(); Rsegs *marked_rsegs = marked_space-\u0026gt;rsegs(); marked_rsegs-\u0026gt;x_lock(); if (marked_rsegs-\u0026gt;is_inactive_explicit()) { // 由外部手动 inactive next_state = DD_SPACE_STATE_EMPTY; marked_rsegs-\u0026gt;set_empty(); } else { // 由后台 purge 选择，可被再使用 next_state = DD_SPACE_STATE_ACTIVE; marked_rsegs-\u0026gt;set_active(); } /****************************************************** S2.6. 在 DD 中更新 space ID 和 state 信息 *******************************************************/ if (DD_FAILURE == dd_tablespace_set_id_and_state(space_name.c_str(), new_space_id, next_state)) return (false); return (true); } 版权声明：如需转载或引用，请附加本文链接并注明来源。 ","permalink":"https://mzyee.github.io/posts/mysql/purge/","summary":"InnoDB Purge System 相关代码学习","title":"InnoDB Purge System 代码学习"},{"content":"1. 前言 已经有不少文章对 InnoDB 的 undo 系统进行了全面介绍，比如：\nInnoDB undo log 漫游 InnoDB之UNDO LOG介绍 庖丁解InnoDB之UNDO LOG 本文主要关注一些 Undo 系统设计/代码实现映射，以及部分笔者关心的代码细节，建议结合阅读上述文章。\n2. Undo的物理组织 首先要明确的一点是 InnoDB 的 undo log 实际上是以表数据格式记录的（所以叫 undo tablespace），并且除了 temporary table 的 undo，正常用户表 undo 都通过 redo 来维护持久性。\n由于通过表空间存储 undo log，所以 undo 文件的基础组织形式是类似 InnoDB 的 ibd 文件，内部有如 File Segment 等概念、有各种文件维护结构的 list、有 page 上文件维护结构相关的 meta，等等。并且分配 undo page 时候也是通过 fsp_reserve_free_extents 这一接口去申请空闲数据页。\n每个 Undo Tablespace 有最多128个可用的 Rollback Segment，Undo Tablespace 第三个页有128个 Rollback Segment 的目录（Rollback Segment Header\u0026rsquo;s Arrary），其中每个元素指向各个 Rollback Segment Header 所在的 page number。初始化时候会处理所有 Rollback Segment，如果未使用则为 FIL_NULL，通过 innodb_rollback_segments 参数控制实际使用数目。\n一个 Rollback Segment Header 包含1024个 slot（四字节），每个 slot 指向 Undo Segment 的 first page number，也就是这个 Undo Segment 的 Undo Header Page。每一时刻，每个使用的 Undo Segment 都被一个事务独占，而每个写事务都会持有至少一个Undo Segment，因此被使用的 Undo Header Page 中只有一个未提交事务的 Undo Segment Header，当活跃事务产生的 undo 超过 Undo Header Page 容量后会被分配新的 Undo Normal Page。\n写事务在修改数据前，会先通过 Undo Log 记录修改前的（主键）数据，并且 undo 分为 insert (UNDO_INSERT) 和 update (UNDO_UPD_EXIST、UNDO_DEL_MARK、UNDO_UPD_DEL) 两类。记录 undo log 前会先写入 Undo Log Header 用于标记和定位，然后逐条写入 Undo Record。\n1 2 3 4 5 6 7 8 9 10 11 12 File Segment |- File Segment Header Rollback Segment |- Rollback Segment Header Undo Segment |- Undo Segment Header Undo Page |- Undo Page Header Undo Header Page Undo Normal Page Undo Log |- Undo Log Header Undo Record |- head, body, tail 可以通过参考资料中的这个图来查看上述物理结构的逻辑关系，更具体的细节结构定义可以查看trx0undo.h文件。\n3. 生成Undo的代码逻辑 分配回滚段 对于只读事务，当事务涉及到对临时表的读写时，会为其分配一个回滚段对其产生的 undo log record 进行记录：\n1 2 trx_assign_rseg_temp() -\u0026gt; get_next_temp_rseg() 对于读写事务，当事务进入读写模式时，会为其分配 trx_id 以及回滚段：\n1 2 3 trx_assign_rseg_durable() -\u0026gt; get_next_redo_rseg() -\u0026gt; get_next_redo_rseg_from_trx_sys() / get_next_redo_rseg_from_undo_spaces() 分配行为只是一个基于轮询的内存操作，顺序为[(space0, rseg0), (space1, rseg0), \u0026hellip; (spaceN, rseg0), (space1, rseg0)\u0026hellip;]，最终会为 trx 返回一个有效的 trx_rseg_t（回滚段的内存结构）。一个事务最多会有2个 rollback segment（temp/durable区分）和 4个 undo segment（insert/update再次区分）。\n分配Undo 1 2 3 4 5 6 7 8 trx_undo_report_row_operation() \u0026lt;- | Path1: btr_cur_ins_lock_and_undo (btr_cur_optimistic_insert / btr_cur_pessimistic_insert) | Path2: btr_cur_upd_lock_and_undo (btr_cur_update_in_place / btr_cur_optimistic_update / btr_cur_pessimistic_update) | Path3: btr_cur_del_mark_set_clust_rec |\u0026lt;- 行级别操作接口 // 通过 BTR_NO_UNDO_LOG_FLAG 标记决定是否记录 undo 函数trx_undo_report_row_operation是写入undo的代码接口，首先会如果当前事务没有分配过所需类型的 undo segment，则会通过trx_undo_assign_undo分配 undo segment/page（trx_undo_t结构），首先尝试从当前 trx_rseg_t 的 insert/update cache中取（接口为trx_undo_reuse_cached，cache 内是如果对应undo事务结束时候所使用的 undo page 所用空间小于3/4则被加入的），cache 中没有则在空间总重新分配一个undo segment（接口为trx_undo_create）。分配好 undo page 从空闲位置开始对应写入 Undo Record，如果 undo page 的剩余空间不满足要求则通过trx_undo_add_page加一个空白的 normal page 记录 Undo Record。\n整个过程中，除了物理 undo page 更新及通过 redo 持久化，还维护了内存中的当前事务所使用的 trx_undo_t 结构体（trx-\u0026gt;rsegs.m_redo/ m_noredo.insert_undo/ update_undo），包括 header page、log header、top undo page/offset等信息。\n写入Undo Reocrd 1 2 3 4 5 6 7 8 9 10 11 12 // Insert类型：TRX_UNDO_INSERT_REC /*-------- header ------*/ Next_record_offset 2B Type_flags 1B Undo_no compressed Table_id compressed /*-------- key field ------*/ Key_field_1_len compressed Key_field_1_data compressed ...(以上 dict_index_get_n_unique 个 field) /*-------- tail ------*/ Prev_record_offset 2B 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 // Update类型：TRX_UNDO_UPD_EXIST_REC、TRX_UNDO_DEL_MARK_REC、TRX_UNDO_UPD_DEL_REC /*-------- header ------*/ Next_record_offset 2B Type_flags 1B Undo_no compressed Table_id compressed Rec_info 1B Trx_id compressed Roll_ptr compressed //指向的是该记录的上一个版本的位置 /*-------- key field ------*/ Key_field_1_len compressed Key_field_1_data len ...(以上 dict_index_get_n_unique 个 field，即索引部分) Update_field_num compressed // 修改的field数目 Update_field_1_pos compressed Update_field_1_len compressed Update_field_1_ctx len //这次修改前的field信息，应用这个可以返回到修改之前到值 ...(以上 upd_t 中个 field 数目) /*-------- tail ------*/ Prev_record_offset 2B Undo Record的内容格式如上所示，这里再额外说明下：\n对于 insert undo，记录的是插入的 entry 的 key 部分，用于回滚时定位寻找并删除，insert 操作不存在可见性判读因此没有记录Trx_id； 对于 update undo，传入的参数时 index_record（rec_t）和 update（upd_t）这个内存结构，实际记录的是 record key 和要被修改的 field，前者用于定位，后者用于还原到原始版本。 对主键数据生成 undo 后就产生了 roll_ptr 并对新 record 进行更新，就是通过 roll_ptr 将 record 的所有版本链接起来。 事务提交时的Undo处理 在 trx_commit 的时候，如果事务发生修改并产生了 undo：\n设置 undo 后续处理状态： undo 使用单个 page 且使用量小于 3/4 的会被设置为 TRX_UNDO_CACHED 状态；其余的 insert undo 状态被设置为 TRX_UNDO_TO_FREE，update undo 状态被设置为 TRX_UNDO_TO_PURGE； 给事务分配提交顺序 trx_no； 对 update undo 回滚段的内存结构，加入 purge_sys-\u0026gt;purge_queue 这一队列（其内按事务提交时的 trx-\u0026gt;no 排列）以给 purge 系统定位回收； 将 trx-\u0026gt;no 写入 rseg/undo header，并 undo header 将加入到 rseg header 中 history list 的开始； 对 update undo，内存结构 trx_undo_t 按状态加入 rseg-\u0026gt;update_undo_cached 或 trx_undo_mem_free； 对 insert undo，内存结构 trx_undo_t 按状态加入 rseg-\u0026gt;insert_undo_list 或 trx_undo_mem_free，并且在 rollback segment 中清理不在 history 的 undo log segment（trx_undo_seg_free）； 4. 应用Undo的代码逻辑 应用 undo 主要是在 mvcc 和 rollback 过程中，而 rollback 又可以分为用户 rollback 和奔溃恢复 rollback。本文主要讨论 rollback 过程，对于 mvcc 操作会开另外的文章专门讨论。\n奔溃恢复时 Rollback 对于奔溃恢复过程中的 Rollback，首先需要恢复 undo 相关信息，然后利用 undo 回滚无效的 DD 相关事务和 user 相关事务：\n通过 redo，在启动（srv_start）的 recv_recovery_from_checkpoint_start -\u0026gt; recv_recovery_begin 阶段恢复 undo 数据； 通过恢复的 undo 数据，在启动（srv_start）的 trx_sys_init_at_db_start 阶段恢复所有回滚段的内存结构，trx_sys 的活跃事物。通过 trx_resurrect 分别扫描 rseg 的 insert_undo_list / update_undo_list，在后台生成崩溃时的活跃事务对象 trx_t，初始化相关参数并加入trx_sys-\u0026gt;rw_trx_set，再按状态加入 trx_sys-\u0026gt;rw_trx_ids（活跃）和 trx_sys-\u0026gt;rw_trx_list（所有）。这里 trx 是否活跃是通过事务对应 undo 的 seg_hdr 中的 TRX_UNDO_STATE 判断的，因此在innodb 层 trx commit 时实际上的持久化就是这个标志位的修改持久化（对应 TRX_UNDO_STATE 的修改在 prepare 和 commit 中, trx_prepare_low -\u0026gt; trx_undo_set_state_at_prepare trx_commit_low -\u0026gt; trx_write_serialisation_history -\u0026gt; trx_undo_set_state_at_finish） 在重启恢复 DD 阶段 (restart_dictionary =\u0026gt; dict_recover)，DICT_RECOVERY_RESTART_SERVER 状态下 srv_dict_recover_on_restart -\u0026gt; trx_rollback_or_clean_recovered(false) 中会加上表锁并（同步的） rollback DD 相关事务（设置了 trx-\u0026gt;ddl_operation 标志）； 在恢复 server 组件 (init_server_components =\u0026gt; post_recover) 阶段末尾（已经完成 Log_DDL 的 post recovery），srv_start_threads_after_ddl_recovery 中开启后台回滚线程，扫描 trx_sys-\u0026gt;rw_trx_list，对后台恢复出来的用户事务，回滚 TRX_STATE_ACTIVE 事务（trx_rollback_active）、清理 COMMITTED_IN_MEMORY 事务（trx_free_resurrected）； 前台运行时，后台回滚线程 trx_recovery_rollback_thread -\u0026gt; trx_rollback_or_clean_recovered(true) 最终通过 trx_rollback_active 反向应用 trx undo log 后提交（rollback 本身也是通过 trx_commit 表示回滚事务完成）来回滚活跃事务。 这里可以看到在 recovery 的前向 redo 是不会使用到 undo 的，已经持久化的 redo 对应需要的 undo 肯定已经被持久化，前向 redo 本身也不在产生额外 undo，也再次可见 undo 内容是以数据模式记录的。另外，DD 相关事务的 undo 和非 DD 相关的表数据 undo 恢复时机不同，前者在启动恢复 DD 系统时恢复，后者在启动后后台恢复。\n回滚应用 Undo Rollback 实际应用 undo 是在 row_undo_step 中:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 // Rollback undo 路径 row_undo_step() -\u0026gt; ... | row_undo_ins // undo insert type | row_undo_ins_parse_undo_rec(undo_node_t) // 将 insert undo 中的内容 parse出来供后续undo使用 | trx_undo_rec_get_pars() // 解析常规参数：是否有LOB field update，type compilation info，undo no | trx_undo_rec_get_row_ref() // 构建 row reference（dtuple_t，即index record的内存对象） | row_undo_search_clust_to_pcur() // 在主键上索引row reference，判断是否找到的主键是需要undo的主键（定位 undo_node_t-\u0026gt;pcur），系统列等也需要一致 | row_undo_ins_remove_sec_rec(undo_node_t, que_thr_t) // 先 删除 所有二级索引（和正向路径相反方向） | row_undo_ins_remove_clust_rec(undo_node_t) // 再 删除 主键级索引 | row_undo_mod // undo undo type | row_undo_mod_parse_undo_rec() | trx_undo_rec_get_pars() | trx_undo_update_rec_get_sys_cols() // 获取老版本record的系统列 | trx_undo_rec_get_row_ref() | trx_undo_update_rec_get_update() // 构建老版本record的update vector用于后续update使用 | row_undo_search_clust_to_pcur() | row_undo_mod_upd_exist_sec() / row_undo_mod_del_mark_sec() / row_undo_mod_upd_del_sec() // 先 更新 二级索引 | row_undo_mod_clust() // 再 更新 主键索引 首先解析需要被应用的 undo，这里涉及开 innodb 表，且在后台回滚情况下这里再需要加上 DD 的 MDL 锁； 在row_undo_search_clust_to_pcur通过 undo 里面的 key 信息在主键上索引 row reference，并根据 roll_ptr 判断找到的主键是否是需要undo的主键。如果找到则构建并存储找到的 index record 的内存结构 dtuple_t，如果是 update，还需构建出来 update 之前的版本（即应用了undo后还原的原先版本）； 对于 insert 的 undo 较为简单，从二级索引到主键删除目标 record； 对于 update 的 undo，在二级索引上，（注意三种 undo 类型是对主键索引 rec 来说的操作类型） UPD_EXIST（原操作为 update 主键记录）的 undo，会判断 undo 链上是否有更新非 delete mark 主键依赖这个二级索引记录，有则标记删除 或 无则物理删除 现有二级索引记录，再 更新 或 插入 undone 的老 record； DEL_MARK（原操作为 delete mark 主键记录）的 undo，会 去除删除标记 现有二级索引记录，如果不存在则 重新插入； UPD_DEL（原操作为 undelete mark，可能 update field 已标记删除的主键记录）的 undo，会根据可见性 标记删除 或 物理删除 现有二级索引记录。 对于 update 的 undo，在主键上则走 btr_cur 的 update 逻辑进行更新，对于 UPD_DEL 还会主动尝试清理标记删除的主键 record。 值得一体的是在崩溃恢复的 undo 事务，事务锁在初始状态下都是隐式锁，是通过其他冲突事务在加锁时通过lock_rec_convert_impl_to_expl 或 undo 应用阶段通过 row_convert_impl_to_expl_if_needed 加上事务锁。此过程中加的都是 record lock，这是由于后台 undo 不被用户所见，其本身不具备可见性要求因此不会加上 next key lock。\n5. 回收Undo的代码逻辑 在 innobase_post_recover 中 ddl恢复完成后，保证 InnoDB metadata 和 file system 一致后的阶段，系统会启动 srv_purge_coordinator_thread 和 srv_worker_thread 来purge undo，相关内容会在 purge 系统中介绍。\n版权声明：如需转载或引用，请附加本文链接并注明来源。 ","permalink":"https://mzyee.github.io/posts/mysql/undo/","summary":"InnoDB Undo Log 相关代码学习","title":"InnoDB Undo Log 代码学习"},{"content":"1. eBPF技术背景 BPF（Berkeley Packet Filter）是类Unix系统上数据链路层的一种原始接口，提供原始链路层封包的收发。BPF在数据包过滤上引入了两大特性：\n一个可有效地工作在基于寄存器结构 CPU 上的虚拟机； 应用程序只复制与过滤数据包相关的数据，不复制数据包的所有信息。 eBPF 在原始BPF基础上进一步针对现代 CPU 硬件进行了指令集优化，增加了 VM 中的寄存器数量，使数据处理速度提高了数倍。总结来说，eBPF 提供了一个基于寄存器的虚拟机，使用自定义的 64 位 RISC 指令集，能够在 Linux 内核内运行本地即时编译的 BPF 程序，并能访问内核功能和内存的特定子集。\neBPF程序分为用户空间程序和内核程序两部分：\n用户空间程序负责加载 BPF 字节码至内核，一般使用者可以通过 LLVM 或者 GCC 将编写的 BPF 代码程序编译成内核可验证的 BPF 字节码。内核加载前会使用验证器 verfier 组件保证字节码的安全性，以避免对内核造成灾难问题。如有需要，用户空间程序也会负责读取并处理内核回传的统计信息或者事件详情。 在内核中加载的 BPF 字节码程序会在内核中执行特定事件，BPF 程序可能基于 kprobes / uprobes / tracepoint / perf_events 等中的一个或多个事件。如有需要，内核程序也会将执行的结果通过缓存 map 或者 perf-event 事件发送至用户空间。 通过上述机制，eBPF 的使用者（不局限于内核开发者）能够基于系统内核或程序事件高效、安全的（在内核中）执行特定代码，通过向内核添加 eBPF 模块来增加功能。\n2. 基于eBPF进行软件开发 eBPF 指令是固定大小的 64 位编码，大约有上百条指令，被分组为 8 类，常用指令支持如从通用内存进行加载/存储，前/后（非）条件跳转、算术/逻辑操作和函数调用等。\n1 2 3 4 5 6 7 8 9 10 11 12 13 /* msb lsb +---------------------+---------------+----+----+-------+ |immediate |offset |src |dst |opcode | +---------------------+---------------+----+----+-------+ */ struct bpf_insn { __u8\tcode;\t/* opcode */ __u8\tdst_reg:4;\t/* dest register */ __u8\tsrc_reg:4;\t/* source register */ __s16\toff;\t/* signed offset */ __s32\timm;\t/* signed immediate constant */ }; 直接使用原始字节码来实现 eBPF 程序，非常像编写汇编代码，这种行为无疑是较为困难，通常使用更高级别的语言和工具来实现功能复杂的 eBPF 用例。一种很自然的想法是能不能将高级语言的中间表示层编译成 eBPF 程序模块，这样我们就可以将使用具有“限制性“的高级抽象层语言来编写 eBPF 程序。这种设计有效地将内核中运行的eBPF字节码的定义（后端）从字节码加载器和前端程序中分离出来。\n基于这样的目的，社区创建了 BCC 项目，使用户可以使用带有限制性的C语言（BPF C）编写 eBPF 后端程序，并且 BCC 为用户封装了许多实用底层函数避免重复造轮子的苦恼。借助 BCC，用户还可以通过编写 python 来快速实现加载器和前端程序。此外，另一个项目 BPFtrace 建立在 BCC 之上，通过特定领域语言提供更高级别的抽象逻辑，以帮助用户实现更快速分析/调试。\n我们以 BCC 的 bashreadline 为例简单介绍下使用 BCC 进行 eBPF 程序开发的流程。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 #!/usr/bin/env python from __future__ import print_function from bcc import BPF from time import strftime import argparse ############################ # part 1 # ############################ # load BPF program bpf_text = \u0026#34;\u0026#34;\u0026#34; #include \u0026lt;uapi/linux/ptrace.h\u0026gt; #include \u0026lt;linux/sched.h\u0026gt; struct str_t { u32 pid; char str[80]; }; BPF_PERF_OUTPUT(events); int printret(struct pt_regs *ctx) { struct str_t data = {}; char comm[TASK_COMM_LEN] = {}; if (!PT_REGS_RC(ctx)) return 0; data.pid = bpf_get_current_pid_tgid() \u0026gt;\u0026gt; 32; bpf_probe_read_user(\u0026amp;data.str, sizeof(data.str), (void *)PT_REGS_RC(ctx)); bpf_get_current_comm(\u0026amp;comm, sizeof(comm)); if (comm[0] == \u0026#39;b\u0026#39; \u0026amp;\u0026amp; comm[1] == \u0026#39;a\u0026#39; \u0026amp;\u0026amp; comm[2] == \u0026#39;s\u0026#39; \u0026amp;\u0026amp; comm[3] == \u0026#39;h\u0026#39; \u0026amp;\u0026amp; comm[4] == 0 ) { events.perf_submit(ctx,\u0026amp;data,sizeof(data)); } return 0; }; \u0026#34;\u0026#34;\u0026#34; ############################ # part 2 # ############################ parser = argparse.ArgumentParser( description=\u0026#34;Print entered bash commands from all running shells\u0026#34;, formatter_class=argparse.RawDescriptionHelpFormatter) parser.add_argument(\u0026#34;-s\u0026#34;, \u0026#34;--shared\u0026#34;, nargs=\u0026#34;?\u0026#34;, const=\u0026#34;/lib/libreadline.so\u0026#34;, type=str, help=\u0026#34;specify the location of libreadline.so library.\\ Default is /lib/libreadline.so\u0026#34;) args = parser.parse_args() name = args.shared if args.shared else \u0026#34;/bin/bash\u0026#34; b = BPF(text=bpf_text) b.attach_uretprobe(name=name, sym=\u0026#34;readline\u0026#34;, fn_name=\u0026#34;printret\u0026#34;) ############################ # part 3 # ############################ # header print(\u0026#34;%-9s %-7s %s\u0026#34; % (\u0026#34;TIME\u0026#34;, \u0026#34;PID\u0026#34;, \u0026#34;COMMAND\u0026#34;)) def print_event(cpu, data, size): event = b[\u0026#34;events\u0026#34;].event(data) print(\u0026#34;%-9s %-7d %s\u0026#34; % (strftime(\u0026#34;%H:%M:%S\u0026#34;), event.pid, event.str.decode(\u0026#39;utf-8\u0026#39;, \u0026#39;replace\u0026#39;))) b[\u0026#34;events\u0026#34;].open_perf_buffer(print_event) while 1: try: b.perf_buffer_poll() except KeyboardInterrupt: exit() 可以将上面这个 eBPF 程序分为3个部分：\nPart 1 是用 BPF C 实现的 eBPF 后端程序，首先这里构建了内置 BPF_PERF_OUTPUT 结构来创建一个 BPF table，通过 perf 环形缓冲区将自定义事件数据推送到用户空间。用户可以通过 events 来获取推送的数据。然后在 printret 函数中通过 PT_REGS_RC 来判断、获取当前环境函数的返回值，并通过内置函数获取程序pid和返回值。通过内置函数 bpf_get_current_comm 判断当前程序名称，如果是 bash 命令就向 events 输出程序运行返回值。 Part 2 是通过 BCC 的支持利用 python 进行前述 eBPF 后端程序的加载器。加载类型是 uretprobe，即将程序挂载到 user-level 的 readline() 函数上，即在用户调用 readline() 函数返回时执行相应 eBPF 后端代码。 Part 3 是显示输出的前端程序，用户对 events 缓冲区进行polling，当缓冲区有内容时将对应内容输出显示。 至此，通过运行这一个 BCC 程序，用户就可以通过 eBPF实现监控所有 bash 进程的 readline 函数，输出对应 pid 和 readline 对返回结果（即 bash 输入的命令内容）。更多的内置功能及接口自行参见相关项目官网。\n事件类型 前面提到 BPF 程序类型可能基于 kprobes / uprobes / tracepoint /perf_events 等事件中的一个或多个，其中：\nkprobes：内核中动态跟踪。可以跟踪到 Linux 内核中的函数入口或返回点，但不是稳定接口，可能会因为内核版本变化导致跟踪失效。 理论上可以跟踪到所有导出符号 /proc/kallsyms 但不在 /sys/kernel/debug/kprobes/blacklist 中的函数。 uprobes：用户级别的动态跟踪。与 kprobes 类似，只是跟踪的函数为用户程序中的函数。 tracepoints：内核中静态跟踪。tracepoints 是内核开发人员维护的跟踪点，能够提供稳定的接口，但是需维护数量且场景受限。 USDT：用户静态探针，类似 tracepoints 但是需要用户空间自己维护。 perf_events：定时采样和 PMC 事件。 eBPF 模块各种类型事件的加载执行在后面有时间的话再展开讲讲，有兴趣的可以搜索 Linux 内核的 text hook/poke、static-key 和 static-jump 等机制实现。\n3. 常用eBPF工具与指令 BCC 和 BPFtrace 提供了很多现成实用的工具，下面分享一些我觉得有用的工具，具体参数参考各个命令的help查看。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # 检查 pid程序打开的文件 opensnoop -p 123 # 检测 disk 的 I/O 状态 biolatency [-D for each disk] [-Q include OS queued time in I/O time] # 追踪pid程序fs操作大于10ms的操作 ext4slower 10 -p 123 # 追踪pid程序fs操作分布 ext4dist -p 123 # 系统 time 时间的 cache 状态 cachestat time llcstat time # 函数 count / latency / call-interval 统计: funccount \u0026#39;./test:read*\u0026#39; -p 123 -d 1 funclatency \u0026#39;./test:read*\u0026#39; -p 123 -d 1 funcinterval \u0026#39;./test:read*\u0026#39; -p 123 -d 1 # on-CPU 和 off-CPU 火焰图，采集 DURATION 秒 profile -p 123 -f DURATION --stack-storage-size=165535 \u0026gt; profile01.txt flamegraph.pl --width=1600 \u0026lt; profile01.txt \u0026gt; profile01.svg offcputime -p 123 -f DURATION --stack-storage-size=165535 \u0026gt; offcpu01.txt flamegraph.pl --width=1600 \u0026lt; offcpu01.txt \u0026gt; offcpu01.svg # 采集 pid 程序cache-misses超过的10000次线程及其10000次的次数 bpftrace -e \u0026#39;hardware:cache-misses:10000 /pid==123/ { @[comm, tid] = count(); }\u0026#39; 版权声明：如需转载或引用，请附加本文链接并注明来源。 ","permalink":"https://mzyee.github.io/posts/perf/ebpf/","summary":"Learning About eBPF Usages","title":"通过 eBPF 工具进行性能分析"},{"content":"友链 baotiao baotiao.github.io catkang catkang.github.io YYM yym-46-57 rongbiaoxie rongbiaoxie.github.io whoiami whoiami.github.io Leviathan leviathan.vip Songzhao kernelmaker.github.io Content E-mail: mzyeee@gmail.com\n知乎: https://www.zhihu.com/people/mzy-59-9\nResearchGate: https://www.researchgate.net/profile/Zheyu-Miao\nGithub: https://github.com/mzyee\n","permalink":"https://mzyee.github.io/contact/","summary":"友链 baotiao baotiao.github.io catkang catkang.github.io YYM yym-46-57 rongbiaoxie rongbiaoxie.github.io whoiami whoiami.github.io Leviathan leviathan.vip Songzhao kernelmaker.github.io Content E-mail: mzyeee@gmail.com\n知乎: https://www.zhihu.com/people/mzy-59-9\nResearchGate: https://www.researchgate.net/profile/Zheyu-Miao\nGithub: https://github.com/mzyee","title":""}]