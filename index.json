[{"content":"前言 InnoDB 的 redo log 模块是保证事务持久性的核心，InnoDB 遵守 WAL 原则保证总是日志先行，即在持久化数据文件时保证其对应的 redo 日志已经写到磁盘，这样在崩溃的情况下，它就可以用于恢复对已修改但尚未刷新到磁盘的页面的修改。本文主要讨论 InnoDB 中 redo 日志的物理组织格式，内存结构及前后向的生成/应用流程。可以参考阅读文档：\n数据库故障恢复机制：ARIES、ARIES/IM InnoDB Redo Log 官方介绍 InnoDB redo log 漫游 庖丁解 InnoDB 之 REDO LOG Redo 日志的物理格式 在 8.0.30 版本前，MySQL 通过 innodb_log_file_size 和 innodb_log_files_in_group 分别控制 redo 文件的大小和数目，文件名为 ib_logfilexx；在 8.0.30 版本后，官方新加了参数 innodb_redo_log_capacity 并允许动态配置 redo 日志的总容量，系统一共维护了 32 个文件名为 #ib_redoxx 和 #ib_redoxx_tmp，具有 _tmp 后缀的文件为空余未使用的文件。redo 日志中的数据是以 append 的形式不断增加，文件中任一位点的数据对应一个永久递增的 LSN 号标志。\n一个 redo 文件首先以 LOG_FILE_HDR_SIZE (2KB = 4*512B) 大小的 file header 开头，包含 header info block、checkpoint 1、encryption info、checkpoint 2 这 4 个block；其中 header info block 记录了一些，4字节的 Log 版本 FORMAT、4字节 LOG_UUID、8字节 START_LSN 标识的当前文件开始LSN、最长32位的 Creator 信息表示当前 mysql 版本。有两个 checkpoint 的原因是通过 double write 机制防止单个 checkpoint 记录因为写盘过程中间 crash 而损坏（现在多数 SSD 支持原子写 4K 粒度）。 这里需要注意的是在 8.0 之前的版本中，checkpoint_lsn 一定指向一个 mtr record group 的开头，并且该 mtr record 应该被恢复（虽然相关页面仍然可能已被刷写下去）。但从 8.0 开始，由于 recent_closed buffer 的存在，这个值可能指向 record group 中间的某个字节，在这种情况下，恢复应该跳过包含检查点 lsn 的日志记录组并从其下一条开始。redo 文件中写入的 checkpoint_lsn 一定在此 redo 的 lsn 范围。\n接着 file header 的是一个个 OS_FILE_LOG_BLOCK_SIZE (512B) 大小的 log block，包括12字节的 Header (block number + data length + first record offset + checkpoint number)、496个字节的 Body、4字节的 Tailer (checksum)。对于 Header 部分的说明是：\n4字节 Block Number，老版本中 Flush Flag 占用最高位bit标识一次 I/O 的第一个 Block，剩下的31个 bit 是当前 Block 编号 2字节 Data Length，长度为 0 为 未使用 block（reuse 情况下可能无效），长度为 [12 , 508) 表示最后一个未写完的 block， 长度为 512 表示写完整的 block； 2字节 First Record Group Offset，用来指向 Block 中第一个 mtr record group 的开始位置，如果和 Data Length 相同则说明内部没有开启记录新的 mtr record group； 4字节 Epoch Number，和 Block Number 一起组成了 block 的唯一标志，通过 64 位 block start lsn 转换获得。在老版本中，这里记录的是每次刷 checkpoint 时推进的 log_sys-\u0026gt;next_checkpoint_no 的低四位。 在 log block 的 body 部分，则是一条条实际的 redo log record，或者在 InnoDB 中称为 mtr record 更为贴切。每个 redo record 的第一个 Byte 是这个记录的类型，其中最高位的 bit 为 MLOG_SINGLE_REC_FLAG，如果被设置则表示此 mtr 只包含了（最多单个页相关）的单条记录，否则是由多条单 record 组成的 mtr record group，并且在 group 结尾会以 MLOG_MULTI_REC_END标记；之后以压缩格式记录当前 record 对应的 space id 和 page no（除 TABLE_DYNAMIC_META 等类型）；接着是当前 record 对应的 record body 部分内容，具体内容会由 record 的类型决定。\nInnoDB 的 redo 是 Physiological Logging，网上一种常用说法是 “Physical to a page，logical within a page”，实际上可以理解是在记录日志时对前向过程日志记录量和后向日志恢复速度的优化考虑，在对于 page 内的修改日志并非一定是 logical 的，但在对于一个或多个 page 的固定模式的修改可以通过 logical log 来减小 redo 记录量。另外在 undo 相关的分析文章中提到过 undo 表空间数据也是通过 redo 来维护的，而 undo 本身是基于逻辑而非物理去做回滚的。\nRedo 日志的生成 首先有必要介绍 InnoDB 中 mtr (mini-transaction) 的概念，InnoDB 中对于物理文件的修改都是以 mtr 作为原子单位（无论其内是 single 还是 multi record 的）。一个 mtr 在前向执行过程中会占据所需的资源，包括 index、page 及其对应物理锁等，以保证并发操作正确性，并将数据修改操作对应生成的 redo 在 mtr 内部缓存。在 mtr commit 的时候此 mtr 会将缓存的 redo record 提交到全局 log buffer 中等待落盘。需要注意的是，在内存逻辑来看（不发生crash情况下）mtr 一旦提交就代表了这个物理操作在系统全局产生持久化效果，哪怕对应所属的 transaction 还没有或最终最终提交，因此这里在必须时就需要 undo mtr 的操作，因此在对应数据操作前都需要先记录 undo；此外，部分物理操作可能不会被撤销，比如空间拓展分配等。\n如果 mtr 产生 redo log 则其在提交过程中：\n根据数据长度向 log_sys 申请对应的全局顺序记录的 sn 范围，其通过加上 block header 和 tailer，不包括 file header 可以转换为对应的 lsn 范围；并且等待到 log buffer 的空间足够对应 sn 范围数据被写入； 对于 mtr 中缓存的所有 redo record，按 log block 的格式将 header 和 tailer 留空 copy body 内容，并将 header 中的 First Record Group Offset 字段（写完后的下一个）填上； 等待至 recent_written link_buf 有空闲位置，表示到这一位置的 lsn 对应的 log 可以被写入；再推进此结构的中相应的 lsn 范围，表示这一段内容已经完整写入到 log buffer 中，并尝试推进其 m_tail（已经连续完整写入到位置）； 等待至 recent_closed link_buf 有空闲位置，表示到这一位置的 lsn 对应的 dirty page 可以被加到 flush list 上；再将 dirty page 挂到 flush list 上； 最终释放所有 page 锁等独占资源。 Redo 日志的写入 在 mtr redo record 写入到全局 log buffer 中后，系统通过全局结构 log_t *log_sys 和后台工作线程来写入 redo 日志，并维护如各种 lsn 位点等相关状态。 后台工作线程有如下这些：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 /* 控制 log 文件的轮转 */ void log_files_governor(log_t *log_ptr); /* 将全局 log buffer 中的日志写入到 OS buffer 中 */ void log_writer(log_t *log_ptr); /* 将 OS buffers 中的数据刷写到磁盘上 (fsyncs) */ void log_flusher(log_t *log_ptr); /* 通知用户线程对应 log write 完成，write_lsn 已更新 */ void log_write_notifier(log_t *log_ptr); /* 通知用户线程对应 log flush 完成，flushed_to_disk_lsn 已更新 */ void log_flush_notifier(log_t *log_ptr); /* 检查是否需要要求完成强制刷脏并写入 checkpoint lsn 到日志文件中 */ void log_checkpointer(log_t *log_ptr); 这里刷写相关的任务其实都是围绕 log file 和 log buffer 首位端的推进：\n系统维护了所有的 log file 的内存对象 Log_files_dict，并通过 log_files_governor 控制已写入完全（可 purge 或仍需要）、当前正在写入、后续可使用的 redo 文件状态； log buffer 头部的推进的相关状态，buf_limit_sn（限制可被写入到 buffer 的最大 sn，即 wirte_lsn + buf_size）、recent_written buffer（追踪控制 mtr 并发写入状态 log buffer）、recent_closed buffer（追踪控制 dirty page 挂载 flush list）； log buffer 刷写状态，到 recent_written tail 位置的 redo 已经完整可写盘（这里需要等至上一次 checkpoint 加 redo file 总容量超过目标写入 lsn），log writer 会将 log block 的 header 和 tailer 填充，然后确定写入范围直接从 log buffer（完整 log_write_ahead_size 大小倍数）或通过 write_ahead_buffer（小于 log_write_ahead_size） 写入 log file； log buffer 尾部的推进的相关状态，如 write_lsn、flushed_to_disk_lsn，事件通知等。 checkpoint 推进，并对 log file 尾部文件的回收。 对应线程具体的执行流程介绍可以参考官方文档。 Redo 日志应用及 InnoDB 奔溃恢复 在奔溃恢复的情况下，InnoDB通过应用 redo 来恢复已经提交但还没有刷盘的事务数据。另外，一些基于物理复制架构的数据库，像 PolarDB、AWS Aurora 等，还存在通过应用 redo 来进行数据同步的流程。我们这里只讨论 recovery 的 redo 阶段，整个 InnoDB server 的启动简化流程如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 dberr_t srv_start(bool create_new_db) { // Step 0. 环境准备和检查，... // Step 1. 初始化 SRV 变量，... srv_boot(); // Step 2. 扫描配置的目录，生成文件map fil_init(innobase_get_open_files_limit()); // 确定路径... // 扫描获取 .IBD 和 undo 文件，从首 page 中读取 space_id 并记录到 fil_system 的 Tablespace_dirs err = fil_scan_for_tablespaces(); // Step 3. INNODB STATUS 监控文件... // Step 4. 初始化 AIO 线程... os_aio_init(srv_n_read_io_threads, srv_n_write_io_threads) // Step 5. 初始化 buffer pool... err = buf_pool_init(srv_buf_pool_size, srv_buf_pool_instances); // Step 6. 初始化多个子系统... fsp_init(); pars_init(); recv_sys_create(); recv_sys_init(); trx_sys_create(); lock_sys_create(srv_lock_table_size); os_aio_start_threads();/* i/o-handler threads */ buf_flush_page_cleaner_init(); // Step 7. 初始化 system tablespace... // ibdata1 文件内含 innodb 表的元数据等，创建对应 sysspace 并加入 fil_system (fil_space_create + fil_node_create) err = srv_sys_space.open_or_create(false, create_new_db, \u0026amp;sum_of_new_sizes, \u0026amp;flushed_lsn); dict_persist_init(); // Step 8. 初始化 log sys，这里还会扫描出所有 redo file 来构建 Log_files_dict err = log_sys_init(create_new_db, flushed_lsn, new_files_lsn); //... if (create_new_db) { // 初始化建立新 db } else { // Step 9. BP状态重置 err = dblwr::v1::init(); /* 初始化 double write */ buf_pool_invalidate(); /* 淘汰所有 page 确保 recovery 重读 */ // Step 10. 打开所有 log files 和 system data files fil_open_system_tablespace_files(); // Step 11. 开始恢复 redo 日志... err = recv_recovery_from_checkpoint_start(*log_sys, flushed_lsn); // ... 初始化 innodb data dictionary system // Step 12. 启动后台 log 线程 ... if (!srv_read_only_mode) { log_start_background_threads(*log_sys); } // Step 13. 应用剩余的最后一批 hashed log records if (srv_force_recovery \u0026lt; SRV_FORCE_NO_LOG_REDO) { err = recv_apply_hashed_log_recs(*log_sys, !recv_sys-\u0026gt;is_cloned_db \u0026amp;\u0026amp; !log_upgrade); } // 一些检查... // Step 14. 刷写所有脏页 if (!srv_force_recovery \u0026amp;\u0026amp; !srv_read_only_mode) { buf_flush_sync_all_buf_pools(); } // Step 15. 完成 redo 日志恢复后的清理，恢复dynamic metadata MetadataRecover *dict_metadata = recv_recovery_from_checkpoint_finish(false); /* 此时 DD（table persistent data）还没有完全 recovery */ if (!recv_sys-\u0026gt;is_cloned_db \u0026amp;\u0026amp; !dict_metadata-\u0026gt;empty()) { fil_space_t *space = fil_space_acquire_silent(dict_sys_t::s_dict_space_id); if (space == nullptr) { dberr_t error = fil_ibd_open(true, FIL_TYPE_TABLESPACE, dict_sys_t::s_dict_space_id, predefined_flags, dict_sys_t::s_dd_space_name, dict_sys_t::s_dd_space_file_name, true, false); } else { fil_space_release(space); } dict_persist-\u0026gt;table_buffer = ut::new_withkey\u0026lt;DDTableBuffer\u0026gt;(UT_NEW_THIS_FILE_PSI_KEY); dict_metadata-\u0026gt;store(); // 将恢复过程中persistent dynamic metadata修改 store 到 mysql.innodb_dynamic_metadata log_buffer_flush_to_disk(*log_sys); } ut::delete_(dict_metadata); // Step 16. 构建 Undo Tablespaces 和 Rollback Segments 内存结构并进行恢复 err = srv_undo_tablespaces_init(false); trx_purge_sys_mem_create(); purge_queue = trx_sys_init_at_db_start(); srv_undo_tablespaces_upgrade(); trx_purge_sys_initialize(srv_threads.m_purge_workers_n, purge_queue); } /* Open temp-tablespace and keep it open until shutdown. */ err = srv_open_tmp_tablespace(create_new_db, \u0026amp;srv_tmp_space); err = ibt::open_or_create(create_new_db); // Step 17. 完整化 undo 表空间 // 增加 rollback segment 数目到 srv_rollback_segments，此次配置可能和上次不同 trx_rseg_adjust_rollback_segments(srv_rollback_segments); // 构建完成完整 undo 表空间，删除 trunc.log，设置active srv_undo_tablespaces_mark_construction_done(); undo::spaces-\u0026gt;s_lock(); for (auto undo_space : undo::spaces-\u0026gt;m_spaces) { if (!undo_space-\u0026gt;is_empty()) { undo_space-\u0026gt;set_active(); } } undo::spaces-\u0026gt;s_unlock(); // Step 18. 监控系统等... // Step 19. something... return (DB_SUCCESS); } 其中比较关键的阶段在于 Step 11 recv_recovery_from_checkpoint_start 中进行 redo 恢复以及 Step 16 恢复 undo 和 trx 系统的状态，后者在 undo 系统的讨论中进行介绍，本文主要讨论前者。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 dberr_t recv_recovery_from_checkpoint_start(log_t \u0026amp;log, lsn_t flush_lsn) { // Step 1. 初始化flush_rbt，保证脏页按序插入flush list buf_flush_init_flush_rbt(); // Step 2. 通过扫描 Log_files_dict 找最后的 checkpoint 记录 Log_checkpoint_location checkpoint; recv_find_max_checkpoint(log, checkpoint); // ... /* Step 3. 解析存储 redo record： 1. 从 checkpoint 读取 redo 文件暂存到 log_sys-\u0026gt;buf； 2. 按 log block 扫描去 block head/tail 存到 recv_sys-\u0026gt;buf 中； 3. 将 recv_sys-\u0026gt;buf 中的记录 parse，生成 recv_t *recv（单个 record）和 recv_addr_t *recv_addr（page 所有 record 串）； 4. 存储 recv_addr_t 到 recv_sys 哈希表对应的 (space_id, page_no) 处 5. （如果占用内存较大）将所有存储的 record apply 到 page 上，bp内的走 recv_recover_page、bp外的走 buf_read_recv_pages */ recv_recovery_begin(log, checkpoint_lsn); // Step 4. 初始化 log_sys 状态，包括各 lsn 位点、各 buf 状态 lsn_t recovered_lsn = log.recovered_lsn = recv_sys-\u0026gt;recovered_lsn; auto check_scanned_lsn = log.m_scanned_lsn; if (check_scanned_lsn % OS_FILE_LOG_BLOCK_SIZE == 0) { // If it is at block boundary, add header size. check_scanned_lsn += LOG_BLOCK_HDR_SIZE; } err = log_start(log, checkpoint_lsn, recovered_lsn, false); if (!srv_read_only_mode) { log.next_checkpoint_header_no = log_next_checkpoint_header(checkpoint.m_checkpoint_header_no); err = log_files_next_checkpoint(log, checkpoint_lsn); } mutex_enter(\u0026amp;recv_sys-\u0026gt;mutex); recv_sys-\u0026gt;apply_log_recs = true; mutex_exit(\u0026amp;recv_sys-\u0026gt;mutex); return DB_SUCCESS; } 对应 redo 的使用主要在于 parse 和 apply 两个阶段，分别对应 recv_parse_log_recs 和 recv_apply_log_rec 两个接口。parse 阶段完成后 recv_sys 内 redo record hash 的结构层次如下，存储的是对应 (space,page) 的解析完成的 redo record body。\n1 2 3 4 recv_sys =\u0026gt; space_m =\u0026gt; page_x =\u0026gt; recv_addr_t =\u0026gt; recv_t(data1)-\u0026gt;recv_t(data2)-\u0026gt;recv_t(data3) =\u0026gt; page_y =\u0026gt; recv_addr_t =\u0026gt; recv_t(data1)-\u0026gt;... =\u0026gt; ... space_n =\u0026gt; ... 解析的逻辑较为简单，由于 InnoDB 对于单个 redo record 不会记录长度，因此就是通过 redo 类型确定走不同解析逻辑确定 redo record 长度（这里 MariaDB 记录了长度来加速，当然其 redo 内容也还有许多其他修改）。需要注意的是对应一些非物理 page 的 redo (如 MLOG_FILE_EXTEND) 或特殊 page (如 page 0) 元信息修改，在 parse 阶段就会做额外的处理工作来维护状态。\n应用的逻辑触发逻辑有两种，对于 bp 内的 page 直接走 recv_recover_page，对于 bp 外的 page 在读 I/O 完成时 buf_page_io_complete 也是调用 recv_recover_page（唯一不同的是后者会转移 page 的 x-latch 的 ownership 到当前线程）。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 void recv_recover_page_func(bool just_read_in, buf_block_t *block) { mutex_enter(\u0026amp;recv_sys-\u0026gt;mutex); if (recv_sys-\u0026gt;apply_log_recs == false) { mutex_exit(\u0026amp;recv_sys-\u0026gt;mutex); return; } recv_addr_t *recv_addr = recv_get_rec(block-\u0026gt;page.id.space(), block-\u0026gt;page.id.page_no()); if (recv_addr == nullptr || recv_addr-\u0026gt;state == RECV_BEING_PROCESSED || recv_addr-\u0026gt;state == RECV_PROCESSED) { mutex_exit(\u0026amp;recv_sys-\u0026gt;mutex); return; } recv_addr-\u0026gt;state = RECV_BEING_PROCESSED; mutex_exit(\u0026amp;recv_sys-\u0026gt;mutex); mtr_t mtr; mtr_start(\u0026amp;mtr); mtr_set_log_mode(\u0026amp;mtr, MTR_LOG_NONE); // redo 应用不在记录redo // 获取 page block page_t *page = block-\u0026gt;frame; page_zip_des_t *page_zip = buf_block_get_page_zip(block); if (just_read_in) { rw_lock_x_lock_move_ownership(\u0026amp;block-\u0026gt;lock); } bool success = buf_page_get_known_nowait( RW_X_LATCH, block, Cache_hint::KEEP_OLD, __FILE__, __LINE__, \u0026amp;mtr); ut_a(success); // 获取 page lsn lsn_t page_lsn = mach_read_from_8(page + FIL_PAGE_LSN); lsn_t page_newest_lsn = buf_page_get_newest_modification(\u0026amp;block-\u0026gt;page); lsn_t end_lsn = 0; lsn_t start_lsn = 0; bool modification_to_page = false; if (page_newest_lsn) { page_lsn = page_newest_lsn; } for (auto recv : recv_addr-\u0026gt;rec_list) { end_lsn = recv-\u0026gt;end_lsn; byte *buf = nullptr; if (recv-\u0026gt;len \u0026gt; RECV_DATA_BLOCK_SIZE) { buf = static_cast\u0026lt;byte *\u0026gt;(ut::malloc_withkey(UT_NEW_THIS_FILE_PSI_KEY, recv-\u0026gt;len)); recv_data_copy_to_buf(buf, recv); } else if (recv-\u0026gt;data != nullptr) { buf = ((byte *)(recv-\u0026gt;data)) + sizeof(recv_data_t); } if (recv-\u0026gt;type == MLOG_INIT_FILE_PAGE) { // init page 类型先修正 page lsn，相当于第一次会强制做 apply page_lsn = page_newest_lsn; memset(FIL_PAGE_LSN + page, 0, 8); memset(UNIV_PAGE_SIZE - FIL_PAGE_END_LSN_OLD_CHKSUM + page, 0, 8); if (page_zip) memset(FIL_PAGE_LSN + page_zip-\u0026gt;data, 0, 8); } // 过滤条件：1. page lsn 不超过 redo 的 lsn；2. 对于 undo 空间没有 truncate if (recv-\u0026gt;start_lsn \u0026gt;= page_lsn \u0026amp;\u0026amp; undo::is_active(recv_addr-\u0026gt;space)) { lsn_t end_lsn; unsigned char *buf_end = nullptr; if (!modification_to_page) { modification_to_page = true; start_lsn = recv-\u0026gt;start_lsn; } if (buf != nullptr) { buf_end = buf + recv-\u0026gt;len; } // 这里按照 redo 类型对 page 真正进行数据修改恢复 recv_parse_or_apply_log_rec_body(recv-\u0026gt;type, buf, buf_end, recv_addr-\u0026gt;space, recv_addr-\u0026gt;page_no, block, \u0026amp;mtr, ULINT_UNDEFINED, LSN_MAX); end_lsn = recv-\u0026gt;start_lsn + recv-\u0026gt;len; // 更新 page lsn mach_write_to_8(FIL_PAGE_LSN + page, end_lsn); mach_write_to_8(UNIV_PAGE_SIZE - FIL_PAGE_END_LSN_OLD_CHKSUM + page, end_lsn); if (page_zip) mach_write_to_8(FIL_PAGE_LSN + page_zip-\u0026gt;data, end_lsn); ++applied_recs; } else { ++skipped_recs; } if (recv-\u0026gt;len \u0026gt; RECV_DATA_BLOCK_SIZE) ut::free(buf); } // 有修改要加入脏页 if (modification_to_page) { buf_flush_recv_note_modification(block, start_lsn, end_lsn); } mtr_commit(\u0026amp;mtr); // 更新 recv_sys 状态 mutex_enter(\u0026amp;recv_sys-\u0026gt;mutex); if (recv_max_page_lsn \u0026lt; page_lsn) recv_max_page_lsn = page_lsn; recv_addr-\u0026gt;state = RECV_PROCESSED; --recv_sys-\u0026gt;n_addrs; mutex_exit(\u0026amp;recv_sys-\u0026gt;mutex); } 版权声明：如需转载或引用，请附加本文链接并注明来源。 ","permalink":"https://mzyee.github.io/posts/mysql/redo/","summary":"InnoDB Redo 日志的前后向流程","title":"InnoDB Redo 日志系统"},{"content":"前言 本文讨论 InnoDB 的 Purge 子系统的代码实现，建议先阅读 Undo 系统的介绍。\nPurge 系统 InnoDB 控制 purge 操作的结构体是 trx_purge_t，其中主要维护了需要被 purge 的回滚段、purge view、purge 状态位置等。全局 trx_purge_t 结构会在 innodb 启动时的trx_sys_init_at_db_start函数通过扫描所有rollback segment 来初始化设定，其内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 struct trx_purge_t { sess_t *sess; // purge的系统session trx_t *trx; // purge的系统trx，不在 trx list rw_lock_t latch; // purge view，state的保护锁 os_event_t event; // state的信号 ulint n_stop; // 停止追踪器 volatile bool running; // 是否在运行 volatile purge_state_t state; // Coordinator状态：INIT、RUN、STOP、EXIT、DISABLED que_t *query; // 运行purge用的query graph ReadView view; // purge view，大于或出现在这个view的undo不会被purge bool view_active; // purge view是否有效 volatile ulint n_submitted; // 提交的purge任务数目 std::atomic\u0026lt;ulint\u0026gt; n_completed; // 完成的purge任务数目 /* 追踪purge的位置，用于history list truncation */ purge_iter_t iter; // 已经read和parsed的UNDO log位置，一定比limit更新 purge_iter_t limit; // 已经purge（或已经分配马上要purge）的UNDO log位置 bool next_stored; // 标记要purge的下一个undo是否存在下面这些变量中 trx_rseg_t *rseg; // 下一个purge的回滚段 page_no_t page_no; // 下一个purge的undo的page no ulint offset; // 下一个purge的undo的page in-page-offset page_no_t hdr_page_no; // 下一个purge的undo的header page ulint hdr_offset; // // 下一个purge的undo的header page in-page-offset TrxUndoRsegsIterator *rseg_iter; // 用于获取下一个purge的回滚段 purge_pq_t *purge_queue; // 按trx_no排序的要被purge的（update）回滚段，内存 PQMutex pq_mutex; undo::Truncate undo_trunc; // 标记要truncate的undospace mem_heap_t *heap; std::vector\u0026lt;trx_rseg_t *\u0026gt; rsegs_queue; // 存储所有的回滚段 }; Purge 主流程代码 在ddl恢复完成（innobase_post_recover），保证 tablespaces 相关元信息状态一致后，系统会启动 srv_purge_coordinator_thread 和 srv_worker_thread 来进行 undo purge。srv_purge_coordinator_thread 是主要控制 purge 流程的任务线程，在运行期间循环调用 srv_do_purge 去尽可能 purge 所有 undo。在srv_do_purge中每次 purge 一批 undo 会根据系统状态自适应调整 purge 系统所使用的线程数目。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 void srv_purge_coordinator_thread() { // 初始化环境，something... do { // 常态循环 /* 如果上一次purge没有清理东西，则suspend暂停等待信号，避免空转； 另外，外部操作像 FLUSH TABLES FOR EXPORT 会静默tablespace等的也会暂停purge */ if (srv_shutdown_state == SRV_SHUTDOWN_NONE \u0026amp;\u0026amp; (purge_sys-\u0026gt;state == PURGE_STATE_STOP || n_total_purged == 0)) { srv_purge_coordinator_suspend(slot, rseg_history_len); } if (srv_purge_should_exit(n_total_purged)) break; n_total_purged = 0; rseg_history_len = srv_do_purge(\u0026amp;n_total_purged); // 自己做purge } while (!srv_purge_should_exit(n_total_purged)); // 退出清理undo /* 如果不是fast shutdown，确保所有记录被purge，退出阶段也可能有加入undo记录，清理所有后台线程参数的undo */ ulint n_pages_purged = ULINT_MAX; while (srv_fast_shutdown == 0 \u0026amp;\u0026amp; n_pages_purged \u0026gt; 0) { n_pages_purged = trx_purge(1, srv_purge_batch_size, false); } n_pages_purged = trx_purge(1, ut_min(srv_purge_batch_size, 20), true); ut_a(n_pages_purged == 0 || srv_fast_shutdown != 0); // 清理环境，something... } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 static ulint srv_do_purge(ulint *n_total_purged) { // 初始化环境，something... do { // 常态循环尽可能purge // S1. 通过当前系统和history状态，调整purge threads数目：... // S2. 判断是否需要进行undo truncate：... // S3. 调用trx_purge实际做purge n_pages_purged = trx_purge(n_use_threads, srv_purge_batch_size, do_truncate); *n_total_purged += n_pages_purged; // S4. 判断是否有需要truncate的undo space以再次进入：... } while (purge_sys-\u0026gt;state == PURGE_STATE_RUN \u0026amp;\u0026amp; (n_pages_purged \u0026gt; 0 || need_explicit_truncate) \u0026amp;\u0026amp; !srv_purge_should_exit(n_pages_purged)); return rseg_history_len; // 上一批purge前的history长度 } Purge coordinator 的实际 purge 任务是在 trx_purge 中进行分配和进行的， coordinator 会将 undo record 分配给 srv_sys-\u0026gt;tasks 中对应数目的 query thread，Purge worker 直接匹配系统环境的 query thread 拿 query thread node（purge_node_t类型）进行执行。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 ulint trx_purge(ulint n_purge_threads, ulint batch_size, bool truncate) { // S0. 初始化，something... /****************************************************** S1. 获取（clone）最老 read view 用于为 MVCC 限制 purge 的位置 *******************************************************/ rw_lock_x_lock(\u0026amp;purge_sys-\u0026gt;latch, UT_LOCATION_HERE); purge_sys-\u0026gt;view_active = false; trx_sys-\u0026gt;mvcc-\u0026gt;clone_oldest_view(\u0026amp;purge_sys-\u0026gt;view); purge_sys-\u0026gt;view_active = true; rw_lock_x_unlock(\u0026amp;purge_sys-\u0026gt;latch); /****************************************************** S2. 给 purge_sys 每个 query thread 的执行节点（purge_node_t）分配 undo recs *******************************************************/ /* S2.0. 在 trx_commit 时候将 update undo 写入 purge_queue，（insert undo 直接更新成 cache 或者 free）； S2.1. purge_sys 维护了 next undo rec 的位置（purge_sys-\u0026gt;iter），这里从这一位置开始获取batch_size大小的 undo rec S2.2. 将获取的 undo rec 分配给不同的work thread，这里官方先按table ID进行分配，再进行平衡尽可能保证各 worker 的 undo 数目均匀 （查看提交 Bug #32089028 CONCURRENTLY UPDATING MANY JSON DOCUMENTS STEADILY INCREASES IBD FILE SIZE） S2.3. 将 undo rec 真正分配挂到 query thread 的执行节点 purge_node_t 上 */ n_pages_handled = trx_purge_attach_undo_recs(n_purge_threads, batch_size); /****************************************************** S3. 启动并进行 purge 任务 *******************************************************/ // S3.1. 启动所有query thread if (n_purge_threads \u0026gt; 1) { for (ulint i = 0; i \u0026lt; n_purge_threads - 1; ++i) { thr = que_fork_scheduler_round_robin(purge_sys-\u0026gt;query, thr); // 向后台 srv_sys-\u0026gt;tasks 提交任务，以供 purger worker 执行 srv_que_task_enqueue_low(thr); } purge_sys-\u0026gt;n_submitted += n_purge_threads - 1; thr = que_fork_scheduler_round_robin(purge_sys-\u0026gt;query, thr); } else { thr = que_fork_scheduler_round_robin(purge_sys-\u0026gt;query, nullptr); } ++purge_sys-\u0026gt;n_submitted; // S3.2. 执行purge任务，并等待所有worker完成 que_run_threads(thr); purge_sys-\u0026gt;n_completed.fetch_add(1); if (n_purge_threads \u0026gt; 1) trx_purge_wait_for_workers_to_complete(); /****************************************************** S4. 将所有 blob\u0026#39;s first page 延迟到末尾统一释放，避免访问 freed page *******************************************************/ for (thr = UT_LIST_GET_FIRST(purge_sys-\u0026gt;query-\u0026gt;thrs); thr != nullptr; thr = UT_LIST_GET_NEXT(thrs, thr)) { purge_node_t *node = static_cast\u0026lt;purge_node_t *\u0026gt;(thr-\u0026gt;child); node-\u0026gt;free_lob_pages(); } /****************************************************** S5. 进行 undospace truncate *******************************************************/ if (truncate || srv_upgrade_old_undo_found) { trx_purge_truncate(); } return (n_pages_handled); } Purge 物理操作 在row_purge中会对在对应 query thread 的 run_node（purge_node_t）中取的 undo record 记录进行 purge，直到 purge 完所有分配的 undo record：\n首先解析 undo record 获取 trxid、tableid 等操作信息，通过 tableid 开启 innodb 表并获取 Shared MDL 锁，然后构建 row reference 等信息。这里的逻辑类似 rollback 操作的的第一步解析； 然后对 delete mark 类型的操作 purge 掉所有二级索引和主键上不再需要的 index record（包括extern field），这里也是先根据 row reference 索引 BTree 到对应的 cursor 上，删除走的是 btr_cur 的 delete 接口 btr_cur_optimistic_delete 和 btr_cur_pessimistic_delete； Purge truncate 前一个阶段 purge 物理操作只是将索引上的数据删除，但是不会处理 undospace 内的空间。虽然 undo log 可以被 purge，但是类似 ibd 文件（不主动optimize）一旦文件增大那么就无法缩小。Innodb 在一个 undo 表空间没有事务使用时，允许将其 truncate 来回收 undo 表空间。回收动作在 coordinator 一批 purge 任务完成后触发，接口为trx_purge_truncate，其内部主要分为 purge rollback segment 和 truncate space 两个部分。\npurge rollback segment（trx_purge_truncate_history）会从所有表空间的所有回滚段回收无用的 undo 数据并清理 history list； truncate space（trx_purge_truncate_undo_spaces）会检索是否存在满足truncate要求的 undospace（1. 手动设置为 inactive 或大小超过限制；2. 空间中 undo 记录被 purge 完全且没有被任何事务使用），并进行物理文件 truncate 操作。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 /** 清理某个 rollback segment */ static void trx_purge_truncate_rseg_history( trx_rseg_t *rseg, /*!\u0026lt; in: rollback segment */ const purge_iter_t *limit) /*!\u0026lt; in: truncate offset */ { // 变量初始化，something... // 从当前 rollback segment 的 history list 上面获取最后位置的 undo log header rseg-\u0026gt;latch(); rseg_hdr = trx_rsegf_get(rseg-\u0026gt;space_id, rseg-\u0026gt;page_no, rseg-\u0026gt;page_size, \u0026amp;mtr); hdr_addr = trx_purge_get_log_from_hist(flst_get_last(rseg_hdr + TRX_RSEG_HISTORY, \u0026amp;mtr)); loop: if (hdr_addr.page == FIL_NULL) { // history 不剩下需要处理的 undo rseg-\u0026gt;unlatch(); mtr_commit(\u0026amp;mtr); return; } undo_page = trx_undo_page_get(page_id_t(rseg-\u0026gt;space_id, hdr_addr.page), rseg-\u0026gt;page_size, \u0026amp;mtr); log_hdr = undo_page + hdr_addr.boffset; seg_hdr = undo_page + TRX_UNDO_SEG_HDR; undo_trx_no = mach_read_from_8(log_hdr + TRX_UNDO_TRX_NO); if (undo_trx_no \u0026gt;= limit-\u0026gt;trx_no) { // 当前要处理的 undo 已经到达、超过 purge limit if (undo_trx_no == limit-\u0026gt;trx_no \u0026amp;\u0026amp; rseg-\u0026gt;space_id == limit-\u0026gt;undo_rseg_space) { // 将所有小于 limit 的 undo normal page free，header page empty trx_undo_truncate_start(rseg, hdr_addr.page, hdr_addr.boffset, limit-\u0026gt;undo_no); } rseg-\u0026gt;unlatch(); mtr_commit(\u0026amp;mtr); return; } prev_hdr_addr = trx_purge_get_log_from_hist(flst_get_prev_addr(log_hdr + TRX_UNDO_HISTORY_NODE, \u0026amp;mtr)); if ((mach_read_from_2(seg_hdr + TRX_UNDO_STATE) == TRX_UNDO_TO_PURGE) \u0026amp;\u0026amp; (mach_read_from_2(log_hdr + TRX_UNDO_NEXT_LOG) == 0)) { // 无log剩余，回收整个 undo segment rseg-\u0026gt;unlatch(); mtr_commit(\u0026amp;mtr); trx_purge_free_segment(rseg, hdr_addr, is_temp); // 内部有trx_purge_remove_log_hdr删除history list节点 } else { // 在history list上删除当前 log header（这里相当于我那次一条history的purge） trx_purge_remove_log_hdr(rseg_hdr, log_hdr, \u0026amp;mtr); rseg-\u0026gt;unlatch(); mtr_commit(\u0026amp;mtr); } // 转移到 history list 上的下一条log mtr_start(\u0026amp;mtr); if (is_temp) { mtr.set_log_mode(MTR_LOG_NO_REDO); } rseg-\u0026gt;latch(); rseg_hdr = trx_rsegf_get(rseg-\u0026gt;space_id, rseg-\u0026gt;page_no, rseg-\u0026gt;page_size, \u0026amp;mtr); hdr_addr = prev_hdr_addr; goto loop; } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 // 进行物理文件 truncate static bool trx_purge_truncate_marked_undo() { // S0. 变量初始化，something... /****************************************************** S1. 获取 MDL 锁 *******************************************************/ MDL_ticket *mdl_ticket; bool dd_result = dd_tablespace_get_mdl(space_name.c_str(), \u0026amp;mdl_ticket, false); // something... /****************************************************** S2. 开始 truncate 操作，切换 undospace *******************************************************/ mutex_enter(\u0026amp;undo::ddl_mutex); if (!trx_purge_truncate_marked_undo_low(space_num, space_name)) { mutex_exit(\u0026amp;undo::ddl_mutex); dd_release_mdl(mdl_ticket); MONITOR_INC_TIME(MONITOR_UNDO_TRUNCATE_MICROSECOND, counter_time_truncate); return (false); } /****************************************************** S3. 删除 undo log file，标志 undo truncate 完成 *******************************************************/ undo::spaces-\u0026gt;x_lock(); undo::done_logging(space_num); undo::spaces-\u0026gt;x_unlock(); // S4. 清理环境，something... return (true); } static bool trx_purge_truncate_marked_undo_low(space_id_t space_num, std::string space_name) { /****************************************************** S2.1. 获取环境 *******************************************************/ // someting... /****************************************************** S2.2. 创建 undo 文件，标志 undo truncate 开始 *******************************************************/ dberr_t err = undo::start_logging(marked_space); // someting... /****************************************************** S2.3. 过滤特殊条件，someting... *******************************************************/ /****************************************************** S2.4. 实际 undo space 文件裁剪轮转 *******************************************************/ /* 计算新 undo space id 和 undo space no； 删除+新建 file tablespace（fil_delete_tablespace + fil_ibd_create）； 重新初始化新 tablespace 文件，构建 undo 文件内容； 重新设置相应 undo space 回滚段内存结构体（Rsegs *m_rsegs） */ bool success = trx_undo_truncate_tablespace(marked_space); // someting... /****************************************************** S2.5. 设定此 undo 空间后面的可用状态 *******************************************************/ space_id_t new_space_id = marked_space-\u0026gt;id(); dd_space_states next_state; undo::spaces-\u0026gt;s_lock(); Rsegs *marked_rsegs = marked_space-\u0026gt;rsegs(); marked_rsegs-\u0026gt;x_lock(); if (marked_rsegs-\u0026gt;is_inactive_explicit()) { // 由外部手动 inactive next_state = DD_SPACE_STATE_EMPTY; marked_rsegs-\u0026gt;set_empty(); } else { // 由后台 purge 选择，可被再使用 next_state = DD_SPACE_STATE_ACTIVE; marked_rsegs-\u0026gt;set_active(); } /****************************************************** S2.6. 在 DD 中更新 space ID 和 state 信息 *******************************************************/ if (DD_FAILURE == dd_tablespace_set_id_and_state(space_name.c_str(), new_space_id, next_state)) return (false); return (true); } 版权声明：如需转载或引用，请附加本文链接并注明来源。 ","permalink":"https://mzyee.github.io/posts/mysql/purge/","summary":"InnoDB Purge System 相关代码学习","title":"InnoDB Purge System 代码学习"},{"content":"前言 已经有不少文章对 InnoDB 的 undo 系统进行了全面介绍，比如：\nInnoDB undo log 漫游 InnoDB之UNDO LOG介绍 庖丁解InnoDB之UNDO LOG 本文主要关注一些 Undo 系统设计/代码实现映射，以及部分笔者关心的代码细节，建议结合阅读上述文章。\nUndo的物理组织 首先要明确的一点是 InnoDB 的 undo log 实际上是以表数据格式记录的（所以叫 undo tablespace），并且除了 temporary table 的 undo，正常用户表 undo 都通过 redo 来维护持久性。\n由于通过表空间存储 undo log，所以 undo 文件的基础组织形式是类似 InnoDB 的 ibd 文件，内部有如 File Segment 等概念、有各种文件维护结构的 list、有 page 上文件维护结构相关的 meta，等等。并且分配 undo page 时候也是通过 fsp_reserve_free_extents 这一接口去申请空闲数据页。\n每个 Undo Tablespace 有最多128个可用的 Rollback Segment，Undo Tablespace 第三个页有128个 Rollback Segment 的目录（Rollback Segment Header\u0026rsquo;s Arrary），其中每个元素指向各个 Rollback Segment Header 所在的 page number。初始化时候会处理所有 Rollback Segment，如果未使用则为 FIL_NULL，通过 innodb_rollback_segments 参数控制实际使用数目。\n一个 Rollback Segment Header 包含1024个 slot（四字节），每个 slot 指向 Undo Segment 的 first page number，也就是这个 Undo Segment 的 Undo Header Page。每一时刻，每个使用的 Undo Segment 都被一个事务独占，而每个写事务都会持有至少一个Undo Segment，因此被使用的 Undo Header Page 中只有一个未提交事务的 Undo Segment Header，当活跃事务产生的 undo 超过 Undo Header Page 容量后会被分配新的 Undo Normal Page。\n写事务在修改数据前，会先通过 Undo Log 记录修改前的（主键）数据，并且 undo 分为 insert (UNDO_INSERT) 和 update (UNDO_UPD_EXIST、UNDO_DEL_MARK、UNDO_UPD_DEL) 两类。记录 undo log 前会先写入 Undo Log Header 用于标记和定位，然后逐条写入 Undo Record。\n1 2 3 4 5 6 7 8 9 10 11 12 File Segment |- File Segment Header Rollback Segment |- Rollback Segment Header Undo Segment |- Undo Segment Header Undo Page |- Undo Page Header Undo Header Page Undo Normal Page Undo Log |- Undo Log Header Undo Record |- head, body, tail 可以通过参考资料中的这个图来查看上述物理结构的逻辑关系，更具体的细节结构定义可以查看trx0undo.h文件。 生成Undo的代码逻辑 分配回滚段 对于只读事务，当事务涉及到对临时表的读写时，会为其分配一个回滚段对其产生的undo log record进行记录：\n1 trx_assign_rseg_temp() -\u0026gt; get_next_temp_rseg() 对于读写事务，当事务进入读写模式时，会为其分配trx_id以及回滚段：\n1 trx_assign_rseg_durable() -\u0026gt; get_next_redo_rseg() -\u0026gt; get_next_redo_rseg_from_trx_sys() / get_next_redo_rseg_from_undo_spaces() 分配行为只是一个基于轮询的内存操作，最终会为 trx 返回一个有效的 trx_rseg_t（回滚段的内存结构）。一个事务最多会有2个 rollback segment（temp/durable区分）和 4个 undo segment（insert/update再次区分）。\n分配Undo 1 2 3 4 5 6 7 8 trx_undo_report_row_operation \u0026lt;- | btr_cur_ins_lock_and_undo (btr_cur_optimistic_insert / btr_cur_pessimistic_insert) | btr_cur_upd_lock_and_undo (btr_cur_update_in_place / btr_cur_optimistic_update / btr_cur_pessimistic_update) | btr_cur_del_mark_set_clust_rec |\u0026lt;- 行级别操作接口 // 通过 BTR_NO_UNDO_LOG_FLAG 标记决定是否记录 undo 函数trx_undo_report_row_operation是写入undo的代码接口，首先会如果当前事务没有分配过所需类型的 undo segment，则会通过trx_undo_assign_undo分配 undo segment/page（trx_undo_t结构），首先尝试从当前 trx_rseg_t 的 insert/update cache中取（接口为trx_undo_reuse_cached，cache 内是如果对应undo事务结束时候所使用的 undo page 所用空间小于3/4则被加入的），cache 中没有则在空间总重新分配一个undo segment（接口为trx_undo_create）。分配好 undo page 从空闲位置开始对应写入 Undo Record，如果 undo page 的剩余空间不满足要求则通过trx_undo_add_page加一个空白的 normal page 记录 Undo Record。\n整个过程中，除了物理 undo page 更新及通过 redo 持久化，还维护了内存中的当前事务所使用的 trx_undo_t 结构体（trx-\u0026gt;rsegs.m_redo/ m_noredo.insert_undo/ update_undo），包括 header page、log header、top undo page/offset等信息。\n写入Undo Reocrd 1 2 3 4 5 6 7 8 9 10 11 12 // Insert类型：TRX_UNDO_INSERT_REC /*-------- header ------*/ Next_record_offset 2B Type_flags 1B Undo_no compressed Table_id compressed /*-------- key field ------*/ Key_field_1_len compressed Key_field_1_data compressed ...(以上 dict_index_get_n_unique 个 field) /*-------- tail ------*/ Next_record_offset 2B 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 // Update类型：TRX_UNDO_UPD_EXIST_REC、TRX_UNDO_DEL_MARK_REC、TRX_UNDO_UPD_DEL_REC /*-------- header ------*/ Next_record_offset 2B Type_flags 1B Undo_no compressed Table_id compressed Rec_info 1B Trx_id compressed Roll_ptr compressed //指向的是该记录的上一个版本的位置 /*-------- key field ------*/ Key_field_1_len compressed Key_field_1_data len ...(以上 dict_index_get_n_unique 个 field) Update_field_num compressed // 修改的field数目 Update_field_1_pos compressed Update_field_1_len compressed Update_field_1_ctx len //这次修改前的field信息，应用这个可以返回到修改之前到值 ...(以上 upd_t 中个 field 数目) /*-------- tail ------*/ Next_record_offset 2B Undo Record的内容格式如上所示，这里再额外说明下：\n对于 insert undo，记录的是插入的 entry 的 key 部分，用于回滚时定位寻找并删除，insert 操作不存在可见性判读因此没有记录Trx_id； 对于 update undo，传入的参数时 index_record（rec_t）和 update（upd_t）这个内存结构，实际记录的是 record key 和要被修改的 field，前者用于定位，后者用于还原到原始版本。 对主键数据生成 undo 后就产生了 roll_ptr 并对新 record 进行更新，就是通过 roll_ptr 将 record 的所有版本链接起来。 事务提交时的Undo处理 在 trx_commit 的时候，如果事务发生修改并产生了 undo：\n设置 undo 后续处理状态： undo 使用单个 page 且使用量小于 3/4 的会被设置为 TRX_UNDO_CACHED 状态；其余的 insert undo 状态被设置为 TRX_UNDO_TO_FREE，update undo 状态被设置为 TRX_UNDO_TO_PURGE； 对 update undo，加入 purge_sys-\u0026gt;purge_queue 这一队列（其内按事务提交时的 trx-\u0026gt;no 排列）以给 purge 系统定位回收；将 trx-\u0026gt;no 写入 rseg/undo header，并 undo header 将加入到 rseg header 中 history list 的开始； 对 update undo，内存结构 trx_undo_t 按状态加入 rseg-\u0026gt;update_undo_cached 或 trx_undo_mem_free； 对 insert undo，内存结构 trx_undo_t 按状态加入 rseg-\u0026gt;insert_undo_list 或 trx_undo_mem_free，并且在 rollback segment 中清理不在 history 的 undo log segment（trx_undo_seg_free）； 应用Undo的代码逻辑 应用 undo 主要是在 mvcc 和 rollback 过程中，而 rollback 又可以分为用户 rollback 和奔溃恢复 rollback。本文主要讨论 rollback 过程，对于 mvcc 操作会开另外的文章专门讨论。\n奔溃恢复时Rollback 对于奔溃恢复过程中的 Rollback，首先需要恢复 undo 相关信息：\n通过 redo，在启动的recv_recovery_from_checkpoint_start-\u0026gt;recv_recovery_begin阶段恢复undo数据； 通过恢复的 undo 数据，在启动的trx_sys_init_at_db_start阶段恢复所有回滚段的内存结构，trx_sys 的活跃事物。活跃事物通过trx_resurrect分别扫描rseg-\u0026gt;insert_undo_list/update_undo_list，生成后台trx对象并初始化相关参数并加入trx_sys-\u0026gt;rw_trx_set，再按状态加入 trx_sys-\u0026gt;rw_trx_ids（活跃）和 trx_sys-\u0026gt;rw_trx_list（所有）。 在 dict_recover DICT_RECOVERY_RESTART_SERVER 阶段，srv_dict_recover_on_restart 中会加上表锁并 rollback DD 相关事务； 在 post_recover 阶段末尾，srv_start_threads_after_ddl_recovery 开启后台回滚线程，扫描 trx_sys-\u0026gt;rw_trx_list，对恢复的事务，回滚TRX_STATE_ACTIVE事务（trx_rollback_active）、清理COMMITTED_IN_MEMORY事务（trx_free_resurrected）。 后台回滚线程trx_recovery_rollback_thread最终通过trx_rollback_active反向应用 trx undo log 后提交（rollback本身也是通过trx_commit表示事务完成）来回滚活跃事务。 这里可以看到在 recovery 的前向 redo 是不会使用到 undo 的，已经持久化的 redo 对应需要的 undo 肯定已经被持久化，前向 redo 本身也不在产生额外 undo。此外也再次可见，undo 内容是以数据模式记录的。\n应用Undo 实际应用 undo 是在 row_undo_step 中:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 row_undo_step -\u0026gt; ... | row_undo_ins | row_undo_ins_parse_undo_rec(undo_node_t) 将 insert undo 中的内容 parse出来供后续undo使用 | trx_undo_rec_get_pars() 解析常规参数：是否有LOB field update，type compilation info，undo no | trx_undo_rec_get_row_ref() 构建 row reference（dtuple_t，即index record的内存对象） | row_undo_search_clust_to_pcur() 在主键上索引row reference，判断是否找到的主键是需要undo的主键（定位 undo_node_t-\u0026gt;pcur），系统列等也需要一致 | row_undo_ins_remove_sec_rec(undo_node_t, que_thr_t) 先 删除 所有二级索引（和正向路径相反方向） | row_undo_ins_remove_clust_rec(undo_node_t) 再 删除 主键级索引 | row_undo_mod | row_undo_mod_parse_undo_rec() | trx_undo_rec_get_pars() 类上 | trx_undo_update_rec_get_sys_cols() 获取老版本record的系统列 | trx_undo_rec_get_row_ref() 类上 | trx_undo_update_rec_get_update() 构建老版本record的update vector用于后续update使用 | row_undo_search_clust_to_pcur() 类上 | row_undo_mod_upd_exist_sec() / row_undo_mod_del_mark_sec() / row_undo_mod_upd_del_sec() 先 更新 二级索引 | row_undo_mod_clust() 再 更新 主键索引 首先解析需要被应用的 undo，这里涉及开 innodb 表，且在后台回滚情况下这里再需要加上 DD 的 MDL 锁； 在row_undo_search_clust_to_pcur通过 undo 里面的 key 信息在主键上索引 row reference，并根据 roll_ptr 判断找到的主键是否是需要undo的主键。如果找到则构建并存储找到的 index record 的内存结构 dtuple_t，如果是 update，还需构建出来 update 之前的版本（即应用了undo后还原的原先版本）； 对于 insert 的 undo 较为简单，从二级索引到主键删除目标 record； 对于 update 的 undo，在二级索引上， UPD_EXIST（原操作为 update record）的 undo 会根据可见性 标记删除 或 物理删除 现有二级索引记录，再更新或插入undone record； DEL_MARK（原操作为 delete mark record）的 undo 会 去除删除标记 现有二级索引记录，如果不存在则 重新插入； UPD_DEL（原操作为 undelete mark record）的 undo 会根据可见性 标记删除 或 物理删除 现有二级索引记录。 对于 update 的 undo，在主键上则走 btr_cur 的 update 逻辑进行更新，对于 UPD_DEL 还会主动尝试清理标记删除的 record。 值得一体的是在崩溃恢复的 undo 事务，事务锁在初始状态下都是隐式锁，是通过其他冲突事务在加锁时通过lock_rec_convert_impl_to_expl 或 undo 应用阶段通过 row_convert_impl_to_expl_if_needed加上事务锁。此过程中加的都是 record lock，这是由于后台 undo 不被用户所见，其本身不具备可见性要求因此不会加上 next key lock。\n回收Undo的代码逻辑 在 innobase_post_recover 中 ddl恢复完成后，保证 InnoDB metadata 和 file system 一致后的阶段，系统会启动 srv_purge_coordinator_thread 和 srv_worker_thread 来purge undo，相关内容会在 purge 系统中介绍。\n版权声明：如需转载或引用，请附加本文链接并注明来源。 ","permalink":"https://mzyee.github.io/posts/mysql/undo/","summary":"InnoDB Undo Log 相关代码学习","title":"InnoDB Undo Log 代码学习"},{"content":"eBPF技术背景 BPF（Berkeley Packet Filter）是类Unix系统上数据链路层的一种原始接口，提供原始链路层封包的收发。BPF在数据包过滤上引入了两大特性：\n一个可有效地工作在基于寄存器结构 CPU 上的虚拟机； 应用程序只复制与过滤数据包相关的数据，不复制数据包的所有信息。 eBPF 在原始BPF基础上进一步针对现代 CPU 硬件进行了指令集优化，增加了 VM 中的寄存器数量，使数据处理速度提高了数倍。总结来说，eBPF 提供了一个基于寄存器的虚拟机，使用自定义的 64 位 RISC 指令集，能够在 Linux 内核内运行本地即时编译的 BPF 程序，并能访问内核功能和内存的特定子集。\neBPF程序分为用户空间程序和内核程序两部分： 用户空间程序负责加载 BPF 字节码至内核，一般使用者可以通过 LLVM 或者 GCC 将编写的 BPF 代码程序编译成内核可验证的 BPF 字节码。内核加载前会使用验证器 verfier 组件保证字节码的安全性，以避免对内核造成灾难问题。如有需要，用户空间程序也会负责读取并处理内核回传的统计信息或者事件详情。 在内核中加载的 BPF 字节码程序会在内核中执行特定事件，BPF 程序可能基于 kprobes / uprobes / tracepoint / perf_events 等中的一个或多个事件。如有需要，内核程序也会将执行的结果通过缓存 map 或者 perf-event 事件发送至用户空间。 通过上述机制，eBPF 的使用者（不局限于内核开发者）能够基于系统内核或程序事件高效、安全的（在内核中）执行特定代码，通过向内核添加 eBPF 模块来增加功能。\n基于eBPF进行软件开发 eBPF 指令是固定大小的 64 位编码，大约有上百条指令，被分组为 8 类，常用指令支持如从通用内存进行加载/存储，前/后（非）条件跳转、算术/逻辑操作和函数调用等。\n1 2 3 4 5 6 7 8 9 10 11 12 13 /* msb lsb +---------------------+---------------+----+----+-------+ |immediate |offset |src |dst |opcode | +---------------------+---------------+----+----+-------+ */ struct bpf_insn { __u8\tcode;\t/* opcode */ __u8\tdst_reg:4;\t/* dest register */ __u8\tsrc_reg:4;\t/* source register */ __s16\toff;\t/* signed offset */ __s32\timm;\t/* signed immediate constant */ }; 直接使用原始字节码来实现 eBPF 程序，非常像编写汇编代码，这种行为无疑是较为困难，通常使用更高级别的语言和工具来实现功能复杂的 eBPF 用例。一种很自然的想法是能不能将高级语言的中间表示层编译成 eBPF 程序模块，这样我们就可以将使用具有“限制性“的高级抽象层语言来编写 eBPF 程序。这种设计有效地将内核中运行的eBPF字节码的定义（后端）从字节码加载器和前端程序中分离出来。\n基于这样的目的，社区创建了 BCC 项目，使用户可以使用带有限制性的C语言（BPF C）编写 eBPF 后端程序，并且 BCC 为用户封装了许多实用底层函数避免重复造轮子的苦恼。借助 BCC，用户还可以通过编写 python 来快速实现加载器和前端程序。此外，另一个项目 BPFtrace 建立在 BCC 之上，通过特定领域语言提供更高级别的抽象逻辑，以帮助用户实现更快速分析/调试。\n我们以 BCC 的 bashreadline 为例简单介绍下使用 BCC 进行 eBPF 程序开发的流程。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 #!/usr/bin/env python from __future__ import print_function from bcc import BPF from time import strftime import argparse ############################ # part 1 # ############################ # load BPF program bpf_text = \u0026#34;\u0026#34;\u0026#34; #include \u0026lt;uapi/linux/ptrace.h\u0026gt; #include \u0026lt;linux/sched.h\u0026gt; struct str_t { u32 pid; char str[80]; }; BPF_PERF_OUTPUT(events); int printret(struct pt_regs *ctx) { struct str_t data = {}; char comm[TASK_COMM_LEN] = {}; if (!PT_REGS_RC(ctx)) return 0; data.pid = bpf_get_current_pid_tgid() \u0026gt;\u0026gt; 32; bpf_probe_read_user(\u0026amp;data.str, sizeof(data.str), (void *)PT_REGS_RC(ctx)); bpf_get_current_comm(\u0026amp;comm, sizeof(comm)); if (comm[0] == \u0026#39;b\u0026#39; \u0026amp;\u0026amp; comm[1] == \u0026#39;a\u0026#39; \u0026amp;\u0026amp; comm[2] == \u0026#39;s\u0026#39; \u0026amp;\u0026amp; comm[3] == \u0026#39;h\u0026#39; \u0026amp;\u0026amp; comm[4] == 0 ) { events.perf_submit(ctx,\u0026amp;data,sizeof(data)); } return 0; }; \u0026#34;\u0026#34;\u0026#34; ############################ # part 2 # ############################ parser = argparse.ArgumentParser( description=\u0026#34;Print entered bash commands from all running shells\u0026#34;, formatter_class=argparse.RawDescriptionHelpFormatter) parser.add_argument(\u0026#34;-s\u0026#34;, \u0026#34;--shared\u0026#34;, nargs=\u0026#34;?\u0026#34;, const=\u0026#34;/lib/libreadline.so\u0026#34;, type=str, help=\u0026#34;specify the location of libreadline.so library.\\ Default is /lib/libreadline.so\u0026#34;) args = parser.parse_args() name = args.shared if args.shared else \u0026#34;/bin/bash\u0026#34; b = BPF(text=bpf_text) b.attach_uretprobe(name=name, sym=\u0026#34;readline\u0026#34;, fn_name=\u0026#34;printret\u0026#34;) ############################ # part 3 # ############################ # header print(\u0026#34;%-9s %-7s %s\u0026#34; % (\u0026#34;TIME\u0026#34;, \u0026#34;PID\u0026#34;, \u0026#34;COMMAND\u0026#34;)) def print_event(cpu, data, size): event = b[\u0026#34;events\u0026#34;].event(data) print(\u0026#34;%-9s %-7d %s\u0026#34; % (strftime(\u0026#34;%H:%M:%S\u0026#34;), event.pid, event.str.decode(\u0026#39;utf-8\u0026#39;, \u0026#39;replace\u0026#39;))) b[\u0026#34;events\u0026#34;].open_perf_buffer(print_event) while 1: try: b.perf_buffer_poll() except KeyboardInterrupt: exit() 可以将上面这个 eBPF 程序分为3个部分：\nPart 1 是用 BPF C 实现的 eBPF 后端程序，首先这里构建了内置 BPF_PERF_OUTPUT 结构来创建一个 BPF table，通过 perf 环形缓冲区将自定义事件数据推送到用户空间。用户可以通过 events 来获取推送的数据。然后在 printret 函数中通过 PT_REGS_RC 来判断、获取当前环境函数的返回值，并通过内置函数获取程序pid和返回值。通过内置函数 bpf_get_current_comm 判断当前程序名称，如果是 bash 命令就向 events 输出程序运行返回值。 Part 2 是通过 BCC 的支持利用 python 进行前述 eBPF 后端程序的加载器。加载类型是 uretprobe，即将程序挂载到 user-level 的 readline() 函数上，即在用户调用 readline() 函数返回时执行相应 eBPF 后端代码。 Part 3 是显示输出的前端程序，用户对 events 缓冲区进行polling，当缓冲区有内容时将对应内容输出显示。 至此，通过运行这一个 BCC 程序，用户就可以通过 eBPF实现监控所有 bash 进程的 readline 函数，输出对应 pid 和 readline 对返回结果（即 bash 输入的命令内容）。更多的内置功能及接口自行参见相关项目官网。\n事件类型 前面提到 BPF 程序类型可能基于 kprobes / uprobes / tracepoint /perf_events 等事件中的一个或多个，其中：\nkprobes：内核中动态跟踪。可以跟踪到 Linux 内核中的函数入口或返回点，但不是稳定接口，可能会因为内核版本变化导致跟踪失效。 理论上可以跟踪到所有导出符号 /proc/kallsyms 但不在 /sys/kernel/debug/kprobes/blacklist 中的函数。 uprobes：用户级别的动态跟踪。与 kprobes 类似，只是跟踪的函数为用户程序中的函数。 tracepoints：内核中静态跟踪。tracepoints 是内核开发人员维护的跟踪点，能够提供稳定的接口，但是需维护数量且场景受限。 USDT：用户静态探针，类似 tracepoints 但是需要用户空间自己维护。 perf_events：定时采样和 PMC 事件。 eBPF 模块各种类型事件的加载执行在后面有时间的话再展开讲讲，有兴趣的可以搜索 Linux 内核的 text hook/poke、static-key 和 static-jump 等机制实现。\n常用eBPF工具与指令 BCC 和 BPFtrace 提供了很多现成实用的工具，下面分享一些我觉得有用的工具，具体参数参考各个命令的help查看。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # 检查 pid程序打开的文件 opensnoop -p 123 # 检测 disk 的 I/O 状态 biolatency [-D for each disk] [-Q include OS queued time in I/O time] # 追踪pid程序fs操作大于10ms的操作 ext4slower 10 -p 123 # 追踪pid程序fs操作分布 ext4dist -p 123 # 系统 time 时间的 cache 状态 cachestat time llcstat time # 函数 count / latency / call-interval 统计: funccount \u0026#39;./test:read*\u0026#39; -p 123 -d 1 funclatency \u0026#39;./test:read*\u0026#39; -p 123 -d 1 funcinterval \u0026#39;./test:read*\u0026#39; -p 123 -d 1 # on-CPU 和 off-CPU 火焰图，采集 DURATION 秒 profile -p 123 -f DURATION --stack-storage-size=165535 \u0026gt; profile01.txt flamegraph.pl --width=1600 \u0026lt; profile01.txt \u0026gt; profile01.svg offcputime -p 123 -f DURATION --stack-storage-size=165535 \u0026gt; offcpu01.txt flamegraph.pl --width=1600 \u0026lt; offcpu01.txt \u0026gt; offcpu01.svg # 采集 pid 程序cache-misses超过的10000次线程及其10000次的次数 bpftrace -e \u0026#39;hardware:cache-misses:10000 /pid==123/ { @[comm, tid] = count(); }\u0026#39; 版权声明：如需转载或引用，请附加本文链接并注明来源。 ","permalink":"https://mzyee.github.io/posts/usebpf/","summary":"Learning About eBPF Usages","title":"通过 eBPF 工具进行性能分析"},{"content":"E-mail: mzyeee@gmail.com\n知乎: https://www.zhihu.com/people/mzy-59-9\n","permalink":"https://mzyee.github.io/contact/","summary":"E-mail: mzyeee@gmail.com\n知乎: https://www.zhihu.com/people/mzy-59-9","title":""}]