[{"content":"前言 已经有不少文章对 InnoDB 的 undo 系统进行了全面介绍，比如：\nInnoDB undo log 漫游 InnoDB之UNDO LOG介绍 庖丁解InnoDB之UNDO LOG 本文主要关注一些系统设计/代码实现映射和部分笔者关心的代码处理细节，建议结合阅读上述文章。\nUndo的物理组织 首先要明确的一点是 InnoDB 的 undo log 实际上是以表数据格式记录的（所以叫 undo tablespace），并且除了 temporary table 的 undo，正常用户表 undo 都通过 redo 来维护持久性。\n由于通过表空间存储 undo log，所以 undo 文件的基础组织形式是类似 InnoDB 的 ibd 文件，内部有如 File Segment 等概念、有各种文件维护结构的 list、有 page 上文件维护结构相关的 meta，等等。并且分配 undo page 时候也是通过 fsp_reserve_free_extents 这一接口去申请空闲数据页。\n每个 Undo Tablespace 有最多128个可用的 Rollback Segment，Undo Tablespace 第三个页有128个 Rollback Segment 的目录（Rollback Segment Header\u0026rsquo;s Arrary），其中每个元素指向各个 Rollback Segment Header 所在的 page number。初始化时候会处理所有 Rollback Segment，如果未使用则为 FIL_NULL，通过 innodb_rollback_segments 参数控制实际使用数目。\n一个 Rollback Segment Header 包含1024个 slot（四字节），每个 slot 指向 Undo Segment 的 first page number，也就是这个 Undo Segment 的 Undo Header Page。每一时刻，每个使用的 Undo Segment 都被一个事务独占，而每个写事务都会持有至少一个Undo Segment，因此被使用的 Undo Header Page 中只有一个未提交事务的 Undo Segment Header，当活跃事务产生的 undo 超过 Undo Header Page 容量后会被分配新的 Undo Normal Page。\n写事务在修改数据前，会先通过 Undo Log 记录修改前的（主键）数据，并且 undo 分为 insert (UNDO_INSERT) 和 update (UNDO_UPD_EXIST、UNDO_DEL_MARK、UNDO_UPD_DEL) 两类。记录 undo log 前会先写入 Undo Log Header 用于标记和定位，然后逐条写入 Undo Record。\n1 2 3 4 5 6 7 8 9 10 11 12 File Segment |- File Segment Header Rollback Segment |- Rollback Segment Header Undo Segment |- Undo Segment Header Undo Page |- Undo Page Header Undo Header Page Undo Normal Page Undo Log |- Undo Log Header Undo Record |- head, body, tail 可以通过参考资料中的这个图来查看上述物理结构的逻辑关系，更具体的细节结构定义可以查看trx0undo.h文件。 生成Undo的代码逻辑 分配回滚段 对于只读事务，当事务涉及到对临时表的读写时，会为其分配一个回滚段对其产生的undo log record进行记录：\n1 trx_assign_rseg_temp() -\u0026gt; get_next_temp_rseg() 对于读写事务，当事务进入读写模式时，会为其分配trx_id以及回滚段：\n1 trx_assign_rseg_durable() -\u0026gt; get_next_redo_rseg() -\u0026gt; get_next_redo_rseg_from_trx_sys() / get_next_redo_rseg_from_undo_spaces() 分配行为只是一个基于轮询的内存操作，最终会为 trx 返回一个有效的 trx_rseg_t（回滚段的内存结构）。一个事务最多会有2个 rollback segment（temp/durable区分）和 4个 undo segment（insert/update再次区分）。\n分配Undo 1 2 3 4 5 6 7 8 trx_undo_report_row_operation \u0026lt;- | btr_cur_ins_lock_and_undo (btr_cur_optimistic_insert / btr_cur_pessimistic_insert) | btr_cur_upd_lock_and_undo (btr_cur_update_in_place / btr_cur_optimistic_update / btr_cur_pessimistic_update) | btr_cur_del_mark_set_clust_rec |\u0026lt;- 行级别操作接口 // 通过 BTR_NO_UNDO_LOG_FLAG 标记决定是否记录 undo 函数trx_undo_report_row_operation是写入undo的代码接口，首先会如果当前事务没有分配过所需类型的 undo segment，则会通过trx_undo_assign_undo分配 undo segment/page（trx_undo_t结构），首先尝试从当前 trx_rseg_t 的 insert/update cache中取（接口为trx_undo_reuse_cached，cache 内是如果对应undo事务结束时候所使用的 undo page 所用空间小于3/4则被加入的），cache 中没有则在空间总重新分配一个undo segment（接口为trx_undo_create）。分配好 undo page 从空闲位置开始对应写入 Undo Record，如果 undo page 的剩余空间不满足要求则通过trx_undo_add_page加一个空白的 normal page 记录 Undo Record。\n整个过程中，除了物理 undo page 更新及通过 redo 持久化，还维护了内存中的当前事务所使用的 trx_undo_t 结构体（trx-\u0026gt;rsegs.m_redo/m_noredo.insert_undo/update_undo），包括 header page、log header、top undo page/offset等信息。\n写入Undo Reocrd 1 2 3 4 5 6 7 8 9 10 11 12 // Insert类型：TRX_UNDO_INSERT_REC /*-------- header ------*/ Next_record_offset 2B Type_flags 1B Undo_no compressed Table_id compressed /*-------- key field ------*/ Key_field_1_len compressed Key_field_1_data compressed ...(以上 dict_index_get_n_unique 个 field) /*-------- tail ------*/ Next_record_offset 2B 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 // Update类型：TRX_UNDO_UPD_EXIST_REC、TRX_UNDO_DEL_MARK_REC、TRX_UNDO_UPD_DEL_REC /*-------- header ------*/ Next_record_offset 2B Type_flags 1B Undo_no compressed Table_id compressed Rec_info 1B Trx_id compressed Roll_ptr compressed //指向的是该记录的上一个版本的位置 /*-------- key field ------*/ Key_field_1_len compressed Key_field_1_data len ...(以上 dict_index_get_n_unique 个 field) Update_field_num compressed // 修改的field数目 Update_field_1_pos compressed Update_field_1_len compressed Update_field_1_ctx len //这次修改前的field信息，应用这个可以返回到修改之前到值 ...(以上 upd_t 中个 field 数目) /*-------- tail ------*/ Next_record_offset 2B Undo Record的内容格式如上所示，这里再额外说明下：\n对于 insert undo，记录的是插入的 entry 的 key 部分，用于回滚时定位寻找并删除，insert 操作不存在可见性判读因此没有记录Trx_id； 对于 update undo，传入的参数时 index_record（rec_t）和 update（upd_t）这个内存结构，实际记录的是 record key 和要被修改的 field，前者用于定位，后者用于还原到原始版本。 对主键数据生成 undo 后就产生了 roll_ptr 并对新 record 进行更新，就是通过 roll_ptr 将 record 的所有版本链接起来。 事务提交时的Undo处理 在 trx_commit 的时候，如果事务发生修改并产生了 undo：\n设置 undo 后续处理状态： undo 使用单个 page 且使用量小于 3/4 的会被设置为 TRX_UNDO_CACHED 状态；其余的 insert undo 状态被设置为 TRX_UNDO_TO_FREE，update undo 状态被设置为 TRX_UNDO_TO_PURGE； 对 update undo，加入 purge_sys-\u0026gt;purge_queue 这一队列（其内按事务提交时的 trx-\u0026gt;no 排列）以给 purge 系统回收；将 trx-\u0026gt;no 写入 rseg/undo header，并 undo header 将加入到 rseg header 中 history list 的开始； 对 update undo，内存结构 trx_undo_t 按状态加入 rseg-\u0026gt;update_undo_cached 或 trx_undo_mem_free； 对 insert undo，内存结构 trx_undo_t 按状态加入 rseg-\u0026gt;insert_undo_list 或 trx_undo_mem_free，并且在 rollback segment 中清理不在 history 的 undo log segment（trx_undo_seg_free）； 应用Undo的代码逻辑 应用 undo 主要是在 mvcc 和 rollback 过程中，而 rollback 又可以分为用户 rollback 和奔溃恢复 rollback。本文主要讨论 rollback 过程，对于 mvcc 操作会开另外的文章专门讨论。\n奔溃恢复时Rollback 对于奔溃恢复过程中的 Rollback，首先需要恢复 undo 相关信息：\n通过 redo，在启动的recv_recovery_from_checkpoint_start-\u0026gt;recv_recovery_begin阶段恢复undo数据； 通过恢复的 undo 数据，在启动的trx_sys_init_at_db_start阶段恢复所有回滚段的内存结构，trx_sys 的活跃事物。活跃事物通过trx_resurrect分别扫描rseg-\u0026gt;insert_undo_list/update_undo_list，生成后台trx对象并初始化相关参数并加入trx_sys-\u0026gt;rw_trx_set，再按状态加入 trx_sys-\u0026gt;rw_trx_ids（活跃）和 trx_sys-\u0026gt;rw_trx_list（所有）。 在 dict_recover DICT_RECOVERY_RESTART_SERVER 阶段，srv_dict_recover_on_restart 中会加上表锁并 rollback DD 相关事务； 在 post_recover 阶段末尾，srv_start_threads_after_ddl_recovery 开启后台回滚线程，扫描 trx_sys-\u0026gt;rw_trx_list，对恢复的事务，回滚TRX_STATE_ACTIVE事务（trx_rollback_active）、清理COMMITTED_IN_MEMORY事务（trx_free_resurrected）。 这里可以看到在 recovery 的前向 redo 是不会使用到 undo 的，已经持久化的 redo 对应需要的 undo 肯定已经被持久化，前向 redo 本身也不在产生额外 undo。此外也再次可见，undo 内容是以数据模式记录的。\n应用Undo 实际应用 undo 是在 row_undo_step 中:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 row_undo_step -\u0026gt; ... | row_undo_ins | row_undo_ins_parse_undo_rec(undo_node_t) 将 insert undo 中的内容 parse出来供后续undo使用 | trx_undo_rec_get_pars() 解析常规参数：是否有LOB field update，type compilation info，undo no | trx_undo_rec_get_row_ref() 构建 row reference（dtuple_t，即index record的内存对象） | row_undo_search_clust_to_pcur() 在主键上索引row reference，判断是否找到的主键是需要undo的主键（定位 undo_node_t-\u0026gt;pcur），系统列等也需要一致 | row_undo_ins_remove_sec_rec(undo_node_t, que_thr_t) 先 删除 所有二级索引（和正向路径相反方向） | row_undo_ins_remove_clust_rec(undo_node_t) 再 删除 主键级索引 | row_undo_mod | row_undo_mod_parse_undo_rec() | trx_undo_rec_get_pars() 类上 | trx_undo_update_rec_get_sys_cols() 获取老版本record的系统列 | trx_undo_rec_get_row_ref() 类上 | trx_undo_update_rec_get_update() 构建老版本record的update vector用于后续update使用 | row_undo_search_clust_to_pcur() 类上 | row_undo_mod_upd_exist_sec() / row_undo_mod_del_mark_sec() / row_undo_mod_upd_del_sec() 先 更新 二级索引 | row_undo_mod_clust() 再 更新 主键索引 首先解析需要被应用的 undo； 在row_undo_search_clust_to_pcur通过 undo 里面的 key 信息在主键上索引 row reference，并根据 roll_ptr 判断找到的主键是否是需要undo的主键。如果找到则构建并存储找到的 index record 的内存结构 dtuple_t，如果是 update，还需构建出来 update 之前的版本（即应用了undo后还原的原先版本）； 对于 insert 的 undo 较为简单，从二级索引到主键删除目标 record； 对于 update 的 undo，在二级索引上， UPD_EXIST（原操作为 update record）的 undo 会根据可见性标记删除或物理删除现有二级索引记录，再更新或插入undone record； DEL_MARK（原操作为 delete mark record）的 undo 会去除删除标记现有二级索引记录，如果不存在则重新插入； UPD_DEL（原操作为 undelete mark record）的 undo 会根据可见性标记删除或物理删除现有二级索引记录。 对于 update 的 undo，在主键上则走 btr_cur 的 update 逻辑进行更新，对于 UPD_DEL 还会主动尝试清理标记删除的 record。 值得一体的是在崩溃恢复的 undo 事务，事务锁在初始状态下都是隐式锁，是通过其他冲突事务在加锁时通过lock_rec_convert_impl_to_expl 或 undo 应用阶段通过 row_convert_impl_to_expl_if_needed加上事务锁。此过程中加的都是 record lock，这是由于后台 undo 不被用户所见，其本身不具备可见性要求因此不会加上 next key lock。\n回收Undo的代码逻辑 在 innobase_post_recover 中 ddl恢复完成后，保证InnoDB Metadata和file system一致后的阶段，系统会启动 srv_purge_coordinator_thread 和 srv_worker_thread 来purge undo，相关内容会在 purge 系统中介绍。\n版权声明：如需转载或引用，请附加本文链接并注明来源。 ","permalink":"https://mzyee.github.io/posts/mysql/undo/","summary":"InnoDB Undo Log 相关代码学习","title":"InnoDB Undo Log 相关代码学习"},{"content":"eBPF技术背景 BPF（Berkeley Packet Filter）是类Unix系统上数据链路层的一种原始接口，提供原始链路层封包的收发。BPF在数据包过滤上引入了两大特性：\n一个可有效地工作在基于寄存器结构 CPU 上的虚拟机； 应用程序只复制与过滤数据包相关的数据，不复制数据包的所有信息。 eBPF 在原始BPF基础上进一步针对现代 CPU 硬件进行了指令集优化，增加了 VM 中的寄存器数量，使数据处理速度提高了数倍。总结来说，eBPF 提供了一个基于寄存器的虚拟机，使用自定义的 64 位 RISC 指令集，能够在 Linux 内核内运行本地即时编译的 BPF 程序，并能访问内核功能和内存的特定子集。\neBPF程序分为用户空间程序和内核程序两部分： 用户空间程序负责加载 BPF 字节码至内核，一般使用者可以通过 LLVM 或者 GCC 将编写的 BPF 代码程序编译成内核可验证的 BPF 字节码。内核加载前会使用验证器 verfier 组件保证字节码的安全性，以避免对内核造成灾难问题。如有需要，用户空间程序也会负责读取并处理内核回传的统计信息或者事件详情。 在内核中加载的 BPF 字节码程序会在内核中执行特定事件，BPF 程序可能基于 kprobes / uprobes / tracepoint / perf_events 等中的一个或多个事件。如有需要，内核程序也会将执行的结果通过缓存 map 或者 perf-event 事件发送至用户空间。 通过上述机制，eBPF 的使用者（不局限于内核开发者）能够基于系统内核或程序事件高效、安全的（在内核中）执行特定代码，通过向内核添加 eBPF 模块来增加功能。\n基于eBPF进行软件开发 eBPF 指令是固定大小的 64 位编码，大约有上百条指令，被分组为 8 类，常用指令支持如从通用内存进行加载/存储，前/后（非）条件跳转、算术/逻辑操作和函数调用等。\n1 2 3 4 5 6 7 8 9 10 11 12 13 /* msb lsb +---------------------+---------------+----+----+-------+ |immediate |offset |src |dst |opcode | +---------------------+---------------+----+----+-------+ */ struct bpf_insn { __u8\tcode;\t/* opcode */ __u8\tdst_reg:4;\t/* dest register */ __u8\tsrc_reg:4;\t/* source register */ __s16\toff;\t/* signed offset */ __s32\timm;\t/* signed immediate constant */ }; 直接使用原始字节码来实现 eBPF 程序，非常像编写汇编代码，这种行为无疑是较为困难，通常使用更高级别的语言和工具来实现功能复杂的 eBPF 用例。一种很自然的想法是能不能将高级语言的中间表示层编译成 eBPF 程序模块，这样我们就可以将使用具有“限制性“的高级抽象层语言来编写 eBPF 程序。这种设计有效地将内核中运行的eBPF字节码的定义（后端）从字节码加载器和前端程序中分离出来。\n基于这样的目的，社区创建了 BCC 项目，使用户可以使用带有限制性的C语言（BPF C）编写 eBPF 后端程序，并且 BCC 为用户封装了许多实用底层函数避免重复造轮子的苦恼。借助 BCC，用户还可以通过编写 python 来快速实现加载器和前端程序。此外，另一个项目 BPFtrace 建立在 BCC 之上，通过特定领域语言提供更高级别的抽象逻辑，以帮助用户实现更快速分析/调试。\n我们以 BCC 的 bashreadline 为例简单介绍下使用 BCC 进行 eBPF 程序开发的流程。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 #!/usr/bin/env python from __future__ import print_function from bcc import BPF from time import strftime import argparse ############################ # part 1 # ############################ # load BPF program bpf_text = \u0026#34;\u0026#34;\u0026#34; #include \u0026lt;uapi/linux/ptrace.h\u0026gt; #include \u0026lt;linux/sched.h\u0026gt; struct str_t { u32 pid; char str[80]; }; BPF_PERF_OUTPUT(events); int printret(struct pt_regs *ctx) { struct str_t data = {}; char comm[TASK_COMM_LEN] = {}; if (!PT_REGS_RC(ctx)) return 0; data.pid = bpf_get_current_pid_tgid() \u0026gt;\u0026gt; 32; bpf_probe_read_user(\u0026amp;data.str, sizeof(data.str), (void *)PT_REGS_RC(ctx)); bpf_get_current_comm(\u0026amp;comm, sizeof(comm)); if (comm[0] == \u0026#39;b\u0026#39; \u0026amp;\u0026amp; comm[1] == \u0026#39;a\u0026#39; \u0026amp;\u0026amp; comm[2] == \u0026#39;s\u0026#39; \u0026amp;\u0026amp; comm[3] == \u0026#39;h\u0026#39; \u0026amp;\u0026amp; comm[4] == 0 ) { events.perf_submit(ctx,\u0026amp;data,sizeof(data)); } return 0; }; \u0026#34;\u0026#34;\u0026#34; ############################ # part 2 # ############################ parser = argparse.ArgumentParser( description=\u0026#34;Print entered bash commands from all running shells\u0026#34;, formatter_class=argparse.RawDescriptionHelpFormatter) parser.add_argument(\u0026#34;-s\u0026#34;, \u0026#34;--shared\u0026#34;, nargs=\u0026#34;?\u0026#34;, const=\u0026#34;/lib/libreadline.so\u0026#34;, type=str, help=\u0026#34;specify the location of libreadline.so library.\\ Default is /lib/libreadline.so\u0026#34;) args = parser.parse_args() name = args.shared if args.shared else \u0026#34;/bin/bash\u0026#34; b = BPF(text=bpf_text) b.attach_uretprobe(name=name, sym=\u0026#34;readline\u0026#34;, fn_name=\u0026#34;printret\u0026#34;) ############################ # part 3 # ############################ # header print(\u0026#34;%-9s %-7s %s\u0026#34; % (\u0026#34;TIME\u0026#34;, \u0026#34;PID\u0026#34;, \u0026#34;COMMAND\u0026#34;)) def print_event(cpu, data, size): event = b[\u0026#34;events\u0026#34;].event(data) print(\u0026#34;%-9s %-7d %s\u0026#34; % (strftime(\u0026#34;%H:%M:%S\u0026#34;), event.pid, event.str.decode(\u0026#39;utf-8\u0026#39;, \u0026#39;replace\u0026#39;))) b[\u0026#34;events\u0026#34;].open_perf_buffer(print_event) while 1: try: b.perf_buffer_poll() except KeyboardInterrupt: exit() 可以将上面这个 eBPF 程序分为3个部分：\nPart 1 是用 BPF C 实现的 eBPF 后端程序，首先这里构建了内置 BPF_PERF_OUTPUT 结构来创建一个 BPF table，通过 perf 环形缓冲区将自定义事件数据推送到用户空间。用户可以通过 events 来获取推送的数据。然后在 printret 函数中通过 PT_REGS_RC 来判断、获取当前环境函数的返回值，并通过内置函数获取程序pid和返回值。通过内置函数 bpf_get_current_comm 判断当前程序名称，如果是 bash 命令就向 events 输出程序运行返回值。 Part 2 是通过 BCC 的支持利用 python 进行前述 eBPF 后端程序的加载器。加载类型是 uretprobe，即将程序挂载到 user-level 的 readline() 函数上，即在用户调用 readline() 函数返回时执行相应 eBPF 后端代码。 Part 3 是显示输出的前端程序，用户对 events 缓冲区进行polling，当缓冲区有内容时将对应内容输出显示。 至此，通过运行这一个 BCC 程序，用户就可以通过 eBPF实现监控所有 bash 进程的 readline 函数，输出对应 pid 和 readline 对返回结果（即 bash 输入的命令内容）。更多的内置功能及接口自行参见相关项目官网。\n事件类型 前面提到 BPF 程序类型可能基于 kprobes / uprobes / tracepoint /perf_events 等事件中的一个或多个，其中：\nkprobes：内核中动态跟踪。可以跟踪到 Linux 内核中的函数入口或返回点，但不是稳定接口，可能会因为内核版本变化导致跟踪失效。 理论上可以跟踪到所有导出符号 /proc/kallsyms 但不在 /sys/kernel/debug/kprobes/blacklist 中的函数。 uprobes：用户级别的动态跟踪。与 kprobes 类似，只是跟踪的函数为用户程序中的函数。 tracepoints：内核中静态跟踪。tracepoints 是内核开发人员维护的跟踪点，能够提供稳定的接口，但是需维护数量且场景受限。 USDT：用户静态探针，类似 tracepoints 但是需要用户空间自己维护。 perf_events：定时采样和 PMC 事件。 eBPF 模块各种类型事件的加载执行在后面有时间的话再展开讲讲，有兴趣的可以搜索 Linux 内核的 text hook/poke、static-key 和 static-jump 等机制实现。\n常用eBPF工具与指令 BCC 和 BPFtrace 提供了很多现成实用的工具，下面分享一些我觉得有用的工具，具体参数参考各个命令的help查看。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # 检查 pid程序打开的文件 opensnoop -p 123 # 检测 disk 的 I/O 状态 biolatency [-D for each disk] [-Q include OS queued time in I/O time] # 追踪pid程序fs操作大于10ms的操作 ext4slower 10 -p 123 # 追踪pid程序fs操作分布 ext4dist -p 123 # 系统 time 时间的 cache 状态 cachestat time llcstat time # 函数 count / latency / call-interval 统计: funccount \u0026#39;./test:read*\u0026#39; -p 123 -d 1 funclatency \u0026#39;./test:read*\u0026#39; -p 123 -d 1 funcinterval \u0026#39;./test:read*\u0026#39; -p 123 -d 1 # on-CPU 和 off-CPU 火焰图，采集 DURATION 秒 profile -p 123 -f DURATION --stack-storage-size=165535 \u0026gt; profile01.txt flamegraph.pl --width=1600 \u0026lt; profile01.txt \u0026gt; profile01.svg offcputime -p 123 -f DURATION --stack-storage-size=165535 \u0026gt; offcpu01.txt flamegraph.pl --width=1600 \u0026lt; offcpu01.txt \u0026gt; offcpu01.svg # 采集 pid 程序cache-misses超过的10000次线程及其10000次的次数 bpftrace -e \u0026#39;hardware:cache-misses:10000 /pid==123/ { @[comm, tid] = count(); }\u0026#39; 版权声明：如需转载或引用，请附加本文链接并注明来源。 ","permalink":"https://mzyee.github.io/posts/usebpf/","summary":"Learning About eBPF Usages","title":"通过 eBPF 工具进行性能分析"},{"content":"E-mail: mzyeee@gmail.com\n知乎: https://www.zhihu.com/people/mzy-59-9\n","permalink":"https://mzyee.github.io/contact/","summary":"E-mail: mzyeee@gmail.com\n知乎: https://www.zhihu.com/people/mzy-59-9","title":""}]